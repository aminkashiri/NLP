{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argon2-cffi==21.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (21.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (21.2.0)\n",
      "Requirement already satisfied: asttokens==2.0.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.0.5)\n",
      "Requirement already satisfied: attrs==21.4.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (21.4.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.10.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: bleach==4.1.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.1.0)\n",
      "Requirement already satisfied: cffi==1.15.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: debugpy==1.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: entrypoints==0.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (0.4)\n",
      "Requirement already satisfied: executing==0.8.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.8.3)\n",
      "Requirement already satisfied: fastjsonschema==2.15.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (2.15.3)\n",
      "Requirement already satisfied: ipykernel==6.12.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (6.12.1)\n",
      "Requirement already satisfied: ipython==8.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (8.2.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: jedi==0.18.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (0.18.1)\n",
      "Requirement already satisfied: Jinja2==3.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (3.1.1)\n",
      "Requirement already satisfied: jsonschema==4.4.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (4.4.0)\n",
      "Requirement already satisfied: jupyter-client==7.2.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (7.2.1)\n",
      "Requirement already satisfied: jupyter-core==4.9.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (4.9.2)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe==2.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (2.1.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (0.1.3)\n",
      "Requirement already satisfied: mistune==0.8.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (0.8.4)\n",
      "Requirement already satisfied: nbclient==0.5.13 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (0.5.13)\n",
      "Requirement already satisfied: nbconvert==6.4.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (6.4.5)\n",
      "Requirement already satisfied: nbformat==5.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (5.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.5.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (1.5.5)\n",
      "Requirement already satisfied: notebook==6.4.10 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (6.4.10)\n",
      "Requirement already satisfied: packaging==21.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (21.3)\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (1.5.0)\n",
      "Requirement already satisfied: parso==0.8.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (0.8.3)\n",
      "Requirement already satisfied: pexpect==4.8.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (0.7.5)\n",
      "Requirement already satisfied: prometheus-client==0.13.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (0.13.1)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.29 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (3.0.29)\n",
      "Requirement already satisfied: psutil==5.9.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (5.9.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 40)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (0.2.2)\n",
      "Requirement already satisfied: pycparser==2.21 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (2.21)\n",
      "Requirement already satisfied: Pygments==2.11.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (2.11.2)\n",
      "Requirement already satisfied: pyparsing==3.0.7 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 44)) (3.0.7)\n",
      "Requirement already satisfied: pyrsistent==0.18.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 46)) (2.8.2)\n",
      "Requirement already satisfied: pyzmq==22.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash==1.8.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 48)) (1.8.0)\n",
      "Requirement already satisfied: six==1.16.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.3.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 50)) (2.3.1)\n",
      "Requirement already satisfied: stack-data==0.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (0.2.0)\n",
      "Requirement already satisfied: terminado==0.13.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (0.13.3)\n",
      "Requirement already satisfied: testpath==0.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (0.6.0)\n",
      "Requirement already satisfied: tornado==6.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (6.1)\n",
      "Requirement already satisfied: traitlets==5.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (5.1.1)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (0.2.5)\n",
      "Requirement already satisfied: webencodings==0.5.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (0.5.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./.venv/lib/python3.9/site-packages (from ipython==8.2.0->-r requirements.txt (line 16)) (44.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_output(output_type, marker, span):\n",
    "    return {\"type\":output_type, \"marker\":marker, \"span\":repr(span)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Symbolizer:\n",
    "    def __init__(self):\n",
    "        self.symbols = []\n",
    "        with open('namad.txt') as symbols_file:\n",
    "            line = symbols_file.readline()\n",
    "            while line:\n",
    "                n = line.split('\\t')\n",
    "                self.symbols.append(n[0])\n",
    "                # TODO: we can add more information to a symbol here\n",
    "                # namad_description.append(n[1])\n",
    "                line = symbols_file.readline()\n",
    "\n",
    "    def symbolize(self, sentence):\n",
    "        output = []\n",
    "        print(output)\n",
    "        for namad in self.symbols:\n",
    "            for m in re.finditer(namad, sentence):\n",
    "                print('---------', namad, len(self.symbols))\n",
    "                output.append(create_output(\"نماد\", namad, m.span()))\n",
    "                print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = ['مثبت','سود','صعود']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_extractor(noun_phrase):\n",
    "    output=[]\n",
    "    for event in events:\n",
    "        for m in re.finditer(event, noun_phrase.group()):\n",
    "                output.append(create_output(\"واقعه\", noun_phrase.group()[1:-3], noun_phrase.span()))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Symbolizer' object has no attribute 'symbols'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000004?line=0'>1</a>\u001b[0m sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mرشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000004?line=1'>2</a>\u001b[0m symbolizer \u001b[39m=\u001b[39m Symbolizer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000004?line=2'>3</a>\u001b[0m output \u001b[39m=\u001b[39m symbolizer\u001b[39m.\u001b[39msymbolize(sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000004?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "\u001b[1;32m/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb Cell 4'\u001b[0m in \u001b[0;36mSymbolizer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000002?line=7'>8</a>\u001b[0m \u001b[39mwhile\u001b[39;00m line:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000002?line=8'>9</a>\u001b[0m     n \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000002?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msymbols\u001b[39m.\u001b[39mappend(n[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000002?line=10'>11</a>\u001b[0m     \u001b[39m# TODO: we can add more information to a symbol here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000002?line=11'>12</a>\u001b[0m     \u001b[39m# namad_description.append(n[1])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000002?line=12'>13</a>\u001b[0m     line \u001b[39m=\u001b[39m symbols_file\u001b[39m.\u001b[39mreadline()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Symbolizer' object has no attribute 'symbols'"
     ]
    }
   ],
   "source": [
    "sentence = 'رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد'\n",
    "symbolizer = Symbolizer()\n",
    "output = symbolizer.symbolize(sentence)\n",
    "print(output)\n",
    "\n",
    "# tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "# chunker = hazm.Chunker(model='../resources/chunker.model')\n",
    "# tagged = tagger.tag(hazm.word_tokenize(sentence))\n",
    "# # sentence_tokens = word_tokenize(sentence)\n",
    "# # print(chunker.parse(tagged))b.group()\n",
    "# print(tagged)\n",
    "# a = hazm.tree2brackets(chunker.parse(tagged))\n",
    "# print(a)\n",
    "# # for b in  re.finditer(r\"\\[[آ-یء چ]+NP\\]\", a):\n",
    "# for b in  re.finditer(r\"\\[[^\\]]*NP\\]\", a):\n",
    "#     print('--------------')\n",
    "#     print(b.group())\n",
    "#     print(event_extractor(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "می‌خوانیم\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('مرد', 'Ne'), ('سود', 'Ne'), ('ده', 'NUM')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hazm\n",
    "from __future__ import unicode_literals\n",
    "tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "print(tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))[3][0])\n",
    "# tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))\n",
    "tagger.tag(hazm.word_tokenize('کتاب خواندن را دوست داریم'))\n",
    "tagger.tag(hazm.word_tokenize('من درخت بلند را دیدم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد زبان نفهم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد سود ده'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP هوا/N) (NP ۵/NUM درجه/N) (ADJP گرم/AJ) (VP شد/V))\n"
     ]
    }
   ],
   "source": [
    "word = 'کتاب خواندن را دوست داریم'\n",
    "word = '۵ گل زیبا خریدم'\n",
    "word = 'سهام ۳ واحد افزایش یافت'\n",
    "word = 'هوا ۵ درجه گرم شد'\n",
    "chunker = hazm.Chunker(model='resources/chunker.model')\n",
    "chunker = hazm.RuleBasedChunker()\n",
    "tagged = tagger.tag(hazm.word_tokenize(word))\n",
    "print(chunker.parse(tagged))\n",
    "print(hazm.tree2brackets(chunker.parse(tagged)))\n",
    "chunker.parse(tagged).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.ChunkParserI('a big cat was dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\u200c', '工', 'ب\\u200cر', 'امی\\u200cن']\n",
      "امی‌ن\n",
      "امی‌ن\n"
     ]
    }
   ],
   "source": [
    "lst = [u'\\u200c', u'\\u5de5', 'ب\\u200cر', 'امی‌ن']\n",
    "print(lst)\n",
    "print('امی\\u200cن')\n",
    "print('امی‌ن')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/amin/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRON'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('little', 'ADJ'),\n",
       " ('tall', 'ADJ'),\n",
       " ('boy', 'NOUN')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "# nltk.download('universal_tagset')\n",
    "# text = nltk.word_tokenize(\"He is a good, tall boy\")\n",
    "text = nltk.word_tokenize(\"He is a little tall boy\")\n",
    "# text = nltk.word_tokenize(\"the little crazy cat sat on the mat\")\n",
    "# text = nltk.word_tokenize(\"amin is clever and good boy\")\n",
    "# text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text, tagset='universal')\n",
    "# nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected string or list of RegexpChunkParsers for the grammar.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=0'>1</a>\u001b[0m unchunked_sent \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mthe little cat sat on the mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=1'>2</a>\u001b[0m rule1 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mchunk\u001b[39m.\u001b[39mregexp\u001b[39m.\u001b[39mChunkRule(\u001b[39m'\u001b[39m\u001b[39m<NN|DT>+\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mChunk sequences of NN and DT\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=3'>4</a>\u001b[0m chunkparser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mchunk\u001b[39m.\u001b[39;49mregexp\u001b[39m.\u001b[39;49mRegexpParser( [rule1] )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=4'>5</a>\u001b[0m chunkparser\u001b[39m.\u001b[39mparse(unchunked_sent)\n",
      "File \u001b[0;32m~/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py:1145\u001b[0m, in \u001b[0;36mRegexpParser.__init__\u001b[0;34m(self, grammar, root_label, loop, trace)\u001b[0m\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1142'>1143</a>\u001b[0m \u001b[39mfor\u001b[39;00m elt \u001b[39min\u001b[39;00m grammar:\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1143'>1144</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(elt, RegexpChunkParser):\n\u001b[0;32m-> <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1144'>1145</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(type_err)\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1145'>1146</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stages \u001b[39m=\u001b[39m grammar\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string or list of RegexpChunkParsers for the grammar."
     ]
    }
   ],
   "source": [
    "unchunked_sent = 'the little cat sat on the mat'\n",
    "rule1 = nltk.chunk.regexp.ChunkRule('<NN|DT>+',\n",
    "    'Chunk sequences of NN and DT')\n",
    "chunkparser = nltk.chunk.regexp.RegexpParser( [rule1] )\n",
    "chunkparser.parse(unchunked_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'little', 'crazy', 'cat', 'is', 'sitting', 'on', 'the', 'mat']\n",
      "[('the', 'DT'), ('little', 'JJ'), ('crazy', 'JJ'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]\n",
      "(S\n",
      "  (NP the/DT little/JJ crazy/JJ cat/NN)\n",
      "  (VP is/VBZ sitting/VBG)\n",
      "  on/IN\n",
      "  (NP the/DT mat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"learn php from guru99\"\n",
    "text = 'the little crazy cat is sitting on the mat'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} \n",
    "VP: {<VB.?>*}\n",
    "\"\"\"\n",
    "cp  = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdff23a6f2e028a72d356297edfd2a8e740711d0ae6d770ba572ded92efe19b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
