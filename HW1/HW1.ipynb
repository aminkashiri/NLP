{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argon2-cffi==21.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (21.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (21.2.0)\n",
      "Requirement already satisfied: asttokens==2.0.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.0.5)\n",
      "Requirement already satisfied: attrs==21.4.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (21.4.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.10.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: bleach==4.1.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.1.0)\n",
      "Requirement already satisfied: cffi==1.15.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: debugpy==1.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: entrypoints==0.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (0.4)\n",
      "Requirement already satisfied: executing==0.8.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (0.8.3)\n",
      "Requirement already satisfied: fastjsonschema==2.15.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (2.15.3)\n",
      "Requirement already satisfied: ipykernel==6.12.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (6.12.1)\n",
      "Requirement already satisfied: ipython==8.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (8.2.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: jedi==0.18.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (0.18.1)\n",
      "Requirement already satisfied: Jinja2==3.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (3.1.1)\n",
      "Requirement already satisfied: jsonschema==4.4.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (4.4.0)\n",
      "Requirement already satisfied: jupyter-client==7.2.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (7.2.1)\n",
      "Requirement already satisfied: jupyter-core==4.9.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (4.9.2)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe==2.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (2.1.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (0.1.3)\n",
      "Requirement already satisfied: mistune==0.8.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (0.8.4)\n",
      "Requirement already satisfied: nbclient==0.5.13 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (0.5.13)\n",
      "Requirement already satisfied: nbconvert==6.4.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (6.4.5)\n",
      "Requirement already satisfied: nbformat==5.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (5.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.5.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (1.5.5)\n",
      "Requirement already satisfied: notebook==6.4.10 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (6.4.10)\n",
      "Requirement already satisfied: packaging==21.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 32)) (21.3)\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (1.5.0)\n",
      "Requirement already satisfied: parso==0.8.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (0.8.3)\n",
      "Requirement already satisfied: pexpect==4.8.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 35)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 36)) (0.7.5)\n",
      "Requirement already satisfied: prometheus-client==0.13.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 37)) (0.13.1)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.29 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (3.0.29)\n",
      "Requirement already satisfied: psutil==5.9.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 39)) (5.9.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 40)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 41)) (0.2.2)\n",
      "Requirement already satisfied: pycparser==2.21 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (2.21)\n",
      "Requirement already satisfied: Pygments==2.11.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 43)) (2.11.2)\n",
      "Requirement already satisfied: pyparsing==3.0.7 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 44)) (3.0.7)\n",
      "Requirement already satisfied: pyrsistent==0.18.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 46)) (2.8.2)\n",
      "Requirement already satisfied: pyzmq==22.3.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash==1.8.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 48)) (1.8.0)\n",
      "Requirement already satisfied: six==1.16.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.3.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 50)) (2.3.1)\n",
      "Requirement already satisfied: stack-data==0.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (0.2.0)\n",
      "Requirement already satisfied: terminado==0.13.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (0.13.3)\n",
      "Requirement already satisfied: testpath==0.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 53)) (0.6.0)\n",
      "Requirement already satisfied: tornado==6.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (6.1)\n",
      "Requirement already satisfied: traitlets==5.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (5.1.1)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (0.2.5)\n",
      "Requirement already satisfied: webencodings==0.5.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 57)) (0.5.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./.venv/lib/python3.9/site-packages (from ipython==8.2.0->-r requirements.txt (line 16)) (44.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_output(output_type, marker, span):\n",
    "    return {\"type\":output_type, \"marker\":marker, \"span\":repr(span)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Symbolizer:\n",
    "    def __init__(self):\n",
    "        self.symbols = []\n",
    "        with open('namad.txt') as symbols_file:\n",
    "            line = symbols_file.readline()\n",
    "            while line:\n",
    "                n = line.split('\\t')\n",
    "                self.symbols.append(n[0])\n",
    "                # TODO: we can add more information to a symbol here\n",
    "                # namad_description.append(n[1])\n",
    "                line = symbols_file.readline()\n",
    "\n",
    "    def symbolize(self, sentence):\n",
    "        output = []\n",
    "        for namad in self.symbols:\n",
    "            for m in re.finditer(namad, sentence):\n",
    "                output.append(create_output(\"نماد\", namad, m.span()))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [\n",
    "    \"مثبت\",\n",
    "    \"سود\",\n",
    "    \"صعود\",\n",
    "    \"افزایش\",\n",
    "    \"کاهش\",\n",
    "    \"رشد\",\n",
    "    \"ریزش\",\n",
    "    \"عرضه\",\n",
    "    \"افشا\",\n",
    "    \"اصلاح\",\n",
    "    \"نزول\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_extractor(noun_phrase):\n",
    "    output=[]\n",
    "    for event in events:\n",
    "        for m in re.finditer(event, noun_phrase.group()):\n",
    "                output.append(create_output(\"واقعه\", noun_phrase.group()[1:-3], noun_phrase.span()))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase(tagged_str, phrase=\"NP\"):\n",
    "    # for b in  re.finditer(r\"\\[[آ-یء چ]+NP\\]\", a):\n",
    "    phrases = []\n",
    "    for b in re.finditer(rf\"\\[[^\\]]*{phrase}\\]\", tagged_str):\n",
    "        phrases.append(b.group())\n",
    "        extracted_event = event_extractor(b)\n",
    "        if extracted_event:\n",
    "            print('event ectracted: ')\n",
    "            print(extracted_event)\n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "ارزش سهام شرکت نفت کاهش یافت\n",
      "symbols: \n",
      "tagged: \n",
      "[('ارزش', 'Ne'), ('سهام', 'Ne'), ('شرکت', 'Ne'), ('نفت', 'N'), ('کاهش', 'N'), ('یافت', 'V')]\n",
      "chukned: \n",
      "[ارزش سهام شرکت نفت NP] [کاهش یافت VP]\n",
      "-------------\n",
      "قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\n",
      "symbols: \n",
      "[{'type': 'نماد', 'marker': 'وغدیر', 'span': '(36, 41)'}]\n",
      "tagged: \n",
      "[('قرارداد', 'N'), ('با', 'P'), ('آمریکا', 'N'), ('باعث', 'AJe'), ('افت', 'Ne'), ('قیمت', 'Ne'), ('سهم', 'N'), ('وغدیر', 'AJ'), ('شد', 'V')]\n",
      "chukned: \n",
      "[قرارداد NP] [با PP] [آمریکا NP] [باعث افت قیمت سهم وغدیر ADJP] [شد VP]\n",
      "-------------\n",
      "ریزش بازار به دلیل حمله‌ی روسیه است.\n",
      "symbols: \n",
      "tagged: \n",
      "[('ریزش', 'Ne'), ('بازار', 'N'), ('به', 'P'), ('دلیل', 'Ne'), ('حمله\\u200cی', 'Ne'), ('روسیه', 'N'), ('است', 'V'), ('.', 'PUNC')]\n",
      "chukned: \n",
      "[ریزش بازار NP] [به PP] [دلیل حمله‌ی روسیه NP] [است VP] .\n",
      "event ectracted: \n",
      "[{'type': 'واقعه', 'marker': 'ریزش بازار ', 'span': '(0, 15)'}]\n",
      "-------------\n",
      "فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\n",
      "symbols: \n",
      "tagged: \n",
      "[('فک', 'N'), ('کنم', 'V'), ('یه', 'NUM'), ('اصلاح', 'Ne'), ('قیمتی', 'AJ'), ('و', 'CONJ'), ('کمی', 'ADV'), ('ریزش', 'N'), ('داشته_باشیم', 'V'), ('.', 'PUNC')]\n",
      "chukned: \n",
      "[فک کنم VP] [یه اصلاح قیمتی NP] و [کمی ADVP] [ریزش NP] [داشته_باشیم VP] .\n",
      "event ectracted: \n",
      "[{'type': 'واقعه', 'marker': 'یه اصلاح قیمتی ', 'span': '(12, 31)'}]\n",
      "event ectracted: \n",
      "[{'type': 'واقعه', 'marker': 'ریزش ', 'span': '(45, 54)'}]\n",
      "-------------\n",
      "یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\n",
      "symbols: \n",
      "tagged: \n",
      "[('یک', 'NUM'), ('نکته\\u200cی', 'Ne'), ('تکنیکالی', 'AJ'), ('هم', 'CONJ'), ('در', 'P'), ('صورت', 'Ne'), ('دستکاری', 'N'), ('نشدن', 'N'), ('اضافه', 'AJ'), ('کنم', 'V'), ('،', 'PUNC'), ('کندلی', 'N'), ('که', 'CONJ'), ('روز', 'Ne'), ('سه', 'NUM'), ('شنبه\\u200cی', 'Ne'), ('گذشته', 'ADV'), ('ثبت', 'N'), ('کرد', 'V'), ('کامل', 'AJ'), ('است', 'V')]\n",
      "chukned: \n",
      "[یک نکته‌ی تکنیکالی NP] [هم ADVP] [در PP] [صورت دستکاری نشدن NP] [اضافه کنم VP] ، [کندلی NP] که [روز سه شنبه‌ی گذشته NP] [ثبت کرد VP] [کامل ADJP] [است VP]\n",
      "-------------\n",
      "روز چهارشنبه یه دفعه برای خودشون افشا زدن\n",
      "symbols: \n",
      "tagged: \n",
      "[('روز', 'Ne'), ('چهارشنبه', 'N'), ('یه', 'NUM'), ('دفعه', 'N'), ('برای', 'Pe'), ('خودشون', 'PRO'), ('افشا', 'N'), ('زدن', 'N')]\n",
      "chukned: \n",
      "[روز چهارشنبه ADVP] [یه دفعه ADVP] [برای PP] [خودشون افشا زدن NP]\n",
      "event ectracted: \n",
      "[{'type': 'واقعه', 'marker': 'خودشون افشا زدن ', 'span': '(45, 65)'}]\n",
      "-------------\n",
      "رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\n",
      "symbols: \n",
      "[{'type': 'نماد', 'marker': 'پرشیا', 'span': '(38, 43)'}]\n",
      "tagged: \n",
      "[('رشد', 'Ne'), ('قیمت\\u200cها', 'N'), ('باعث', 'AJe'), ('ایجاد', 'Ne'), ('صف', 'Ne'), ('خرید', 'N'), ('در', 'P'), ('سهم', 'Ne'), ('پرشیا', 'N'), ('شد', 'V')]\n",
      "chukned: \n",
      "[رشد قیمت‌ها NP] [باعث ایجاد صف خرید ADJP] [در PP] [سهم NP] [پرشیا شد VP]\n",
      "event ectracted: \n",
      "[{'type': 'واقعه', 'marker': 'رشد قیمت\\u200cها ', 'span': '(0, 16)'}]\n",
      "-------------\n",
      "شاخص به ۲ میلیون می‌رسه\n",
      "symbols: \n",
      "tagged: \n",
      "[('شاخص', 'N'), ('به', 'P'), ('۲', 'NUM'), ('میلیون', 'NUM'), ('می\\u200cرسه', 'N')]\n",
      "chukned: \n",
      "[شاخص NP] [به PP] [۲ میلیون می‌رسه NP]\n",
      "-------------\n",
      "آمریکا باعث ریزش بازار شد\n",
      "symbols: \n",
      "tagged: \n",
      "[('آمریکا', 'N'), ('باعث', 'AJe'), ('ریزش', 'Ne'), ('بازار', 'N'), ('شد', 'V')]\n",
      "chukned: \n",
      "[آمریکا NP] [باعث ریزش بازار ADJP] [شد VP]\n",
      "-------------\n",
      "آمریکا موجب ریزش بازار شد\n",
      "symbols: \n",
      "tagged: \n",
      "[('آمریکا', 'N'), ('موجب', 'Ne'), ('ریزش', 'Ne'), ('بازار', 'N'), ('شد', 'V')]\n",
      "chukned: \n",
      "[آمریکا NP] [موجب ریزش بازار ADJP] [شد VP]\n",
      "-------------\n",
      "آمریکا دلیل ریزش بازار شد\n",
      "symbols: \n",
      "tagged: \n",
      "[('آمریکا', 'N'), ('دلیل', 'Ne'), ('ریزش', 'Ne'), ('بازار', 'N'), ('شد', 'V')]\n",
      "chukned: \n",
      "[آمریکا NP] [دلیل ریزش NP] [بازار شد VP]\n",
      "event ectracted: \n",
      "[{'type': 'واقعه', 'marker': 'دلیل ریزش ', 'span': '(12, 26)'}]\n",
      "-------------\n",
      "کاهش قیمت سهم عجیب بود\n",
      "symbols: \n",
      "tagged: \n",
      "[('کاهش', 'Ne'), ('قیمت', 'Ne'), ('سهم', 'Ne'), ('عجیب', 'AJ'), ('بود', 'V')]\n",
      "chukned: \n",
      "[کاهش قیمت سهم عجیب NP] [بود VP]\n",
      "event ectracted: \n",
      "[{'type': 'واقعه', 'marker': 'کاهش قیمت سهم عجیب ', 'span': '(0, 23)'}]\n",
      "-------------\n",
      "قیمت زیاد شد\n",
      "symbols: \n",
      "tagged: \n",
      "[('قیمت', 'Ne'), ('زیاد', 'AJ'), ('شد', 'V')]\n",
      "chukned: \n",
      "[قیمت NP] [زیاد ADJP] [شد VP]\n",
      "-------------\n",
      "قیمت زیاد است\n",
      "symbols: \n",
      "tagged: \n",
      "[('قیمت', 'Ne'), ('زیاد', 'AJ'), ('است', 'V')]\n",
      "chukned: \n",
      "[قیمت NP] [زیاد ADJP] [است VP]\n",
      "-------------\n",
      "سهم قیمتش پایین است\n",
      "symbols: \n",
      "tagged: \n",
      "[('سهم', 'Ne'), ('قیمتش', 'N'), ('پایین', 'AJ'), ('است', 'V')]\n",
      "chukned: \n",
      "[سهم قیمتش NP] [پایین NP] [است VP]\n",
      "-------------\n",
      "حضور تو موجب خوشحالی من در هوای بارانی است\n",
      "symbols: \n",
      "tagged: \n",
      "[('حضور', 'Ne'), ('تو', 'PRO'), ('موجب', 'Ne'), ('خوشحالی', 'Ne'), ('من', 'PRO'), ('در', 'P'), ('هوای', 'Ne'), ('بارانی', 'AJ'), ('است', 'V')]\n",
      "chukned: \n",
      "[حضور تو NP] [موجب خوشحالی من NP] [در PP] [هوای بارانی NP] [است VP]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"ارزش سهام شرکت نفت کاهش یافت\",\n",
    "    \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    \"ریزش بازار به دلیل حمله‌ی روسیه است.\",\n",
    "    \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "    \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    \"آمریکا باعث ریزش بازار شد\",\n",
    "    \"آمریکا موجب ریزش بازار شد\",\n",
    "    \"آمریکا دلیل ریزش بازار شد\",\n",
    "    \"کاهش قیمت سهم عجیب بود\",\n",
    "    \"قیمت زیاد شد\",\n",
    "    \"قیمت زیاد است\",\n",
    "    \"سهم قیمتش پایین است\",\n",
    "    \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "]\n",
    "tagger = hazm.POSTagger(model=\"../resources/postagger.model\")\n",
    "chunker = hazm.Chunker(model=\"../resources/chunker.model\")\n",
    "for sentence in sentences:\n",
    "    print(\"-------------\")\n",
    "    print(sentence)\n",
    "    print(\"symbols: \")\n",
    "    symbolizer = Symbolizer()\n",
    "    output = symbolizer.symbolize(sentence)\n",
    "    if output:\n",
    "        print(output)\n",
    "\n",
    "    tagged = tagger.tag(hazm.word_tokenize(sentence))\n",
    "    print(\"tagged: \")\n",
    "    print(tagged)\n",
    "    chunked_str = hazm.tree2brackets(chunker.parse(tagged))\n",
    "    print(\"chukned: \")\n",
    "    print(chunked_str)\n",
    "    get_phrase(chunked_str, phrase=\"NP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "می‌خوانیم\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('مرد', 'Ne'), ('سود', 'Ne'), ('ده', 'NUM')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hazm\n",
    "from __future__ import unicode_literals\n",
    "tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "print(tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))[3][0])\n",
    "# tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))\n",
    "tagger.tag(hazm.word_tokenize('کتاب خواندن را دوست داریم'))\n",
    "tagger.tag(hazm.word_tokenize('من درخت بلند را دیدم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد زبان نفهم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد سود ده'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP هوا/N) (NP ۵/NUM درجه/N) (ADJP گرم/AJ) (VP شد/V))\n",
      "[هوا NP] [۵ درجه NP] [گرم ADJP] [شد VP]\n"
     ]
    }
   ],
   "source": [
    "word = 'کتاب خواندن را دوست داریم'\n",
    "word = '۵ گل زیبا خریدم'\n",
    "word = 'سهام ۳ واحد افزایش یافت'\n",
    "word = 'هوا ۵ درجه گرم شد'\n",
    "chunker = hazm.Chunker(model='../resources/chunker.model')\n",
    "chunker = hazm.RuleBasedChunker()\n",
    "tagged = tagger.tag(hazm.word_tokenize(word))\n",
    "print(chunker.parse(tagged))\n",
    "print(hazm.tree2brackets(chunker.parse(tagged)))\n",
    "chunker.parse(tagged).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.ChunkParserI('a big cat was dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\u200c', '工', 'ب\\u200cر', 'امی\\u200cن']\n",
      "امی‌ن\n",
      "امی‌ن\n"
     ]
    }
   ],
   "source": [
    "lst = [u'\\u200c', u'\\u5de5', 'ب\\u200cر', 'امی‌ن']\n",
    "print(lst)\n",
    "print('امی\\u200cن')\n",
    "print('امی‌ن')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/amin/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRON'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('little', 'ADJ'),\n",
       " ('tall', 'ADJ'),\n",
       " ('boy', 'NOUN')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "# nltk.download('universal_tagset')\n",
    "# text = nltk.word_tokenize(\"He is a good, tall boy\")\n",
    "text = nltk.word_tokenize(\"He is a little tall boy\")\n",
    "# text = nltk.word_tokenize(\"the little crazy cat sat on the mat\")\n",
    "# text = nltk.word_tokenize(\"amin is clever and good boy\")\n",
    "# text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text, tagset='universal')\n",
    "# nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected string or list of RegexpChunkParsers for the grammar.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=0'>1</a>\u001b[0m unchunked_sent \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mthe little cat sat on the mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=1'>2</a>\u001b[0m rule1 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mchunk\u001b[39m.\u001b[39mregexp\u001b[39m.\u001b[39mChunkRule(\u001b[39m'\u001b[39m\u001b[39m<NN|DT>+\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mChunk sequences of NN and DT\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=3'>4</a>\u001b[0m chunkparser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mchunk\u001b[39m.\u001b[39;49mregexp\u001b[39m.\u001b[39;49mRegexpParser( [rule1] )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=4'>5</a>\u001b[0m chunkparser\u001b[39m.\u001b[39mparse(unchunked_sent)\n",
      "File \u001b[0;32m~/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py:1145\u001b[0m, in \u001b[0;36mRegexpParser.__init__\u001b[0;34m(self, grammar, root_label, loop, trace)\u001b[0m\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1142'>1143</a>\u001b[0m \u001b[39mfor\u001b[39;00m elt \u001b[39min\u001b[39;00m grammar:\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1143'>1144</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(elt, RegexpChunkParser):\n\u001b[0;32m-> <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1144'>1145</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(type_err)\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1145'>1146</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stages \u001b[39m=\u001b[39m grammar\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string or list of RegexpChunkParsers for the grammar."
     ]
    }
   ],
   "source": [
    "unchunked_sent = 'the little cat sat on the mat'\n",
    "rule1 = nltk.chunk.regexp.ChunkRule('<NN|DT>+',\n",
    "    'Chunk sequences of NN and DT')\n",
    "chunkparser = nltk.chunk.regexp.RegexpParser( [rule1] )\n",
    "chunkparser.parse(unchunked_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'little', 'crazy', 'cat', 'is', 'sitting', 'on', 'the', 'mat']\n",
      "[('the', 'DT'), ('little', 'JJ'), ('crazy', 'JJ'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]\n",
      "(S\n",
      "  (NP the/DT little/JJ crazy/JJ cat/NN)\n",
      "  (VP is/VBZ sitting/VBG)\n",
      "  on/IN\n",
      "  (NP the/DT mat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"learn php from guru99\"\n",
    "text = 'the little crazy cat is sitting on the mat'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} \n",
    "VP: {<VB.?>*}\n",
    "\"\"\"\n",
    "cp  = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(دختر ۵ خوب)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"306pt\" height=\"305pt\"\n viewBox=\"0.00 0.00 306.00 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-301 302,-301 302,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<text text-anchor=\"middle\" x=\"168\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n</g>\n<!-- 2 -->\n<g id=\"node2\" class=\"node\">\n<title>2</title>\n<text text-anchor=\"middle\" x=\"130\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (دختر)</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M150.71,-260.7C146.27,-255.4 141.93,-249.3 139,-243 135.73,-235.96 133.64,-227.85 132.31,-220.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"135.78,-219.74 130.93,-210.32 128.85,-220.71 135.78,-219.74\"/>\n<text text-anchor=\"middle\" x=\"160.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">ROOT</text>\n</g>\n<!-- 6 -->\n<g id=\"node3\" class=\"node\">\n<title>6</title>\n<text text-anchor=\"middle\" x=\"224\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (گل)</text>\n</g>\n<!-- 0&#45;&gt;6 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M179.33,-260.8C187.3,-248.7 198.11,-232.3 207.09,-218.67\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"210.11,-220.45 212.68,-210.18 204.26,-216.6 210.11,-220.45\"/>\n<text text-anchor=\"middle\" x=\"221.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">ROOT</text>\n</g>\n<!-- 1 -->\n<g id=\"node4\" class=\"node\">\n<title>1</title>\n<text text-anchor=\"middle\" x=\"27\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (۵)</text>\n</g>\n<!-- 2&#45;&gt;1 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M98.14,-184.27C80.74,-179.14 59.96,-170.41 46,-156 39.84,-149.65 35.64,-141.1 32.78,-132.87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"36.11,-131.76 29.93,-123.15 29.39,-133.74 36.11,-131.76\"/>\n<text text-anchor=\"middle\" x=\"86\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">NPREMOD</text>\n</g>\n<!-- 3 -->\n<g id=\"node5\" class=\"node\">\n<title>3</title>\n<text text-anchor=\"middle\" x=\"130\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (خوب)</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M130,-173.8C130,-162.16 130,-146.55 130,-133.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"133.5,-133.18 130,-123.18 126.5,-133.18 133.5,-133.18\"/>\n<text text-anchor=\"middle\" x=\"174.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">NPOSTMOD</text>\n</g>\n<!-- 4 -->\n<g id=\"node6\" class=\"node\">\n<title>4</title>\n<text text-anchor=\"middle\" x=\"242\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (مدرسه‌ی)</text>\n</g>\n<!-- 6&#45;&gt;4 -->\n<g id=\"edge6\" class=\"edge\">\n<title>6&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M227.64,-173.8C230.11,-162.16 233.41,-146.55 236.23,-133.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"239.72,-133.68 238.36,-123.18 232.87,-132.23 239.72,-133.68\"/>\n<text text-anchor=\"middle\" x=\"266.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">PREDEP</text>\n</g>\n<!-- 5 -->\n<g id=\"node7\" class=\"node\">\n<title>5</title>\n<text text-anchor=\"middle\" x=\"242\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (ما)</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M242,-86.8C242,-75.16 242,-59.55 242,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"245.5,-46.18 242,-36.18 238.5,-46.18 245.5,-46.18\"/>\n<text text-anchor=\"middle\" x=\"260\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">MOZ</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<DependencyGraph with 7 nodes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'زنگ‌ها برای که به صدا درمی‌آید؟' \n",
    "text = ' ۵ دختر خوب مدرسه‌ی ما گل'\n",
    "# text = 'گل باغ شهر ما'\n",
    "# text = '۳ سگ وحشی زشت علی زیبا به دریا رفتند.'\n",
    "# text = 'گل زیبای خانه‌ی مادربزرگ من'\n",
    "# text = 'سهام ۳ واحد افزایش یافت'\n",
    "tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "parser = hazm.DependencyParser (tagger = tagger, lemmatizer = lemmatizer, working_dir='../resources')\n",
    "print(parser.parse (  hazm.word_tokenize(text)).tree().pprint())\n",
    "parser.parse (  hazm.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "استفاده از FIS روی توییت ها \n",
    "افعال مثبت و منفی و کلمات و متفی در تنفی مثبت و اینا\n",
    "فراکاپ\n",
    "فرق گشتن دنبال keywordها در NP و VP\n",
    "یه کلمه‌ی مثل افت توی ADJP هم مهمه باید بررسی کنیم."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdff23a6f2e028a72d356297edfd2a8e740711d0ae6d770ba572ded92efe19b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
