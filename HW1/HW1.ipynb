{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_output(output_type, marker, span):\n",
    "    return {\"type\":output_type, \"marker\":marker, \"span\":repr(span)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Symbolizer:\n",
    "    def __init__(self):\n",
    "        self.symbols = []\n",
    "        with open('namad.txt') as symbols_file:\n",
    "            line = symbols_file.readline()\n",
    "            while line:\n",
    "                n = line.split('\\t')\n",
    "                self.symbols.append(n[0])\n",
    "                # TODO: we can add more information to a symbol here\n",
    "                # namad_description.append(n[1])\n",
    "                line = symbols_file.readline()\n",
    "\n",
    "    def symbolize(self, sentence):\n",
    "        output = []\n",
    "        for namad in self.symbols:\n",
    "            for m in re.finditer(namad, sentence):\n",
    "                output.append(create_output(\"نماد\", namad, m.span()))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [\n",
    "    \"مثبت\",\n",
    "    \"سود\",\n",
    "    \"صعود\",\n",
    "    \"افزایش\",\n",
    "    \"کاهش\",\n",
    "    \"رشد\",\n",
    "    \"ریزش\",\n",
    "    \"عرضه\",\n",
    "    \"افشا\",\n",
    "    \"اصلاح\",\n",
    "    \"نزول\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_extractor(noun_phrase):\n",
    "    output=[]\n",
    "    for event in events:\n",
    "        for m in re.finditer(event, noun_phrase.group()):\n",
    "                output.append(create_output(\"واقعه\", noun_phrase.group()[1:-3], noun_phrase.span()))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase(tagged_str, phrase=\"NP\"):\n",
    "    # for b in  re.finditer(r\"\\[[آ-یء چ]+NP\\]\", a):\n",
    "    phrases = []\n",
    "    for b in re.finditer(rf\"\\[[^\\]]*{phrase}\\]\", tagged_str):\n",
    "        phrases.append(b.group())\n",
    "        extracted_event = event_extractor(b)\n",
    "        if extracted_event:\n",
    "            print('event ectracted: ')\n",
    "            print(extracted_event)\n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: cannot open input model file: ../resources/postagger.model\n",
      "\t<No such file or directory>\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/home/ahura/.vscode/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"ارزش سهام شرکت نفت کاهش یافت\",\n",
    "    \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    \"ریزش بازار به دلیل حمله‌ی روسیه است.\",\n",
    "    \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "    \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    \"آمریکا باعث ریزش بازار شد\",\n",
    "    \"آمریکا موجب ریزش بازار شد\",\n",
    "    \"آمریکا دلیل ریزش بازار شد\",\n",
    "    \"کاهش قیمت سهم عجیب بود\",\n",
    "    \"قیمت زیاد شد\",\n",
    "    \"قیمت زیاد است\",\n",
    "    \"سهم قیمتش پایین است\",\n",
    "    \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "]\n",
    "\n",
    "tagger = hazm.POSTagger(model=\"../resources/postagger.model\")\n",
    "chunker = hazm.Chunker(model=\"../resources/chunker.model\")\n",
    "for sentence in sentences:\n",
    "    print(\"-------------\")\n",
    "    print(sentence)\n",
    "    print(\"symbols: \")\n",
    "    symbolizer = Symbolizer()\n",
    "    output = symbolizer.symbolize(sentence)\n",
    "    if output:\n",
    "        print(output)\n",
    "\n",
    "    tagged = tagger.tag(hazm.word_tokenize(sentence))\n",
    "    print(\"tagged: \")\n",
    "    print(tagged)\n",
    "    chunked_str = hazm.tree2brackets(chunker.parse(tagged))\n",
    "    print(\"chukned: \")\n",
    "    print(chunked_str)\n",
    "    get_phrase(chunked_str, phrase=\"NP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import hazm\n",
    "from __future__ import unicode_literals\n",
    "tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "print(tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))[3][0])\n",
    "# tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))\n",
    "tagger.tag(hazm.word_tokenize('کتاب خواندن را دوست داریم'))\n",
    "tagger.tag(hazm.word_tokenize('من درخت بلند را دیدم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد زبان نفهم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد سود ده'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "word = 'کتاب خواندن را دوست داریم'\n",
    "word = '۵ گل زیبا خریدم'\n",
    "word = 'سهام ۳ واحد افزایش یافت'\n",
    "word = 'هوا ۵ درجه گرم شد'\n",
    "chunker = hazm.Chunker(model='../resources/chunker.model')\n",
    "chunker = hazm.RuleBasedChunker()\n",
    "tagged = tagger.tag(hazm.word_tokenize(word))\n",
    "print(chunker.parse(tagged))\n",
    "print(hazm.tree2brackets(chunker.parse(tagged)))\n",
    "chunker.parse(tagged).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.ChunkParserI('a big cat was dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lst = [u'\\u200c', u'\\u5de5', 'ب\\u200cر', 'امی‌ن']\n",
    "print(lst)\n",
    "print('امی\\u200cن')\n",
    "print('امی‌ن')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "# nltk.download('universal_tagset')\n",
    "# text = nltk.word_tokenize(\"He is a good, tall boy\")\n",
    "text = nltk.word_tokenize(\"He is a little tall boy\")\n",
    "# text = nltk.word_tokenize(\"the little crazy cat sat on the mat\")\n",
    "# text = nltk.word_tokenize(\"amin is clever and good boy\")\n",
    "# text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text, tagset='universal')\n",
    "# nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "unchunked_sent = 'the little cat sat on the mat'\n",
    "rule1 = nltk.chunk.regexp.ChunkRule('<NN|DT>+',\n",
    "    'Chunk sequences of NN and DT')\n",
    "chunkparser = nltk.chunk.regexp.RegexpParser( [rule1] )\n",
    "chunkparser.parse(unchunked_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"learn php from guru99\"\n",
    "text = 'the little crazy cat is sitting on the mat'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} \n",
    "VP: {<VB.?>*}\n",
    "\"\"\"\n",
    "cp  = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Canceled. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text = 'زنگ‌ها برای که به صدا درمی‌آید؟' \n",
    "text = ' ۵ دختر خوب مدرسه‌ی ما گل'\n",
    "# text = 'گل باغ شهر ما'\n",
    "# text = '۳ سگ وحشی زشت علی زیبا به دریا رفتند.'\n",
    "# text = 'گل زیبای خانه‌ی مادربزرگ من'\n",
    "# text = 'سهام ۳ واحد افزایش یافت'\n",
    "tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "parser = hazm.DependencyParser (tagger = tagger, lemmatizer = lemmatizer, working_dir='../resources')\n",
    "print(parser.parse (  hazm.word_tokenize(text)).tree().pprint())\n",
    "parser.parse (  hazm.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "استفاده از FIS روی توییت ها \n",
    "افعال مثبت و منفی و کلمات و متفی در تنفی مثبت و اینا\n",
    "فراکاپ\n",
    "فرق گشتن دنبال keywordها در NP و VP\n",
    "یه کلمه‌ی مثل افت توی ADJP هم مهمه باید بررسی کنیم."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdff23a6f2e028a72d356297edfd2a8e740711d0ae6d770ba572ded92efe19b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
