{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argon2-cffi==21.3.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (21.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings==21.2.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (21.2.0)\n",
      "Requirement already satisfied: asttokens==2.0.5 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.0.5)\n",
      "Requirement already satisfied: attrs==21.4.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (21.4.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.10.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: bleach==4.1.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (4.1.0)\n",
      "Requirement already satisfied: cffi==1.15.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: debugpy==1.6.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: decorator==5.1.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: entrypoints==0.4 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.4)\n",
      "Requirement already satisfied: executing==0.8.3 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (0.8.3)\n",
      "Requirement already satisfied: fastjsonschema==2.15.3 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (2.15.3)\n",
      "Requirement already satisfied: ipykernel==6.12.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (6.12.1)\n",
      "Requirement already satisfied: ipython==8.2.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (8.2.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: jedi==0.18.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (0.18.1)\n",
      "Requirement already satisfied: Jinja2==3.1.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (3.1.1)\n",
      "Requirement already satisfied: jsonschema==4.4.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (4.4.0)\n",
      "Requirement already satisfied: jupyter-client==7.2.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (7.2.1)\n",
      "Requirement already satisfied: jupyter-core==4.9.2 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (4.9.2)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 23)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe==2.1.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (2.1.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.3 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (0.1.3)\n",
      "Requirement already satisfied: mistune==0.8.4 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (0.8.4)\n",
      "Requirement already satisfied: nbclient==0.5.13 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 27)) (0.5.13)\n",
      "Requirement already satisfied: nbconvert==6.4.5 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (6.4.5)\n",
      "Requirement already satisfied: nbformat==5.3.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 29)) (5.3.0)\n",
      "Requirement already satisfied: nest-asyncio==1.5.5 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 30)) (1.5.5)\n",
      "Requirement already satisfied: notebook==6.4.10 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 31)) (6.4.10)\n",
      "Requirement already satisfied: packaging==21.3 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (21.3)\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 33)) (1.5.0)\n",
      "Requirement already satisfied: parso==0.8.3 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (0.8.3)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 35)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 36)) (0.7.5)\n",
      "Requirement already satisfied: prometheus-client==0.13.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 37)) (0.13.1)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.29 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (3.0.29)\n",
      "Requirement already satisfied: psutil==5.9.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 39)) (5.9.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 41)) (0.2.2)\n",
      "Requirement already satisfied: pycparser==2.21 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 42)) (2.21)\n",
      "Requirement already satisfied: Pygments==2.11.2 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 43)) (2.11.2)\n",
      "Requirement already satisfied: pyparsing==3.0.7 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 44)) (3.0.7)\n",
      "Requirement already satisfied: pyrsistent==0.18.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 45)) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 46)) (2.8.2)\n",
      "Requirement already satisfied: pyzmq==22.3.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 47)) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash==1.8.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 48)) (1.8.0)\n",
      "Requirement already satisfied: six==1.16.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 49)) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.3.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 50)) (2.3.1)\n",
      "Requirement already satisfied: stack-data==0.2.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 51)) (0.2.0)\n",
      "Requirement already satisfied: terminado==0.13.3 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 52)) (0.13.3)\n",
      "Requirement already satisfied: testpath==0.6.0 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 53)) (0.6.0)\n",
      "Requirement already satisfied: tornado==6.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 54)) (6.1)\n",
      "Requirement already satisfied: traitlets==5.1.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 55)) (5.1.1)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 56)) (0.2.5)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from -r requirements.txt (line 57)) (0.5.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from ipython==8.2.0->-r requirements.txt (line 16)) (44.0.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from jsonschema==4.4.0->-r requirements.txt (line 20)) (5.6.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema==4.4.0->-r requirements.txt (line 20)) (3.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_output(output_type, marker, span):\n",
    "    return {\"type\":output_type, \"marker\":marker, \"span\":repr(span)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class Symbolizer:\n",
    "    def __init__(self):\n",
    "        self.symbols = []\n",
    "        with open('symbols.json') as symbols_file:\n",
    "            data = json.load(symbols_file)\n",
    "            for symbol in data['symbols']:\n",
    "                self.symbols.append(symbol)\n",
    "                # TODO: we can add more information to a symbol here\n",
    "                # namad_description.append(n[1])\n",
    "\n",
    "    def symbolize(self, sentence):\n",
    "        output = []\n",
    "        for namad in self.symbols:\n",
    "            for m in re.finditer(rf'\\b{namad}\\b', sentence):\n",
    "                output.append(create_output(\"نماد\", namad, m.span()))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [\n",
    "    \"سود\",\n",
    "    \"ضرر\",\n",
    "    \"مثبت\",\n",
    "    \"منفی\",\n",
    "    \"صعود\",\n",
    "    \"نزول\",\n",
    "    \"افزایش\",\n",
    "    \"کاهش\",\n",
    "    \"رشد\",\n",
    "    \"ریزش\",\n",
    "    \"افشا\",\n",
    "    \"اصلاح\",\n",
    "    \"افت\",\n",
    "    \"نوسان\",\n",
    "    \"نوسانات\",\n",
    "    \"سقوط\",\n",
    "    \"تقسیم سود\",\n",
    "    \"تعیین نرخ\",\n",
    "    \"عرضه اولیه\",\n",
    "    \"صف خرید\",\n",
    "    \"صف فروش\",\n",
    "    \"خروج سرمایه\",\n",
    "    \"افزایش سرمایه\"\n",
    "    \"خروج پول\",\n",
    "]\n",
    "official_expression = [\n",
    "    \"اطلاعیه\",\n",
    "    \"مجمع عمومی\",\n",
    "    \"گزارش\",\n",
    "]\n",
    "stock_expression = [\n",
    "    \"سهم رانتی\",\n",
    "    \"تحلیل تکنیکال\",\n",
    "    \"تحلیل فاندامنتال\",\n",
    "    \"کندل استیک\",\n",
    "    \"سیگنال\",\n",
    "    \"تکنیکال\",\n",
    "    \"کندل\",\n",
    "    \"تیک\",\n",
    "    \"سرشانه\",\n",
    "    \"مقاومت\",\n",
    "    \"حمایت\",\n",
    "    \"کراس\",\n",
    "    \"واگرایی\",\n",
    "    \"اندیکاتور\",\n",
    "]\n",
    "stock_characters =[\n",
    "    \"بازیگر\",\n",
    "    \"حقیقی\",\n",
    "    \"حقوقی\",\n",
    "    \"بازی‌گردان\",\n",
    "    \"نوسان‌گیر\",\n",
    "]\n",
    "reason_propositions = [\n",
    "    \"دلیل\",\n",
    "    \"باعث\",\n",
    "    \"موجب\",\n",
    "    \"برای\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_extractor(noun_phrase, phrase):\n",
    "    output=[]\n",
    "    for event in events:\n",
    "        for m in re.finditer(event, noun_phrase.group()):\n",
    "            body = noun_phrase.group()[1:-1-len(phrase)].strip()\n",
    "            for reason_proposition in reason_propositions:\n",
    "                if body.startswith(reason_proposition):\n",
    "                    body = body[len(reason_proposition):]\n",
    "            output.append(create_output(\"واقعه\", body, noun_phrase.span()))\n",
    "                \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase(tagged_str, phrase):\n",
    "    # for b in  re.finditer(r\"\\[[آ-یء چ]+NP\\]\", a):\n",
    "    phrases = []\n",
    "    for b in re.finditer(rf\"\\[[^\\]]*{phrase}\\]\", tagged_str):\n",
    "        phrases.append(b.group())\n",
    "        extracted_event = event_extractor(b, phrase)\n",
    "        if extracted_event:\n",
    "            print(extracted_event)\n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sentences = [\n",
    "    \"جریان اغاز معماملات فزر با ۱۵.۲ واحد تاثیر مثبت بر روند صعودی بازار فرابورس اثرگذار بود.\", # فزر توی نمادامون نیست\n",
    "    \"کارشناسان روند متعادل و گاها نزولی برای بازار سهام امروز پیش‌بینی کرده‌اند\", # اگر بعد از کارشناسان ، بزاریم درست تشخیص میده  وگرنه ابهام داره واقعا\n",
    "    \"بورس سه‌شنبه افت بیش از ۳۳ هزار واحدی داشت و در محدوده یک میلیون و ۴۴۶ هزار واحدی ایستاد.\", # افت بیش از... رو تشخیص نمیده\n",
    "    \"امروز چند سیگنال برای سهامداران وجود دارد.\", # چند\n",
    "    \"این تعیین نرخ به صورت رسمی، می‌تواند تاثیر مثبتی روی سوددهی شرکت‌ها داشته باشد\", #این\n",
    "    \"کارشناسان معتقدند که با توافق، بورس در میان مدت و بلند مدت روند صعودی به خود می‌گیرد.\", # بازم کاما\n",
    "    \"شرکت پالایش نفت شیراز با نماد شراز در مجمع عمومی فوق العاده ۱۲ ماهه افزایش سرمایه خود اعلام کرد\", # بازم کاما\n",
    "\n",
    "]\n",
    "correct_sentences = [\n",
    "    \"ارزش سهام شرکت نفت کاهش یافت\",\n",
    "    \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    \"ریزش بازار به دلیل حمله‌ی روسیه است.\",\n",
    "    \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "    \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    \"آمریکا باعث ریزش بازار شد\",\n",
    "    \"آمریکا موجب ریزش بازار شد\",\n",
    "    \"آمریکا دلیل ریزش بازار شد\",\n",
    "    \"کاهش قیمت سهم عجیب بود\",\n",
    "    \"قیمت زیاد شد\",\n",
    "    \"قیمت زیاد است\",\n",
    "    \"سهم قیمتش پایین است\",\n",
    "    \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "    \"برکت همین افشای ب باعث شد سهم سه درصد مثبت شود\",\n",
    "    \"بخاطر همین میگم پیگیر باشید\",\n",
    "    \"گزارش فعالیت ماهانه دوره ۱ ماهه منتهی به ۱۴۰۰/۰۹/۳۰ برای دیران منتشر شد\",\n",
    "    \"نرخ گاز خوراک پتروشیمی‌ها طبق فرمول ابلاغی وزارت نفت و با اعمال سقف نرخ گاز صادراتی، حداکثر پنج هزار تومان تعیین شده است\",\n",
    "    \"نرخ گاز مصرفی صنایع سیمان و سایر صنایع معادن ۱۰ درصد نرخ خوراک پتروشیمی‌ها تعیین شده است\",\n",
    "    \"کارشناسان می‌گویند تعیین شدن نرخ خوراک پتروشیمی‌ها به سودآوری شرکت‌ها در بلند مدت کمک می‌کند\",\n",
    "    \"قیمت دلار بازار آزاد به بیش از ۲۸ هزار تومان رسید. کارشناسان عقیده دارند افزایش مقطعی قیمت دلار، تاثیر منفی روی بازار سرمایه دارد\",\n",
    "    \"عرضه اولیه جدید می‌تواند، نقدینگی بیشتری به بازارا سرمایه وارد کند\",\"کارشناسان می‌گویند که اگر ارز ۴۲۰۰ تومانی حذف شود، صنعت دارو در کوتاه مدت نزولی می‌شود\",\n",
    "    \"شرکت سرمایه‌گذاری توسعه صنعت و تجارت اعلام کرد که در نظر دارد نسبت به فروش ملک خود واقع در شهر کرمان از طریق مزایده عمومی اقدام کند\",\n",
    "    \"کارشناسان می‌گویند کاهش نرخ سود بین بانکی می‌تواند نقدینگی جدید به بورس وارد کند.\",\n",
    "    \"کارشناسان می‌گویند که کاهش تامین مالی از بورس می‌تواند این بازار را صعودی و نقدینگی جدید وارد آن کند.\",\n",
    "    \"جلسه معارفه آسیاتک روز دوشنبه برگزار می‌شود. (سنا) احتمالا آسیاتک اولین عرضه اولیه بازار سرمایه در قرن جدید باشد.\",\n",
    "    \"پذیره‌نویسی نخستین صندوق سهامی با نماد سلام از روز دوشنه در بازار سهام آغاز می‌شود.\",\n",
    "    \"کارشناسان معتقدند که پذیره‌نویسی صندوق‌های جدید نقدینگی جریان نقدینگی در بازار را تقویت می‌کند\",\n",
    "    \"شرکت پالایش نفت اصفهان با نماد شپنا افزایش سرمایه خود را از محل سود انباشته ثبت کرد.\",\n",
    "    \"ولبهمن قرار است سرمایه ۸۰‌ میلیارد تومانی خود را به ۱۰۰ میلیارد تومان برسد.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = wrong_sentences+correct_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'نماد', 'marker': 'فرابورس', 'span': '(68, 75)'}]\n",
      "[{'type': 'واقعه', 'marker': '۱۵.۲ واحد تاثیر مثبت', 'span': '(37, 62)'}]\n",
      "[{'type': 'واقعه', 'marker': 'روند صعودی بازار فرابورس', 'span': '(71, 100)'}]\n",
      "[{'type': 'واقعه', 'marker': 'کارشناسان روند متعادل و گاها نزولی', 'span': '(0, 39)'}]\n",
      "[{'type': 'نماد', 'marker': 'بورس', 'span': '(0, 4)'}]\n",
      "[{'type': 'واقعه', 'marker': 'افت بیش', 'span': '(18, 30)'}]\n",
      "[{'type': 'واقعه', 'marker': 'چند سیگنال', 'span': '(13, 28)'}]\n",
      "[{'type': 'واقعه', 'marker': 'این تعیین نرخ', 'span': '(0, 18)'}]\n",
      "[{'type': 'واقعه', 'marker': 'تاثیر مثبتی', 'span': '(58, 74)'}]\n",
      "[{'type': 'واقعه', 'marker': 'سوددهی شرکت\\u200cها', 'span': '(84, 103)'}]\n",
      "[{'type': 'نماد', 'marker': 'بورس', 'span': '(31, 35)'}]\n",
      "[{'type': 'واقعه', 'marker': 'میان مدت و بلند مدت روند صعودی', 'span': '(70, 105)'}]\n",
      "[{'type': 'نماد', 'marker': 'شراز', 'span': '(30, 34)'}, {'type': 'نماد', 'marker': 'شیراز', 'span': '(16, 21)'}, {'type': 'نماد', 'marker': 'پالایش', 'span': '(5, 11)'}]\n",
      "[{'type': 'واقعه', 'marker': 'مجمع عمومی فوق العاده ۱۲ ماهه افزایش سرمایه خود', 'span': '(58, 110)'}]\n",
      "[{'type': 'نماد', 'marker': 'ارزش', 'span': '(0, 4)'}]\n",
      "[{'type': 'نماد', 'marker': 'وغدیر', 'span': '(36, 41)'}]\n",
      "[{'type': 'واقعه', 'marker': ' افت قیمت سهم وغدیر', 'span': '(33, 63)'}]\n",
      "[{'type': 'واقعه', 'marker': 'ریزش بازار', 'span': '(0, 15)'}]\n",
      "[{'type': 'واقعه', 'marker': 'یه اصلاح قیمتی', 'span': '(12, 31)'}]\n",
      "[{'type': 'واقعه', 'marker': 'ریزش', 'span': '(45, 54)'}]\n",
      "[{'type': 'واقعه', 'marker': 'خودشون افشا زدن', 'span': '(45, 65)'}]\n",
      "[{'type': 'نماد', 'marker': 'پرشیا', 'span': '(38, 43)'}]\n",
      "[{'type': 'واقعه', 'marker': 'رشد قیمت\\u200cها', 'span': '(0, 16)'}]\n",
      "[{'type': 'واقعه', 'marker': ' ریزش بازار', 'span': '(12, 34)'}]\n",
      "[{'type': 'واقعه', 'marker': ' ریزش بازار', 'span': '(12, 34)'}]\n",
      "[{'type': 'واقعه', 'marker': ' ریزش', 'span': '(12, 26)'}]\n",
      "[{'type': 'واقعه', 'marker': 'کاهش قیمت سهم عجیب', 'span': '(0, 23)'}]\n",
      "[{'type': 'نماد', 'marker': 'برکت', 'span': '(0, 4)'}]\n",
      "[{'type': 'واقعه', 'marker': 'برکت همین افشای ب', 'span': '(0, 22)'}]\n",
      "[{'type': 'واقعه', 'marker': 'مثبت', 'span': '(60, 71)'}]\n",
      "[{'type': 'نماد', 'marker': 'دیران', 'span': '(57, 62)'}]\n",
      "[{'type': 'واقعه', 'marker': 'سودآوری شرکت\\u200cها', 'span': '(74, 94)'}]\n",
      "[{'type': 'واقعه', 'marker': 'افزایش مقطعی قیمت دلار', 'span': '(121, 148)'}]\n",
      "[{'type': 'واقعه', 'marker': 'تاثیر منفی', 'span': '(151, 166)'}]\n",
      "[{'type': 'واقعه', 'marker': 'عرضه اولیه جدید', 'span': '(0, 20)'}]\n",
      "[{'type': 'نماد', 'marker': 'دارو', 'span': '(56, 60)'}]\n",
      "[{'type': 'نماد', 'marker': 'کرمان', 'span': '(95, 100)'}]\n",
      "[{'type': 'نماد', 'marker': 'بورس', 'span': '(67, 71)'}]\n",
      "[{'type': 'واقعه', 'marker': 'کاهش نرخ سود', 'span': '(29, 46)'}, {'type': 'واقعه', 'marker': 'کاهش نرخ سود', 'span': '(29, 46)'}]\n",
      "[{'type': 'نماد', 'marker': 'بورس', 'span': '(41, 45)'}]\n",
      "[{'type': 'واقعه', 'marker': 'کاهش تامین مالی', 'span': '(32, 52)'}]\n",
      "[{'type': 'واقعه', 'marker': 'صعودی', 'span': '(111, 123)'}]\n",
      "[{'type': 'نماد', 'marker': 'آسیاتک', 'span': '(12, 18)'}, {'type': 'نماد', 'marker': 'آسیاتک', 'span': '(59, 65)'}, {'type': 'نماد', 'marker': 'قرن', 'span': '(99, 102)'}]\n",
      "[{'type': 'واقعه', 'marker': 'آسیاتک اولین عرضه اولیه', 'span': '(91, 119)'}]\n",
      "[{'type': 'نماد', 'marker': 'شپنا', 'span': '(31, 35)'}, {'type': 'نماد', 'marker': 'پالایش', 'span': '(5, 11)'}]\n",
      "[{'type': 'واقعه', 'marker': 'افزایش سرمایه', 'span': '(51, 69)'}]\n",
      "[{'type': 'واقعه', 'marker': 'محل سود انباشته', 'span': '(98, 118)'}]\n",
      "[{'type': 'نماد', 'marker': 'ولبهمن', 'span': '(0, 6)'}]\n"
     ]
    }
   ],
   "source": [
    "tagger = hazm.POSTagger(model=\"../resources/postagger.model\")\n",
    "chunker = hazm.Chunker(model=\"../resources/chunker.model\")\n",
    "for sentence in new_sentences:\n",
    "    # print(\"-------------\")\n",
    "    # print(sentence)\n",
    "    # print(\"symbols: \")\n",
    "    symbolizer = Symbolizer()\n",
    "    output = symbolizer.symbolize(sentence)\n",
    "    if output:\n",
    "        print(output)\n",
    "\n",
    "    tagged = tagger.tag(hazm.word_tokenize(sentence))\n",
    "    # print(\"tagged: \")\n",
    "    # print(tagged)\n",
    "    chunked_str = hazm.tree2brackets(chunker.parse(tagged))\n",
    "    # print(\"chukned: \")\n",
    "    # print(chunked_str)\n",
    "    get_phrase(chunked_str, phrase=\"NP\")\n",
    "    get_phrase(chunked_str, phrase=\"ADJP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "می‌خوانیم\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('مرد', 'Ne'), ('سود', 'Ne'), ('ده', 'NUM')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hazm\n",
    "from __future__ import unicode_literals\n",
    "tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "print(tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))[3][0])\n",
    "# tagger.tag(hazm.word_tokenize('ما بسیار کتاب می‌خوانیم'))\n",
    "tagger.tag(hazm.word_tokenize('کتاب خواندن را دوست داریم'))\n",
    "tagger.tag(hazm.word_tokenize('من درخت بلند را دیدم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد زبان نفهم'))\n",
    "tagger.tag(hazm.word_tokenize('مرد سود ده'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP هوا/N) (NP ۵/NUM درجه/N) (ADJP گرم/AJ) (VP شد/V))\n",
      "[هوا NP] [۵ درجه NP] [گرم ADJP] [شد VP]\n"
     ]
    }
   ],
   "source": [
    "word = 'کتاب خواندن را دوست داریم'\n",
    "word = '۵ گل زیبا خریدم'\n",
    "word = 'سهام ۳ واحد افزایش یافت'\n",
    "word = 'هوا ۵ درجه گرم شد'\n",
    "chunker = hazm.Chunker(model='../resources/chunker.model')\n",
    "chunker = hazm.RuleBasedChunker()\n",
    "tagged = tagger.tag(hazm.word_tokenize(word))\n",
    "print(chunker.parse(tagged))\n",
    "print(hazm.tree2brackets(chunker.parse(tagged)))\n",
    "chunker.parse(tagged).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChunkParserI() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000010?line=2'>3</a>\u001b[0m nltk\u001b[39m.\u001b[39;49mChunkParserI(\u001b[39m'\u001b[39;49m\u001b[39ma big cat was dead\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: ChunkParserI() takes no arguments"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.ChunkParserI('a big cat was dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\u200c', '工', 'ب\\u200cر', 'امی\\u200cن']\n",
      "امی‌ن\n",
      "امی‌ن\n"
     ]
    }
   ],
   "source": [
    "lst = [u'\\u200c', u'\\u5de5', 'ب\\u200cر', 'امی‌ن']\n",
    "print(lst)\n",
    "print('امی\\u200cن')\n",
    "print('امی‌ن')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/fatemeh/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/fatemeh/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/fatemeh/uni/Term8/NLP/nlp_env/nltk_data'\n    - '/home/fatemeh/uni/Term8/NLP/nlp_env/share/nltk_data'\n    - '/home/fatemeh/uni/Term8/NLP/nlp_env/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=0'>1</a>\u001b[0m \u001b[39m# nltk.download('punkt')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=1'>2</a>\u001b[0m \u001b[39m# nltk.download('popular')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=2'>3</a>\u001b[0m \u001b[39m# nltk.download('universal_tagset')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=3'>4</a>\u001b[0m \u001b[39m# text = nltk.word_tokenize(\"He is a good, tall boy\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(\u001b[39m\"\u001b[39;49m\u001b[39mHe is a little tall boy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39m# text = nltk.word_tokenize(\"the little crazy cat sat on the mat\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=6'>7</a>\u001b[0m \u001b[39m# text = nltk.word_tokenize(\"amin is clever and good boy\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=7'>8</a>\u001b[0m \u001b[39m# text = nltk.word_tokenize(\"And now for something completely different\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/fatemeh/uni/Term8/NLP/NLP/HW1/HW1.ipynb#ch0000013?line=8'>9</a>\u001b[0m nltk\u001b[39m.\u001b[39mpos_tag(text, tagset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muniversal\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py:128\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=112'>113</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=113'>114</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=114'>115</a>\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=115'>116</a>\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=125'>126</a>\u001b[0m \u001b[39m    :type preserver_line: bool\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=126'>127</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=127'>128</a>\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=128'>129</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=129'>130</a>\u001b[0m             \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)]\n",
      "File \u001b[0;32m~/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py:94\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=83'>84</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=84'>85</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=85'>86</a>\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=86'>87</a>\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=91'>92</a>\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=92'>93</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=93'>94</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39m'\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(language))\n\u001b[1;32m     <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py?line=94'>95</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py:836\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=832'>833</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (resource_url,))\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=834'>835</a>\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=835'>836</a>\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=837'>838</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=838'>839</a>\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py:954\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=950'>951</a>\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=952'>953</a>\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=953'>954</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=954'>955</a>\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=955'>956</a>\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=956'>957</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py:675\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=672'>673</a>\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=673'>674</a>\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (sep, msg, sep)\n\u001b[0;32m--> <a href='file:///home/fatemeh/uni/Term8/NLP/nlp_env/lib/python3.8/site-packages/nltk/data.py?line=674'>675</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/home/fatemeh/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/fatemeh/uni/Term8/NLP/nlp_env/nltk_data'\n    - '/home/fatemeh/uni/Term8/NLP/nlp_env/share/nltk_data'\n    - '/home/fatemeh/uni/Term8/NLP/nlp_env/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "# nltk.download('universal_tagset')\n",
    "# text = nltk.word_tokenize(\"He is a good, tall boy\")\n",
    "text = nltk.word_tokenize(\"He is a little tall boy\")\n",
    "# text = nltk.word_tokenize(\"the little crazy cat sat on the mat\")\n",
    "# text = nltk.word_tokenize(\"amin is clever and good boy\")\n",
    "# text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text, tagset='universal')\n",
    "# nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected string or list of RegexpChunkParsers for the grammar.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=0'>1</a>\u001b[0m unchunked_sent \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mthe little cat sat on the mat\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=1'>2</a>\u001b[0m rule1 \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mchunk\u001b[39m.\u001b[39mregexp\u001b[39m.\u001b[39mChunkRule(\u001b[39m'\u001b[39m\u001b[39m<NN|DT>+\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mChunk sequences of NN and DT\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=3'>4</a>\u001b[0m chunkparser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mchunk\u001b[39m.\u001b[39;49mregexp\u001b[39m.\u001b[39;49mRegexpParser( [rule1] )\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/amin/amin/University/Term8/NLP/HW1/notebook.ipynb#ch0000010?line=4'>5</a>\u001b[0m chunkparser\u001b[39m.\u001b[39mparse(unchunked_sent)\n",
      "File \u001b[0;32m~/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py:1145\u001b[0m, in \u001b[0;36mRegexpParser.__init__\u001b[0;34m(self, grammar, root_label, loop, trace)\u001b[0m\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1142'>1143</a>\u001b[0m \u001b[39mfor\u001b[39;00m elt \u001b[39min\u001b[39;00m grammar:\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1143'>1144</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(elt, RegexpChunkParser):\n\u001b[0;32m-> <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1144'>1145</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(type_err)\n\u001b[1;32m   <a href='file:///home/amin/amin/University/Term8/NLP/HW1/.venv/lib/python3.9/site-packages/nltk/chunk/regexp.py?line=1145'>1146</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stages \u001b[39m=\u001b[39m grammar\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected string or list of RegexpChunkParsers for the grammar."
     ]
    }
   ],
   "source": [
    "unchunked_sent = 'the little cat sat on the mat'\n",
    "rule1 = nltk.chunk.regexp.ChunkRule('<NN|DT>+',\n",
    "    'Chunk sequences of NN and DT')\n",
    "chunkparser = nltk.chunk.regexp.RegexpParser( [rule1] )\n",
    "chunkparser.parse(unchunked_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'little', 'crazy', 'cat', 'is', 'sitting', 'on', 'the', 'mat']\n",
      "[('the', 'DT'), ('little', 'JJ'), ('crazy', 'JJ'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]\n",
      "(S\n",
      "  (NP the/DT little/JJ crazy/JJ cat/NN)\n",
      "  (VP is/VBZ sitting/VBG)\n",
      "  on/IN\n",
      "  (NP the/DT mat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"learn php from guru99\"\n",
    "text = 'the little crazy cat is sitting on the mat'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} \n",
    "VP: {<VB.?>*}\n",
    "\"\"\"\n",
    "cp  = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hazm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19438/3424471780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# text = 'گل زیبای خانه‌ی مادربزرگ من'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'سهام ۳ واحد افزایش یافت'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhazm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../resources/postagger.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhazm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhazm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyParser\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../resources'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hazm' is not defined"
     ]
    }
   ],
   "source": [
    "# text = 'زنگ‌ها برای که به صدا درمی‌آید؟' \n",
    "# text = ' ۵ دختر خوب مدرسه‌ی ما گل'\n",
    "# text = 'گل باغ شهر ما'\n",
    "# text = '۳ سگ وحشی زشت علی زیبا به دریا رفتند.'\n",
    "# text = 'گل زیبای خانه‌ی مادربزرگ من'\n",
    "text = 'سهام ۳ واحد افزایش یافت'\n",
    "tagger = hazm.POSTagger(model='../resources/postagger.model')\n",
    "lemmatizer = hazm.Lemmatizer()\n",
    "parser = hazm.DependencyParser (tagger = tagger, lemmatizer = lemmatizer, working_dir='../resources')\n",
    "# print(parser.parse(hazm.word_tokenize(text)).tree().pprint())\n",
    "parser.parse(hazm.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "استفاده از FIS روی توییت ها \n",
    "افعال مثبت و منفی و کلمات و متفی در تنفی مثبت و اینا\n",
    "فراکاپ\n",
    "فرق گشتن دنبال keywordها در NP و VP\n",
    "یه کلمه‌ی مثل افت توی ADJP هم مهمه باید بررسی کنیم."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdff23a6f2e028a72d356297edfd2a8e740711d0ae6d770ba572ded92efe19b3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
