{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy_stanza\n",
    "# import dadmatools.pipeline.language as language\n",
    "# from dadmatools.models.normalizer import Normalizer\n",
    "from parsivar import Normalizer as ParsivarNormalizer\n",
    "import json \n",
    "import pytse_client as tse\n",
    "import warnings\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.install_corenlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_nlp = spacy_stanza.load_pipeline(\"fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DadmaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here lemmatizer and pos tagger will be loaded\n",
    "# # as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "# pips = 'ner, pos, dep, cons, chunk, lem, tok' \n",
    "# dadma_nlp = language.Pipeline(pips)\n",
    "\n",
    "# # you can see the pipeline with this code\n",
    "# print(dadma_nlp.analyze_pipes(pretty=True))\n",
    "\n",
    "# # dadma_doc is an SpaCy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dadma Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer1 = Normalizer(\n",
    "#     full_cleaning=False,\n",
    "#     unify_chars=True,\n",
    "#     refine_punc_spacing=True,\n",
    "#     remove_extra_space=True,\n",
    "#     remove_puncs=False,\n",
    "#     remove_html=False,\n",
    "#     remove_stop_word=False,\n",
    "#     replace_email_with=\"<EMAIL>\",\n",
    "#     replace_number_with=None,\n",
    "#     replace_url_with=\"<URL\",\n",
    "#     replace_mobile_number_with=\"<MOBILE_NUMBER>\",\n",
    "#     replace_emoji_with=\"<EMOJI>\",\n",
    "#     replace_home_number_with=\"<HOME_NUMBER>\"\n",
    "# )\n",
    "\n",
    "normalizer2 = ParsivarNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Write Symbols "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download And Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# symbol_to_price_records = tse.download(symbols=\"all\", write_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = list(symbol_to_price_records.keys())\n",
    "# symbols = sorted(symbols, key=str.lower)\n",
    "\n",
    "\n",
    "# # with open('symbols.json', 'w',) as file:\n",
    "#     # json.dump({\"symbols\": symbols}, file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [\"Symbol\", \"Corp. Title\", \"TSE URL\", \"Group Name\"]\n",
    "\n",
    "# symbols_info_list = []\n",
    "# for symbol in tqdm(symbol_to_price_records.keys()):\n",
    "#     ticker = tse.Ticker(symbol, adjust=True, )\n",
    "\n",
    "#     row = [symbol, ticker.title, ticker.url, ticker.group_name]\n",
    "\n",
    "#     symbols_info_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(symbols_info_list, columns=columns)\n",
    "# df.to_csv(\"symbols_info.csv\", \n",
    "          # index=False,\n",
    "        #   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"نماد برکت افزایش و نماد کگل کاهش یافت.\",\n",
    "    \"برکت امروز اطلاعیه‌ای مهم منتشر میکند.\",\n",
    "    \"نماد برکت امروز عرضه‌ی اولیه خیلی خوبی داره.\",\n",
    "    \"نماد برکت امروز عرضه ی اولیه خیلی خوبی داره.\",\n",
    "    \"نماد برکت امروز عرضه ی اولیه دارد.\",\n",
    "    \"نماد برکت امروز عرضه ی اولیه خیلی خوبی داره.\",\n",
    "    \"عرضه های اولیه امروز خوب هستند.\",\n",
    "    \"عرضه اولیه‌های امروز خوب هستند.\",\n",
    "    \"برکت همین افشای ب باعث می شود سهم سه درصد مثبت بشود. بخاطر همین میگم پیگیر باشید.\",\n",
    "    'روز چهارشنبه یک دفعه برای خودشون افشا زدن.',\n",
    "    \"سهام وغدیر و خزر کاهش یافت.\",\n",
    "    \"برای خودشون ی افشایی زدن.\",\n",
    "    \"آ س پ امروز بالا رفت.\",\n",
    "    \"این سهم تا کنون سودهای زیادی داده است.\",\n",
    "    \"این سهم تا کنون سود های زیادی داده است.\",\n",
    "    # \"ریزش بازار به دلیل حمله‌ی روسیه هست.\",\n",
    "    # \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    # \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    # \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "    # \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    # \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    # \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    # \"آمریکا باعث ریزش بازار شد\",\n",
    "    # \"آمریکا موجب ریزش بازار شد\",\n",
    "    # \"آمریکا دلیل ریزش بازار شد\",\n",
    "    # \"کاهش قیمت سهم عجیب بود\",\n",
    "    # \"قیمت زیاد شد\",\n",
    "    # \"قیمت زیاد است\",\n",
    "    # \"به کتابخانه رفتم.\",\n",
    "    # \"به کتابخانه رفت.\",\n",
    "    # \"پول در جیب من است.\",\n",
    "    # \"سهم قیمتش پایین است\",\n",
    "    # \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "]\n",
    "\n",
    "stock_terms = [\n",
    "    'ضرر',\n",
    "    'سود',\n",
    "    'اطلاعیه',\n",
    "    'افزایش سرمایه',\n",
    "    'تقسیم سود',\n",
    "    'دامنه نوسان',\n",
    "    'نوسان',\n",
    "    'سهم رانتی',\n",
    "    'عرضه اولیه',\n",
    "    'افشا',\n",
    "    'فعالیت ماهانه',\n",
    "    'فعالیت سالانه',\n",
    "    'کاهش',\n",
    "    'افزایش',\n",
    "    'بالا',\n",
    "    'پایین'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher: Matcher = Matcher(stanza_nlp.vocab, validate=True)\n",
    "\n",
    "patterns = []\n",
    "for term in stock_terms:\n",
    "    term_tokens = stanza_nlp.tokenizer(term)\n",
    "    # y = r'([\\u200c|ی|ا|]{0,4})|(ی\\s)'\n",
    "    # y = r'[\\u200c|ی|ا]{,4}'\n",
    "    \n",
    "    pre_det = [\"این\", \"هر\", \"چند\", \"چندین\", \"آن\", \"همین\", \"همان\", \"چنین\", \"چنین\"]\n",
    "    \n",
    "    if len(term_tokens) == 1:\n",
    "        tok1_text = term_tokens[0].text\n",
    "        pattern = [\n",
    "            {'TEXT': {\"IN\": pre_det}, \"OP\": \"?\"},\n",
    "            {'TEXT': {\"REGEX\": tok1_text}},\n",
    "        ]\n",
    "\n",
    "    elif len(term_tokens) == 2:\n",
    "        tok1_text = term_tokens[0].text\n",
    "        tok2_text = term_tokens[1].text\n",
    "        pattern = [\n",
    "            {'TEXT': {\"IN\": pre_det}, \"OP\": \"?\"},\n",
    "            {'TEXT': {\"REGEX\": tok1_text}},\n",
    "            {'TEXT': {\"IN\": [\"ای\", \"ی\", \"ها\", \"های\"]}, \"OP\": \"?\"},\n",
    "            {'TEXT': {\"IN\": pre_det}, \"OP\": \"?\"},\n",
    "            {'TEXT': {\"REGEX\": tok2_text}},\n",
    "        ]\n",
    "        \n",
    "    patterns.append(pattern)\n",
    "\n",
    "matcher.add(\"TERM\", patterns, greedy=\"LONGEST\")\n",
    "\n",
    "# for text in texts:\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(\"Text: \" + text)\n",
    "#     # for id, start, end in matcher(doc):\n",
    "#     #     print(stanza_nlp.vocab.strings[id], doc[start:end])\n",
    "\n",
    "#     for id, start, end in matcher(doc): \n",
    "#         print(id, start, end)\n",
    "#         m = doc[start: end]\n",
    "#         print(m)\n",
    "    \n",
    "    # term_spans = list(map(lambda match: doc.char_span(match[1], match[2], label=\"TERM\"),\n",
    "#                         #   matcher(doc)))\n",
    "    \n",
    "#     # print(term_spans)\n",
    "\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_subtree_to_str(token, not_in=[], remove_ADP=False):\n",
    "    extracted_event = ''\n",
    "    start = None\n",
    "    end = None\n",
    "    for x in token.subtree:\n",
    "        if x.pos_ in not_in:\n",
    "            continue\n",
    "        # print('1', x, x.pos_, x.head)\n",
    "        if remove_ADP and x.pos_ == 'ADP':\n",
    "            if x.head.text == token.text:\n",
    "                continue\n",
    "\n",
    "        extracted_event += str(x) + ' '\n",
    "        if start is None:\n",
    "            start = x.idx\n",
    "        end = x.idx + len(x.text)\n",
    "    extracted_event = extracted_event[:-1]\n",
    "    # print('--- in convert s2s ', extracted_event)\n",
    "    return extracted_event, start, end\n",
    "\n",
    "\n",
    "def create_output(output_type, marker, span, **kwargs):\n",
    "    defaults = {\n",
    "        \"type\":output_type, \n",
    "        \"marker\":marker, \n",
    "        \"span\":repr(span),\n",
    "        }\n",
    "    return {\n",
    "        **defaults,\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "import re\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "\n",
    "\n",
    "TERM = \"TERM\"\n",
    "SYMBOL = \"SYMBOL\"\n",
    "\n",
    "for text_index, text in enumerate(texts):\n",
    "    # text = normalizer1.normalize(text)\n",
    "    text = normalizer2.normalize(text)\n",
    "\n",
    "    print(f'Text {text_index + 1}: ...')\n",
    "    print(f\"Normalized Text: {text}\")\n",
    "\n",
    "    doc = stanza_nlp(text)\n",
    "\n",
    "    expression = \"|\".join(symbols)\n",
    "    \n",
    "    symbol_spans = list(map(lambda match: doc.char_span(*match.span(), label=SYMBOL),\n",
    "                            re.finditer(expression, text)))\n",
    "\n",
    "    symbol_spans = list(filter(lambda span: span is not None,\n",
    "                               symbol_spans))\n",
    "\n",
    "    term_spans = list(map(lambda match: Span(doc, match[1], match[2], label=TERM),\n",
    "                          matcher(doc)))\n",
    "    \n",
    "    term_spans = list(filter(lambda span: span is not None,\n",
    "                             term_spans))\n",
    "        \n",
    "    spans = symbol_spans + term_spans\n",
    "    \n",
    "    print(term_spans)\n",
    "    \n",
    "    doc.set_ents(spans)\n",
    "    \n",
    "    with doc.retokenize() as retokenizer:\n",
    "        attrs = {'POS': \"NOUN\"}\n",
    "        for span in symbol_spans:\n",
    "            retokenizer.merge(span, attrs)\n",
    "\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        attrs = {'POS': \"NOUN\"}\n",
    "        for span in term_spans:\n",
    "            retokenizer.merge(span, attrs)\n",
    "\n",
    "    # print('Entities: ...')\n",
    "    # displacy.render(doc, style='ent')\n",
    "    \n",
    "    # print('Tokens: ...')\n",
    "    # print(list(doc[:]))\n",
    "    \n",
    "    print('Dependency Tree: ...')\n",
    "    displacy.render(doc, style='dep')\n",
    "    \n",
    "    term_ents = list(filter(lambda ent: ent.label_ == TERM,\n",
    "                     doc.ents))\n",
    "    \n",
    "    from spacy.symbols import nsubj, VERB, NOUN\n",
    "    \n",
    "    for term_ent in term_ents:\n",
    "        token = term_ent[0]\n",
    "        \n",
    "        if token.dep_ == 'compound:lvc':\n",
    "            print('type is: coumpound:lvd')\n",
    "            print('parent is: ', token.head)\n",
    "            if token.head.pos == VERB:\n",
    "                print('parent was a verb')\n",
    "                print('children of parent: ', list(token.head.children))\n",
    "                subject = None\n",
    "                for child in token.head.children:\n",
    "                    if child.dep == nsubj:\n",
    "                        print('Found a subject, outputing subtree of subject as subject')\n",
    "                        subject, start_subj, end_subj = convert_subtree_to_str(child)\n",
    "                        break\n",
    "                    else:\n",
    "                        print('NOT IMP - child dep is: ', child.dep_)\n",
    "                        pass\n",
    "            # else:e\n",
    "            # print('didn\\'t found any subj')\n",
    "\n",
    "                start = token.idx\n",
    "                end = token.idx + len(token.text) + 1 + len(token.head.text)\n",
    "                extracted_event = token.text + ' ' + token.head.text #TODO: check\n",
    "                if subject:\n",
    "                    print(create_output('واقعه', extracted_event, (start,end), subject=subject))\n",
    "                else:\n",
    "                    print(create_output('واقعه', extracted_event, (start,end)))\n",
    "\n",
    "            # elif token.dep_ == 'nmod':\n",
    "            #     print('type is: nmod')\n",
    "            #     print('parent is: ', token.head)\n",
    "\n",
    "            #     print('finding parent that is not nmod')\n",
    "            #     main_noun = token\n",
    "            #     while main_noun.dep_ == 'nmod' and main_noun.head.pos == NOUN:\n",
    "            #       main_noun = main_noun.head\n",
    "            #     if main_noun.pos != NOUN:\n",
    "            #       continue\n",
    "\n",
    "            #     print('Found a Noun parent that is not nmod')\n",
    "            #     extracted_event, start, end = convert_subtree_to_str(token.head, not_in=['ADP'])\n",
    "\n",
    "        elif token.dep_ == 'nmod' or token.dep_ == 'amod':\n",
    "            print('type is: ', token.dep_)\n",
    "            print('parent is: ', token.head)\n",
    "\n",
    "            print('finding parent that is not nmod or amod')\n",
    "            main_noun = token\n",
    "            # print('1', main_noun, main_noun.dep_, main_noun.head.pos_)\n",
    "            while (main_noun.dep_ == 'nmod' or main_noun.dep_ == 'amod') and main_noun.head.pos == NOUN:\n",
    "                main_noun = main_noun.head\n",
    "                # print('2', main_noun, main_noun.dep_, main_noun.head.pos_)\n",
    "            if main_noun.pos != NOUN:\n",
    "                continue\n",
    "\n",
    "            print('Found a Noun parent that is not nmod or amod')\n",
    "            extracted_event, start, end = convert_subtree_to_str(main_noun, remove_ADP=True)\n",
    "            # extracted_event, start, end = convert_subtree_to_str(main_noun, not_in=['ADP'])\n",
    "\n",
    "            print(create_output('واقعه', extracted_event, (start,end)))\n",
    "\n",
    "        else:\n",
    "            print('type is: other')\n",
    "            extracted_event, start, end = convert_subtree_to_str(token, remove_ADP=True)\n",
    "\n",
    "            # print(text[start:end])\n",
    "            print(create_output('واقعه', extracted_event, (start,end)))\n",
    "\n",
    "           \n",
    "    \n",
    "    print('-' * 75)\n",
    "\n",
    "# stock_terms.extend(extended_stock_terms)\n",
    "\n",
    "# for text in texts:\n",
    "    # doc = stanza_nlp(text)\n",
    "    # print(\"Text: \" + text)\n",
    "    # for id, start, end in matcher(doc):\n",
    "    #     print(stanza_nlp.vocab.strings[id], doc[start:end])\n",
    "\n",
    "    # pattern = \"|\".join(stock_terms)\n",
    "\n",
    "    # print(\"Text: \" + text)\n",
    "    # for match in re.finditer(pattern, text):\n",
    "        # s, e = match.span()\n",
    "        # print(f\"Match Found: ({text[s:e]})\")\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "# stock_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol Tagging Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t_index, text in enumerate(texts): \n",
    "#     text = normalizer1.normalize(text)\n",
    "#     text = normalizer2.normalize(text)\n",
    "    \n",
    "#     print(f\"Normalized Text: {text}\")\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "    \n",
    "#     # for m_index, (match_id, start, end) in enumerate(matches):\n",
    "#     #     span:Span = Span(doc, start, end, label=match_id)\n",
    "#     #     print(f\"Match {m_index + 1}: {span.text}, {span.label_}\")\n",
    "#     #     if span.label_ == \"term_pattern\":\n",
    "#     #         print(list(span.subtree))\n",
    "        \n",
    "#     # for s_index, sentence in enumerate(doc.sents):\n",
    "#     #     print(f'Sentence {s_index + 1}: ...')\n",
    "\n",
    "#     print(doc.ents)\n",
    "\n",
    "#     displacy.render(doc, style=\"ent\")\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "    \n",
    "# # stanza_nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#             # print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         for child1 in sentence.root.children:\n",
    "#             print(f\"{child1.text}, {child1.pos_}, {child1.tag_} {child1.dep_}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "#         displacy.render(sentence, style=\"dep\")\n",
    "        \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dadma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "\n",
    "    \n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = dadma_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         # for child1 in sentence.root.children:\n",
    "#             # print(f\"{child1.text} Span is: {children_string(child1.subtree)}\")\n",
    "            \n",
    "#         # print(f\"constituency: {doc._.constituency}\")\n",
    "#         print(f\"chunks: {doc._.chunks}\")\n",
    "        \n",
    "    \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display Matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "# def collect_sents(matcher, doc, i, matches):\n",
    "#     match_id, start, end = matches[i]\n",
    "#     span = doc[start:end]  # Matched span\n",
    "#     sent = span.sent  # Sentence containing matched span\n",
    "#     # Append mock entity for match in displaCy style to matched_sents\n",
    "#     # get the match span by ofsetting the start and end of the span with the\n",
    "#     # start and end of the sentence in the doc\n",
    "#     match_ents = [{\n",
    "#         \"start\": span.start_char - sent.start_char,\n",
    "#         \"end\": span.end_char - sent.start_char,\n",
    "#         \"label\": \"MATCH\",\n",
    "#     }]\n",
    "#     matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "# pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "#            {\"POS\": \"ADJ\"}]\n",
    "# matcher.add(\"FacebookIs\", [pattern], on_match=collect_sents)  # add pattern\n",
    "# doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
    "# matches = matcher(doc)\n",
    "\n",
    "# # Serve visualization of sentences containing match with displaCy\n",
    "# # set manual=True to make displaCy render straight from a dictionary\n",
    "# # (if you're not running the code within a Jupyer environment, you can\n",
    "# # use displacy.serve instead)\n",
    "# displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = PhraseMatcher(nlp.vocab)\n",
    "# terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# # Only run nlp.make_doc to speed things up\n",
    "# patterns = [nlp.make_doc(text) for text in terms]\n",
    "# matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "# doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "#           \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "# matches = matcher(doc)\n",
    "# for match_id, start, end in matches:\n",
    "#     span = doc[start:end]\n",
    "#     print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# nlp = English()\n",
    "# matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "# patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
    "# matcher.add(\"Names\", patterns)\n",
    "\n",
    "# doc = nlp(\"angela merkel and us president barack Obama\")\n",
    "# for match_id, start, end in matcher(doc):\n",
    "#     print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher: Matcher = Matcher(dadma_nlp.vocab, validate=True)\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         for token in sentence:\n",
    "#             print(f\"{token.text:10}, {token.pos_:10}, {token.tag_:10}, {token.dep_}\")\n",
    "                \n",
    "        \n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# for index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     try:\n",
    "        \n",
    "#         doc = dadma_nlp(text)\n",
    "        \n",
    "#         # print(f'sentence {index + 1}: ...')\n",
    "#         # for token in doc:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "#         #     print(\"\\n\")\n",
    "        \n",
    "#         sentences = doc._.sentences\n",
    "        \n",
    "#         # for sentence in sentences:\n",
    "#         #     sentence_text = sentence.text\n",
    "#         #     for token in sentence:\n",
    "#         #         token_text = token.text\n",
    "#         #         lemma = token.lemma_ ## this has value only if lem is called\n",
    "#         #         pos_tag = token.pos_ ## this has value only if pos is called\n",
    "#         #         dep = token.dep_ ## this has value only if dep is called\n",
    "#         #         dep_arc = token._.dep_arc ## this has value only if dep is called\n",
    "#         #         print(token_text, pos_tag, dep, dep_arc)\n",
    "#         #         if token.pos_ == \"AUX\":\n",
    "#         #             token.pos_ = \"VERB\"\n",
    "                \n",
    "#         sent_constituency = doc._.constituency ## this has value only if cons is called\n",
    "#         sent_chunks = doc._.chunks ## this has value only if cons is called\n",
    "#         # ners = doc._.ners ## this has value only if ner is called\n",
    "#         # print(sent_constituency)\n",
    "#         print(sent_chunks)\n",
    "        \n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "#         displacy.render(doc, style=\"dep\")\n",
    "        \n",
    "#     except Exception:\n",
    "        \n",
    "#         print(f\"ERRR {text}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pattern Matching](https://spacy.io/usage/spacy-101#architecture-matchers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94bffb7bb37b548aa5e0061bb472f4819fbdd606d7ef446f7f021a1efc0be190"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('DataEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
