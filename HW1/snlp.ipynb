{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-04-10 05:09:40 WARNING: Directory /home/ahura/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy_stanza\n",
    "\n",
    "stanza.install_corenlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json: 142kB [00:00, 20.3MB/s]                    \n",
      "2022-04-10 05:09:41 INFO: Downloading default packages for language: fa (Persian)...\n",
      "2022-04-10 05:09:43 INFO: File exists: /home/ahura/stanza_resources/fa/default.zip.\n",
      "2022-04-10 05:09:47 INFO: Finished downloading models and saved to /home/ahura/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 05:09:47 INFO: Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "=======================\n",
      "\n",
      "2022-04-10 05:09:47 INFO: Use device: gpu\n",
      "2022-04-10 05:09:47 INFO: Loading: tokenize\n",
      "2022-04-10 05:09:52 INFO: Loading: mwt\n",
      "2022-04-10 05:09:52 INFO: Loading: pos\n",
      "2022-04-10 05:09:53 INFO: Loading: lemma\n",
      "2022-04-10 05:09:53 INFO: Loading: depparse\n",
      "2022-04-10 05:09:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp = spacy_stanza.load_pipeline(\"fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DadmaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fa_tokenizer exists in /home/ahura/.pernlp/fa_tokenizer.pt\n",
      "Model fa_mwt exists in /home/ahura/.pernlp/fa_mwt.pt\n",
      "Model fa_lemmatizer exists in /home/ahura/.pernlp/fa_lemmatizer.pt\n",
      "Model parsbert exists in /home/ahura/.pernlp/parsbert.tar.gz\n",
      "Model dependencyparser exists in /home/ahura/.pernlp/dependencyparser.pt\n",
      "2022-04-10 05:10:10,207 loading file /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages/dadmatools/saved_models/dependencyparser/dependencyparser.pt\n",
      "Model parsbert exists in /home/ahura/.pernlp/parsbert.tar.gz\n",
      "Model postagger exists in /home/ahura/.pernlp/postagger.pt\n",
      "2022-04-10 05:10:12,710 loading file /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages/dadmatools/saved_models/postagger/postagger.pt\n",
      "Model fa_constituency exists in /home/ahura/.pernlp/fa_constituency.pt\n",
      "Model ner exists in /home/ahura/.pernlp/ner.tar.gz\n",
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component            Assigns       Requires   Scores   Retokenizes\n",
      "-   ------------------   -----------   --------   ------   -----------\n",
      "0   tokenizer                                              True       \n",
      "                                                                      \n",
      "1   lemmatize            token.lemma                       False      \n",
      "                                                                      \n",
      "2   dependancyparser     token.dep                         False      \n",
      "                                                                      \n",
      "3   postagger            token.pos                         False      \n",
      "                                                                      \n",
      "4   chunking                                               False      \n",
      "                                                                      \n",
      "5   constituencyparser                                     False      \n",
      "                                                                      \n",
      "6   ners                                                   False      \n",
      "\n",
      "\u001b[38;5;2m✔ No problems found.\u001b[0m\n",
      "{'summary': {'tokenizer': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': True}, 'lemmatize': {'assigns': ['token.lemma'], 'requires': [], 'scores': [], 'retokenizes': False}, 'dependancyparser': {'assigns': ['token.dep'], 'requires': [], 'scores': [], 'retokenizes': False}, 'postagger': {'assigns': ['token.pos'], 'requires': [], 'scores': [], 'retokenizes': False}, 'chunking': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': False}, 'constituencyparser': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': False}, 'ners': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': False}}, 'problems': {'tokenizer': [], 'lemmatize': [], 'dependancyparser': [], 'postagger': [], 'chunking': [], 'constituencyparser': [], 'ners': []}, 'attrs': {'token.lemma': {'assigns': ['lemmatize'], 'requires': []}, 'token.pos': {'assigns': ['postagger'], 'requires': []}, 'token.dep': {'assigns': ['dependancyparser'], 'requires': []}}}\n"
     ]
    }
   ],
   "source": [
    "import dadmatools.pipeline.language as language\n",
    "\n",
    "# here lemmatizer and pos tagger will be loaded\n",
    "# as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "pips = 'ner, pos, dep, cons, chunk, lem, tok' \n",
    "dadma_nlp = language.Pipeline(pips)\n",
    "\n",
    "# you can see the pipeline with this code\n",
    "print(dadma_nlp.analyze_pipes(pretty=True))\n",
    "\n",
    "# doc is an SpaCy object\n",
    "# doc = dadma_nlp('از قصهٔ کودکیشان که می‌گفت، گاهی حرص می‌خورد!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dadma Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadmatools.models.normalizer import Normalizer\n",
    "\n",
    "normalizer = Normalizer(\n",
    "    full_cleaning=False,\n",
    "    unify_chars=True,\n",
    "    refine_punc_spacing=True,\n",
    "    remove_extra_space=True,\n",
    "    remove_puncs=False,\n",
    "    remove_html=False,\n",
    "    remove_stop_word=False,\n",
    "    replace_email_with=\"<EMAIL>\",\n",
    "    replace_number_with=None,\n",
    "    replace_url_with=\"<URL\",\n",
    "    replace_mobile_number_with=\"<MOBILE_NUMBER>\",\n",
    "    replace_emoji_with=\"<EMOJI>\",\n",
    "    replace_home_number_with=\"<HOME_NUMBER>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"ارزش سهام شرکت نفت کاهش یافت\",\n",
    "    # \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    # \"ریزش بازار به دلیل حمله‌ی روسیه است.\",\n",
    "    # \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    # \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "    # \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    # \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    # \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    # \"آمریکا باعث ریزش بازار شد\",\n",
    "    # \"آمریکا موجب ریزش بازار شد\",\n",
    "    # \"آمریکا دلیل ریزش بازار شد\",\n",
    "    # \"کاهش قیمت سهم عجیب بود\",\n",
    "    # \"قیمت زیاد شد\",\n",
    "    # \"قیمت زیاد است\",\n",
    "    # \"سهم قیمتش پایین است\",\n",
    "    # \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# for index, text in enumerate(texts):\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'sentence {index + 1}: ...')\n",
    "#     for token in doc:\n",
    "#         print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#     displacy.render(list(doc.sents), style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 05:40:19 INFO: [Ensembling dict with seq2seq lemmatizer...]\n",
      "[2022-04-10 05:40:19,907 INFO] [Ensembling dict with seq2seq lemmatizer...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1: ...\n",
      "ارزش NOUN 6\n",
      "سهام NOUN 1\n",
      "شرکت PROPN 2\n",
      "نفت PROPN 3\n",
      "کاهش NOUN 6\n",
      "یافت VERB 0\n",
      "['[ارزش NP] [سهام NP] [شرکت نفت NP] [کاهش یافت VP]']\n"
     ]
    }
   ],
   "source": [
    "for index, text in enumerate(texts):\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    doc = dadma_nlp(text)\n",
    "    print(f'sentence {index + 1}: ...')\n",
    "    # for token in doc:\n",
    "    #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "    #     print(\"\\n\")\n",
    "    \n",
    "    sentences = doc._.sentences\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_text = sentence.text\n",
    "        for token in sentence:\n",
    "            token_text = token.text\n",
    "            lemma = token.lemma_ ## this has value only if lem is called\n",
    "            pos_tag = token.pos_ ## this has value only if pos is called\n",
    "            dep = token.dep_ ## this has value only if dep is called\n",
    "            dep_arc = token._.dep_arc ## this has value only if dep is called\n",
    "            print(token_text, pos_tag, dep_arc)\n",
    "            \n",
    "    sent_constituency = doc._.constituency ## this has value only if cons is called\n",
    "    sent_chunks = doc._.chunks ## this has value only if cons is called\n",
    "    ners = doc._.ners ## this has value only if ner is called\n",
    "    \n",
    "    print(sent_chunks)\n",
    "    # displacy.render(list(sentences), style=\"dep\")\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pattern Matching](https://spacy.io/usage/spacy-101#architecture-matchers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94bffb7bb37b548aa5e0061bb472f4819fbdd606d7ef446f7f021a1efc0be190"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('DataEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
