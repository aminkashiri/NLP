{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:17:49.808878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-14 11:17:49.808895: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy_stanza\n",
    "# import dadmatools.pipeline.language as language\n",
    "# from dadmatools.models.normalizer import Normalizer\n",
    "from parsivar import Normalizer as ParsivarNormalizer\n",
    "import json \n",
    "import pytse_client as tse\n",
    "import warnings\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:40 WARNING: Directory /home/ahura/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:40,109 Directory /home/ahura/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    }
   ],
   "source": [
    "stanza.install_corenlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e8ba0d276749149e422bad66713eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:41 INFO: Downloading default packages for language: fa (Persian)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:41,466 Downloading default packages for language: fa (Persian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:48 INFO: File exists: /home/ahura/stanza_resources/fa/default.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:48,887 File exists: /home/ahura/stanza_resources/fa/default.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:55 INFO: Finished downloading models and saved to /home/ahura/stanza_resources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:55,110 Finished downloading models and saved to /home/ahura/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:56 INFO: Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:56,097 Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:56 INFO: Use device: gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:56,100 Use device: gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:56 INFO: Loading: tokenize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:56,104 Loading: tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58 INFO: Loading: mwt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58,191 Loading: mwt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58,198 Loading: pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58 INFO: Loading: lemma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58,412 Loading: lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58 INFO: Loading: depparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58,453 Loading: depparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 11:18:58,822 Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp = spacy_stanza.load_pipeline(\"fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DadmaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here lemmatizer and pos tagger will be loaded\n",
    "# # as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "# pips = 'ner, pos, dep, cons, chunk, lem, tok' \n",
    "# dadma_nlp = language.Pipeline(pips)\n",
    "\n",
    "# # you can see the pipeline with this code\n",
    "# print(dadma_nlp.analyze_pipes(pretty=True))\n",
    "\n",
    "# # dadma_doc is an SpaCy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dadma Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer1 = Normalizer(\n",
    "#     full_cleaning=False,\n",
    "#     unify_chars=True,\n",
    "#     refine_punc_spacing=True,\n",
    "#     remove_extra_space=True,\n",
    "#     remove_puncs=False,\n",
    "#     remove_html=False,\n",
    "#     remove_stop_word=False,\n",
    "#     replace_email_with=\"<EMAIL>\",\n",
    "#     replace_number_with=None,\n",
    "#     replace_url_with=\"<URL\",\n",
    "#     replace_mobile_number_with=\"<MOBILE_NUMBER>\",\n",
    "#     replace_emoji_with=\"<EMOJI>\",\n",
    "#     replace_home_number_with=\"<HOME_NUMBER>\"\n",
    "# )\n",
    "\n",
    "normalizer2 = ParsivarNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Write Symbols "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download And Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# symbol_to_price_records = tse.download(symbols=\"all\", write_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = list(symbol_to_price_records.keys())\n",
    "# symbols = sorted(symbols, key=str.lower)\n",
    "\n",
    "\n",
    "# with open('symbols.json', 'w',) as file:\n",
    "#     json.dump({\"symbols\": symbols}, file, ensure_ascii=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# columns = [\"Symbol\", \"Corp. Title\", \"TSE URL\", \"Group Name\"]\n",
    "\n",
    "# symbols_info_list = []\n",
    "# for symbol in tqdm(x.keys()):\n",
    "#     ticker = tse.Ticker(symbol, adjust=True, )\n",
    "\n",
    "#     row = [symbol, ticker.title, ticker.url, ticker.group_name]\n",
    "\n",
    "#     symbols_info_list.append(row)\n",
    "\n",
    "# pd.DataFrame(symbols_info_list, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('symbols.json', 'r') as file:\n",
    "    symbols = json.load(file)['symbols']\n",
    "    \n",
    "len(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 61.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "افشا[ی|ا|]{0,3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"برکت امروز اطلاعیه‌ای مهم منتشر میکند.\"\n",
    "    \"برکت همین افشای ب باعث می شود سهم سه درصد مثبت بشود. بخاطر همین میگم پیگیر باشید.\",\n",
    "    \"نماد برکت امروز عرضه ی اولیه خیلی خوبی داره.\",\n",
    "    \"نماد برکت امروز عرضه‌ی اولیه خیلی خوبی داره.\",\n",
    "    'روز چهارشنبه یک دفعه برای خودشون افشا زدن.',\n",
    "    \"سهام وغدیر و خزر کاهش یافت.\",\n",
    "    \"برای خودشون ی افشایی زدن.\",\n",
    "    \"آ س پ امروز بالا رفت.\",\n",
    "    # \"ریزش بازار به دلیل حمله‌ی روسیه هست.\",\n",
    "    # \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    # \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    # \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "    # \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    # \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    # \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    # \"آمریکا باعث ریزش بازار شد\",\n",
    "    # \"آمریکا موجب ریزش بازار شد\",\n",
    "    # \"آمریکا دلیل ریزش بازار شد\",\n",
    "    # \"کاهش قیمت سهم عجیب بود\",\n",
    "    # \"قیمت زیاد شد\",\n",
    "    # \"قیمت زیاد است\",\n",
    "    # \"به کتابخانه رفتم.\",\n",
    "    # \"به کتابخانه رفت.\",\n",
    "    # \"پول در جیب من است.\",\n",
    "    # \"سهم قیمتش پایین است\",\n",
    "    # \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "]\n",
    "\n",
    "stock_terms = [\n",
    "    'ضرر',\n",
    "    'سود',\n",
    "    'اطلاعیه',\n",
    "    'افزایش سرمایه',\n",
    "    'تقسیم سود',\n",
    "    'دامنه نوسان',\n",
    "    'نوسان شدید',\n",
    "    'سهم رانتی',\n",
    "    'عرضه اولیه',\n",
    "    'عرضه‌ی اولیه',\n",
    "    'عرضه ی اولیه',\n",
    "    'افشا',\n",
    "    'فعالیت ماهانه',\n",
    "    'فعالیت سالانه',\n",
    "    'کاهش',\n",
    "    'افزایش',\n",
    "    'بالا',\n",
    "    'پایین'\n",
    "]\n",
    "\n",
    "extended_stock_terms = []\n",
    "for term in tqdm(stock_terms):\n",
    "    \n",
    "    term_tokens = stanza_nlp.tokenizer(term)\n",
    "    term_text_parts = [token.text for token in term_tokens]\n",
    "    \n",
    "    y = '[ی|ا|]{0,3}'\n",
    "    pattern = list(\"\".join(part + y) for n, part in enumerate(term_text_parts))\n",
    "    pattern = \"\".join(pattern)\n",
    "    \n",
    "    # pattern = \"[ی|ا|]{0,3}\".join(term_text_parts)\n",
    "    # extended_stock_terms.append(pattern)\n",
    "    \n",
    "    \n",
    "    if term == 'افشا':\n",
    "        print(pattern)\n",
    "    \n",
    "    # for tok in term_tokens:\n",
    "        # print(tok.text, tok.pos_, tok.dep_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: ...\n",
      "Normalized Text: برکت امروز اطلاعیه‌ای مهم منتشر می‌کند . برکت همین افشای ب باعث می‌شود سهم سه درصد مثبت بشود . بخاطر همین میگم پیگیر باشید .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    برکت\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " امروز اطلاعیه‌ای مهم منتشر می‌کند . \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    برکت\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " همین افشای ب باعث می‌شود سهم سه درصد مثبت بشود . بخاطر همین میگم پیگیر باشید .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 2: ...\n",
      "Normalized Text: نماد برکت امروز عرضه ی اولیه خیلی خوبی داره .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">نماد \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    برکت\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " امروز \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    عرضه ی اولیه\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TERM</span>\n",
       "</mark>\n",
       " خیلی خوبی داره .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 3: ...\n",
      "Normalized Text: نماد برکت امروز عرضه‌ی اولیه خیلی خوبی داره .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">نماد \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    برکت\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " امروز \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    عرضه‌ی اولیه\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TERM</span>\n",
       "</mark>\n",
       " خیلی خوبی داره .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 4: ...\n",
      "Normalized Text: روز چهارشنبه یک دفعه برای خودشون افشا زدن .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages/stanza/models/common/beam.py:86: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  prevK = bestScoresId // numWords\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">روز چهارشنبه یک دفعه برای خودشون \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    افشا\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TERM</span>\n",
       "</mark>\n",
       " زدن .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 5: ...\n",
      "Normalized Text: سهام وغدیر و خزر کاهش یافت .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">سهام \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    وغدیر\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " و \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    خزر\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    کاهش\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TERM</span>\n",
       "</mark>\n",
       " یافت .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 6: ...\n",
      "Normalized Text: برای خودشون ی افشایی زدن .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">برای خودشون ی افشایی زدن .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 7: ...\n",
      "Normalized Text: آ س پ امروز بالا رفت .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    آ س پ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " امروز \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    بالا\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TERM</span>\n",
       "</mark>\n",
       " رفت .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for text_index,text in enumerate(texts):\n",
    "    # text = normalizer1.normalize(text)\n",
    "    text = normalizer2.normalize(text)\n",
    "\n",
    "    print(f'Text {text_index + 1}: ...')\n",
    "    print(f\"Normalized Text: {text}\")\n",
    "        \n",
    "    doc = stanza_nlp(text)\n",
    "    \n",
    "    expression = \"|\".join(symbols)\n",
    "    \n",
    "    symbol_spans = list(map(lambda match: doc.char_span(*match.span(), label=\"SYMBOL\"),\n",
    "                       re.finditer(expression, doc.text))) \n",
    "    \n",
    "    symbol_spans = list(filter(lambda span: span is not None, \n",
    "                              symbol_spans))\n",
    "    \n",
    "    expression = \"|\".join(stock_terms + extended_stock_terms)\n",
    "    \n",
    "    term_spans = list(map(lambda match: doc.char_span(*match.span(), label=\"TERM\"),\n",
    "                       re.finditer(expression, doc.text))) \n",
    "    \n",
    "    term_spans = list(filter(lambda span: span is not None, \n",
    "                              term_spans))\n",
    "    \n",
    "    spans = term_spans + symbol_spans\n",
    "    \n",
    "    doc.set_ents(spans)\n",
    "    \n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for span in spans:\n",
    "            retokenizer.merge(span)\n",
    "    \n",
    "    # print(list(doc[:]))    \n",
    "            \n",
    "    displacy.render(doc, \"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol Tagging Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_index, text in enumerate(texts): \n",
    "    text = normalizer1.normalize(text)\n",
    "    text = normalizer2.normalize(text)\n",
    "    \n",
    "    print(f\"Normalized Text: {text}\")\n",
    "    doc = stanza_nlp(text)\n",
    "    print(f'Text {t_index + 1}: ...')\n",
    "    \n",
    "    # for m_index, (match_id, start, end) in enumerate(matches):\n",
    "    #     span:Span = Span(doc, start, end, label=match_id)\n",
    "    #     print(f\"Match {m_index + 1}: {span.text}, {span.label_}\")\n",
    "    #     if span.label_ == \"term_pattern\":\n",
    "    #         print(list(span.subtree))\n",
    "        \n",
    "    # for s_index, sentence in enumerate(doc.sents):\n",
    "    #     print(f'Sentence {s_index + 1}: ...')\n",
    "\n",
    "    print(doc.ents)\n",
    "\n",
    "    displacy.render(doc, style=\"ent\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "    \n",
    "# # stanza_nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#             # print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         for child1 in sentence.root.children:\n",
    "#             print(f\"{child1.text}, {child1.pos_}, {child1.tag_} {child1.dep_}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "#         displacy.render(sentence, style=\"dep\")\n",
    "        \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dadma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "\n",
    "    \n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = dadma_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         # for child1 in sentence.root.children:\n",
    "#             # print(f\"{child1.text} Span is: {children_string(child1.subtree)}\")\n",
    "            \n",
    "#         # print(f\"constituency: {doc._.constituency}\")\n",
    "#         print(f\"chunks: {doc._.chunks}\")\n",
    "        \n",
    "    \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display Matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "# def collect_sents(matcher, doc, i, matches):\n",
    "#     match_id, start, end = matches[i]\n",
    "#     span = doc[start:end]  # Matched span\n",
    "#     sent = span.sent  # Sentence containing matched span\n",
    "#     # Append mock entity for match in displaCy style to matched_sents\n",
    "#     # get the match span by ofsetting the start and end of the span with the\n",
    "#     # start and end of the sentence in the doc\n",
    "#     match_ents = [{\n",
    "#         \"start\": span.start_char - sent.start_char,\n",
    "#         \"end\": span.end_char - sent.start_char,\n",
    "#         \"label\": \"MATCH\",\n",
    "#     }]\n",
    "#     matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "# pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "#            {\"POS\": \"ADJ\"}]\n",
    "# matcher.add(\"FacebookIs\", [pattern], on_match=collect_sents)  # add pattern\n",
    "# doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
    "# matches = matcher(doc)\n",
    "\n",
    "# # Serve visualization of sentences containing match with displaCy\n",
    "# # set manual=True to make displaCy render straight from a dictionary\n",
    "# # (if you're not running the code within a Jupyer environment, you can\n",
    "# # use displacy.serve instead)\n",
    "# displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = PhraseMatcher(nlp.vocab)\n",
    "# terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# # Only run nlp.make_doc to speed things up\n",
    "# patterns = [nlp.make_doc(text) for text in terms]\n",
    "# matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "# doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "#           \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "# matches = matcher(doc)\n",
    "# for match_id, start, end in matches:\n",
    "#     span = doc[start:end]\n",
    "#     print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# nlp = English()\n",
    "# matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "# patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
    "# matcher.add(\"Names\", patterns)\n",
    "\n",
    "# doc = nlp(\"angela merkel and us president barack Obama\")\n",
    "# for match_id, start, end in matcher(doc):\n",
    "#     print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher: Matcher = Matcher(dadma_nlp.vocab, validate=True)\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         for token in sentence:\n",
    "#             print(f\"{token.text:10}, {token.pos_:10}, {token.tag_:10}, {token.dep_}\")\n",
    "                \n",
    "        \n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# for index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     try:\n",
    "        \n",
    "#         doc = dadma_nlp(text)\n",
    "        \n",
    "#         # print(f'sentence {index + 1}: ...')\n",
    "#         # for token in doc:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "#         #     print(\"\\n\")\n",
    "        \n",
    "#         sentences = doc._.sentences\n",
    "        \n",
    "#         # for sentence in sentences:\n",
    "#         #     sentence_text = sentence.text\n",
    "#         #     for token in sentence:\n",
    "#         #         token_text = token.text\n",
    "#         #         lemma = token.lemma_ ## this has value only if lem is called\n",
    "#         #         pos_tag = token.pos_ ## this has value only if pos is called\n",
    "#         #         dep = token.dep_ ## this has value only if dep is called\n",
    "#         #         dep_arc = token._.dep_arc ## this has value only if dep is called\n",
    "#         #         print(token_text, pos_tag, dep, dep_arc)\n",
    "#         #         if token.pos_ == \"AUX\":\n",
    "#         #             token.pos_ = \"VERB\"\n",
    "                \n",
    "#         sent_constituency = doc._.constituency ## this has value only if cons is called\n",
    "#         sent_chunks = doc._.chunks ## this has value only if cons is called\n",
    "#         # ners = doc._.ners ## this has value only if ner is called\n",
    "#         # print(sent_constituency)\n",
    "#         print(sent_chunks)\n",
    "        \n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "#         displacy.render(doc, style=\"dep\")\n",
    "        \n",
    "#     except Exception:\n",
    "        \n",
    "#         print(f\"ERRR {text}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pattern Matching](https://spacy.io/usage/spacy-101#architecture-matchers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94bffb7bb37b548aa5e0061bb472f4819fbdd606d7ef446f7f021a1efc0be190"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('DataEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
