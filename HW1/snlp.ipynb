{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:17.437193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-13 14:23:17.437212: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy_stanza\n",
    "import dadmatools.pipeline.language as language\n",
    "from dadmatools.models.normalizer import Normalizer\n",
    "from parsivar import Normalizer as ParsivarNormalizer\n",
    "import json \n",
    "import pytse_client as tse\n",
    "import warnings\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:26 WARNING: Directory /home/ahura/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:26,624 Directory /home/ahura/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    }
   ],
   "source": [
    "stanza.install_corenlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d7917145754afeba59a0a4958cc84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:27 INFO: Downloading default packages for language: fa (Persian)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:27,817 Downloading default packages for language: fa (Persian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:30 INFO: File exists: /home/ahura/stanza_resources/fa/default.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:30,417 File exists: /home/ahura/stanza_resources/fa/default.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:34 INFO: Finished downloading models and saved to /home/ahura/stanza_resources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:34,878 Finished downloading models and saved to /home/ahura/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:35 INFO: Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:35,666 Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:35 INFO: Use device: gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:35,667 Use device: gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:35 INFO: Loading: tokenize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:35,668 Loading: tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:37 INFO: Loading: mwt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:37,435 Loading: mwt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:37 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:37,477 Loading: pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:39 INFO: Loading: lemma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:39,935 Loading: lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:40 INFO: Loading: depparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:40,031 Loading: depparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:44 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-13 14:23:44,119 Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp = spacy_stanza.load_pipeline(\"fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DadmaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here lemmatizer and pos tagger will be loaded\n",
    "# # as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "# pips = 'ner, pos, dep, cons, chunk, lem, tok' \n",
    "# dadma_nlp = language.Pipeline(pips)\n",
    "\n",
    "# # you can see the pipeline with this code\n",
    "# print(dadma_nlp.analyze_pipes(pretty=True))\n",
    "\n",
    "# # dadma_doc is an SpaCy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dadma Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalizer1 = Normalizer(\n",
    "    full_cleaning=False,\n",
    "    unify_chars=True,\n",
    "    refine_punc_spacing=True,\n",
    "    remove_extra_space=True,\n",
    "    remove_puncs=False,\n",
    "    remove_html=False,\n",
    "    remove_stop_word=False,\n",
    "    replace_email_with=\"<EMAIL>\",\n",
    "    replace_number_with=None,\n",
    "    replace_url_with=\"<URL\",\n",
    "    replace_mobile_number_with=\"<MOBILE_NUMBER>\",\n",
    "    replace_emoji_with=\"<EMOJI>\",\n",
    "    replace_home_number_with=\"<HOME_NUMBER>\"\n",
    ")\n",
    "\n",
    "normalizer2 = ParsivarNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Write Symbols "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download And Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# symbol_to_price_records = tse.download(symbols=\"all\", write_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = list(symbol_to_price_records.keys())\n",
    "# symbols = sorted(symbols, key=str.lower)\n",
    "\n",
    "\n",
    "# with open('symbols.json', 'w',) as file:\n",
    "#     json.dump({\"symbols\": symbols}, file, ensure_ascii=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# columns = [\"Symbol\", \"Corp. Title\", \"TSE URL\", \"Group Name\"]\n",
    "\n",
    "# symbols_info_list = []\n",
    "# for symbol in tqdm(x.keys()):\n",
    "#     ticker = tse.Ticker(symbol, adjust=True, )\n",
    "\n",
    "#     row = [symbol, ticker.title, ticker.url, ticker.group_name]\n",
    "\n",
    "#     symbols_info_list.append(row)\n",
    "\n",
    "# pd.DataFrame(symbols_info_list, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('symbols.json', 'r') as file:\n",
    "    symbols = json.load(file)['symbols']\n",
    "    \n",
    "len(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    # \"برکت همین افشای ب باعث می شود سهم سه درصد مثبت بشود. بخاطر همین میگم پیگیر باشید.\",\n",
    "    # \"نماد برکت امروز عرضه اولیه خیلی خوبی داره.\"\n",
    "    # 'روز چهارشنبه یک دفعه برای خودشون افشا زدن.'\n",
    "    \"سهام وغدیر و خزر کاهش یافت.\",\n",
    "    # \"ریزش بازار به دلیل حمله‌ی روسیه هست.\",\n",
    "    # \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    # \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    # \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "\n",
    "    # \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    # \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    # \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    # \"آمریکا باعث ریزش بازار شد\",\n",
    "    # \"آمریکا موجب ریزش بازار شد\",\n",
    "    # \"آمریکا دلیل ریزش بازار شد\",\n",
    "    # \"کاهش قیمت سهم عجیب بود\",\n",
    "    # \"قیمت زیاد شد\",\n",
    "    # \"قیمت زیاد است\",\n",
    "    # \"به کتابخانه رفتم.\",\n",
    "    # \"به کتابخانه رفت.\",\n",
    "    # \"پول در جیب من است.\",\n",
    "    # \"سهم قیمتش پایین است\",\n",
    "    # \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "]\n",
    "\n",
    "terms = [\n",
    "    'ضرر',\n",
    "    'سود',\n",
    "    'اطلاعیه',\n",
    "    'افزایش سرمایه',\n",
    "    'تقسیم سود',\n",
    "    'دامنه نوسان',\n",
    "    'نوسان شدید',\n",
    "    'سهم رانتی',\n",
    "    'عرضه اولیه',\n",
    "    'افشا',\n",
    "    'فعالیت ماهانه',\n",
    "    'فعالیت سالانه',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol Tagging Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['symbol_component']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(برکت,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(2000)\n",
    "\n",
    "symbols_patterns = stanza_nlp.pipe(symbols, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "matcher = PhraseMatcher(stanza_nlp.vocab)\n",
    "matcher.add(\"SYMBOL\", symbols_patterns)\n",
    "\n",
    "@Language.component(\"symbol_component\")\n",
    "def symbol_component_function(doc): \n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    spans = [Span(doc, start, end, label=\"SYMBOL\") for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "\n",
    "    return doc\n",
    "\n",
    "if \"symbol_component\" not in stanza_nlp.pipe_names:\n",
    "    stanza_nlp.add_pipe(\"symbol_component\", first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(برکت,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"نماد برکت امروز عرضه اولیه خیلی خوبی داره.\"\n",
    "doc = stanza_nlp(text)\n",
    "doc.ents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: سهام وغدیر و خزر کاهش یافت .\n",
      "Text 1: ...\n",
      "Match 1: وغدیر, SYMBOL\n",
      "Match 2: خزر, SYMBOL\n",
      "Sentence 1: ...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza_nlp\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "term_patterns = [[{\"TEXT\": {\"REGEX\": term + \"*\"}}] for term in terms]\n",
    "symbol_patterns = [[{\"TEXT\": symbol}] for symbol in symbols]\n",
    "\n",
    "matcher.add(\"TERM\", term_patterns)\n",
    "matcher.add(\"SYMBOL\", symbol_patterns)\n",
    "\n",
    "for t_index, text in enumerate(texts): \n",
    "    text = normalizer1.normalize(text)\n",
    "    text = normalizer2.normalize(text)\n",
    "    \n",
    "    print(f\"Normalized Text: {text}\")\n",
    "    doc = nlp(text)\n",
    "    print(f'Text {t_index + 1}: ...')\n",
    "    \n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    for m_index, (match_id, start, end) in enumerate(matches):\n",
    "        span:Span = Span(doc, start, end, label=match_id)\n",
    "        print(f\"Match {m_index + 1}: {span.text}, {span.label_}\")\n",
    "        if span.label_ == \"TERM\":\n",
    "            print(list(span.subtree))\n",
    "        \n",
    "        \n",
    "    for s_index, sentence in enumerate(doc.sents):\n",
    "        print(f'Sentence {s_index + 1}: ...')\n",
    "    #     displacy.render(sentence, style=\"dep\")\n",
    "    #     print(f'\\n\\n')\n",
    "                \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ahura/NLP Project 1/NLP/HW1/snlp.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahura/NLP%20Project%201/NLP/HW1/snlp.ipynb#ch0000025?line=3'>4</a>\u001b[0m \u001b[39m# stanza_nlp.add_pipe(\"merge_noun_chunks\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahura/NLP%20Project%201/NLP/HW1/snlp.ipynb#ch0000025?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m t_index, text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(texts):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ahura/NLP%20Project%201/NLP/HW1/snlp.ipynb#ch0000025?line=8'>9</a>\u001b[0m     text \u001b[39m=\u001b[39m normalizer\u001b[39m.\u001b[39mnormalize(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ahura/NLP%20Project%201/NLP/HW1/snlp.ipynb#ch0000025?line=9'>10</a>\u001b[0m     doc \u001b[39m=\u001b[39m stanza_nlp(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ahura/NLP%20Project%201/NLP/HW1/snlp.ipynb#ch0000025?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mText \u001b[39m\u001b[39m{\u001b[39;00mt_index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: ...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalizer' is not defined"
     ]
    }
   ],
   "source": [
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "    \n",
    "# # stanza_nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#             # print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         for child1 in sentence.root.children:\n",
    "#             print(f\"{child1.text}, {child1.pos_}, {child1.tag_} {child1.dep_}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "#         displacy.render(sentence, style=\"dep\")\n",
    "        \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dadma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "\n",
    "    \n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = dadma_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         # for child1 in sentence.root.children:\n",
    "#             # print(f\"{child1.text} Span is: {children_string(child1.subtree)}\")\n",
    "            \n",
    "#         # print(f\"constituency: {doc._.constituency}\")\n",
    "#         print(f\"chunks: {doc._.chunks}\")\n",
    "        \n",
    "    \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display Matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "# def collect_sents(matcher, doc, i, matches):\n",
    "#     match_id, start, end = matches[i]\n",
    "#     span = doc[start:end]  # Matched span\n",
    "#     sent = span.sent  # Sentence containing matched span\n",
    "#     # Append mock entity for match in displaCy style to matched_sents\n",
    "#     # get the match span by ofsetting the start and end of the span with the\n",
    "#     # start and end of the sentence in the doc\n",
    "#     match_ents = [{\n",
    "#         \"start\": span.start_char - sent.start_char,\n",
    "#         \"end\": span.end_char - sent.start_char,\n",
    "#         \"label\": \"MATCH\",\n",
    "#     }]\n",
    "#     matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "# pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "#            {\"POS\": \"ADJ\"}]\n",
    "# matcher.add(\"FacebookIs\", [pattern], on_match=collect_sents)  # add pattern\n",
    "# doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
    "# matches = matcher(doc)\n",
    "\n",
    "# # Serve visualization of sentences containing match with displaCy\n",
    "# # set manual=True to make displaCy render straight from a dictionary\n",
    "# # (if you're not running the code within a Jupyer environment, you can\n",
    "# # use displacy.serve instead)\n",
    "# displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = PhraseMatcher(nlp.vocab)\n",
    "# terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# # Only run nlp.make_doc to speed things up\n",
    "# patterns = [nlp.make_doc(text) for text in terms]\n",
    "# matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "# doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "#           \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "# matches = matcher(doc)\n",
    "# for match_id, start, end in matches:\n",
    "#     span = doc[start:end]\n",
    "#     print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# nlp = English()\n",
    "# matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "# patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
    "# matcher.add(\"Names\", patterns)\n",
    "\n",
    "# doc = nlp(\"angela merkel and us president barack Obama\")\n",
    "# for match_id, start, end in matcher(doc):\n",
    "#     print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher: Matcher = Matcher(dadma_nlp.vocab, validate=True)\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         for token in sentence:\n",
    "#             print(f\"{token.text:10}, {token.pos_:10}, {token.tag_:10}, {token.dep_}\")\n",
    "                \n",
    "        \n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# for index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     try:\n",
    "        \n",
    "#         doc = dadma_nlp(text)\n",
    "        \n",
    "#         # print(f'sentence {index + 1}: ...')\n",
    "#         # for token in doc:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "#         #     print(\"\\n\")\n",
    "        \n",
    "#         sentences = doc._.sentences\n",
    "        \n",
    "#         # for sentence in sentences:\n",
    "#         #     sentence_text = sentence.text\n",
    "#         #     for token in sentence:\n",
    "#         #         token_text = token.text\n",
    "#         #         lemma = token.lemma_ ## this has value only if lem is called\n",
    "#         #         pos_tag = token.pos_ ## this has value only if pos is called\n",
    "#         #         dep = token.dep_ ## this has value only if dep is called\n",
    "#         #         dep_arc = token._.dep_arc ## this has value only if dep is called\n",
    "#         #         print(token_text, pos_tag, dep, dep_arc)\n",
    "#         #         if token.pos_ == \"AUX\":\n",
    "#         #             token.pos_ = \"VERB\"\n",
    "                \n",
    "#         sent_constituency = doc._.constituency ## this has value only if cons is called\n",
    "#         sent_chunks = doc._.chunks ## this has value only if cons is called\n",
    "#         # ners = doc._.ners ## this has value only if ner is called\n",
    "#         # print(sent_constituency)\n",
    "#         print(sent_chunks)\n",
    "        \n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "#         displacy.render(doc, style=\"dep\")\n",
    "        \n",
    "#     except Exception:\n",
    "        \n",
    "#         print(f\"ERRR {text}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pattern Matching](https://spacy.io/usage/spacy-101#architecture-matchers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94bffb7bb37b548aa5e0061bb472f4819fbdd606d7ef446f7f021a1efc0be190"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('DataEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
