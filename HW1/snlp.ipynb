{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:15.073975: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-14 08:21:15.073992: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy_stanza\n",
    "import dadmatools.pipeline.language as language\n",
    "from dadmatools.models.normalizer import Normalizer\n",
    "from parsivar import Normalizer as ParsivarNormalizer\n",
    "import json \n",
    "import pytse_client as tse\n",
    "import warnings\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:17 WARNING: Directory /home/ahura/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:17,660 Directory /home/ahura/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    }
   ],
   "source": [
    "stanza.install_corenlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce32d30b51845b9ab409b3940b8f187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:18 INFO: Downloading default packages for language: fa (Persian)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:18,496 Downloading default packages for language: fa (Persian)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:18 INFO: File exists: /home/ahura/stanza_resources/fa/default.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:18,846 File exists: /home/ahura/stanza_resources/fa/default.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22 INFO: Finished downloading models and saved to /home/ahura/stanza_resources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22,226 Finished downloading models and saved to /home/ahura/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22 INFO: Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22,297 Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22 INFO: Use device: gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22,298 Use device: gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22 INFO: Loading: tokenize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:22,299 Loading: tokenize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:23 INFO: Loading: mwt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:23,851 Loading: mwt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:23 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:23,858 Loading: pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:24 INFO: Loading: lemma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:24,057 Loading: lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:24 INFO: Loading: depparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:24,097 Loading: depparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:24 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 08:21:24,460 Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza_nlp = spacy_stanza.load_pipeline(\"fa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DadmaTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # here lemmatizer and pos tagger will be loaded\n",
    "# # as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "# pips = 'ner, pos, dep, cons, chunk, lem, tok' \n",
    "# dadma_nlp = language.Pipeline(pips)\n",
    "\n",
    "# # you can see the pipeline with this code\n",
    "# print(dadma_nlp.analyze_pipes(pretty=True))\n",
    "\n",
    "# # dadma_doc is an SpaCy object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dadma Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalizer1 = Normalizer(\n",
    "    full_cleaning=False,\n",
    "    unify_chars=True,\n",
    "    refine_punc_spacing=True,\n",
    "    remove_extra_space=True,\n",
    "    remove_puncs=False,\n",
    "    remove_html=False,\n",
    "    remove_stop_word=False,\n",
    "    replace_email_with=\"<EMAIL>\",\n",
    "    replace_number_with=None,\n",
    "    replace_url_with=\"<URL\",\n",
    "    replace_mobile_number_with=\"<MOBILE_NUMBER>\",\n",
    "    replace_emoji_with=\"<EMOJI>\",\n",
    "    replace_home_number_with=\"<HOME_NUMBER>\"\n",
    ")\n",
    "\n",
    "normalizer2 = ParsivarNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Libs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Write Symbols "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download And Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# symbol_to_price_records = tse.download(symbols=\"all\", write_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = list(symbol_to_price_records.keys())\n",
    "# symbols = sorted(symbols, key=str.lower)\n",
    "\n",
    "\n",
    "# with open('symbols.json', 'w',) as file:\n",
    "#     json.dump({\"symbols\": symbols}, file, ensure_ascii=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# columns = [\"Symbol\", \"Corp. Title\", \"TSE URL\", \"Group Name\"]\n",
    "\n",
    "# symbols_info_list = []\n",
    "# for symbol in tqdm(x.keys()):\n",
    "#     ticker = tse.Ticker(symbol, adjust=True, )\n",
    "\n",
    "#     row = [symbol, ticker.title, ticker.url, ticker.group_name]\n",
    "\n",
    "#     symbols_info_list.append(row)\n",
    "\n",
    "# pd.DataFrame(symbols_info_list, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('symbols.json', 'r') as file:\n",
    "    symbols = json.load(file)['symbols']\n",
    "    \n",
    "len(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"برکت همین افشای ب باعث می شود سهم سه درصد مثبت بشود. بخاطر همین میگم پیگیر باشید.\",\n",
    "    \"نماد برکت امروز عرضه اولیه خیلی خوبی داره.\",\n",
    "    'روز چهارشنبه یک دفعه برای خودشون افشا زدن.',\n",
    "    \"سهام وغدیر و خزر کاهش یافت.\",\n",
    "    \"برای خودشون ی افشایی زدن.\",\n",
    "    \"آ س پ امروز بالا رفت.\",\n",
    "    # \"ریزش بازار به دلیل حمله‌ی روسیه هست.\",\n",
    "    # \"فک کنم یه اصلاح قیمتی و کمی ریزش داشته باشیم.\",\n",
    "    # \"قرارداد با آمریکا باعث افت قیمت سهم وغدیر شد\",\n",
    "    # \"یک نکته‌ی تکنیکالی هم در صورت دستکاری نشدن اضافه کنم، کندلی که روز سه شنبه‌ی گذشته ثبت کرد کامل است\",\n",
    "    # \"روز چهارشنبه یه دفعه برای خودشون افشا زدن\",\n",
    "    # \"رشد قیمت‌ها باعث ایجاد صف خرید در سهم پرشیا شد\",\n",
    "    # \"شاخص به ۲ میلیون می‌رسه\",\n",
    "    # \"آمریکا باعث ریزش بازار شد\",\n",
    "    # \"آمریکا موجب ریزش بازار شد\",\n",
    "    # \"آمریکا دلیل ریزش بازار شد\",\n",
    "    # \"کاهش قیمت سهم عجیب بود\",\n",
    "    # \"قیمت زیاد شد\",\n",
    "    # \"قیمت زیاد است\",\n",
    "    # \"به کتابخانه رفتم.\",\n",
    "    # \"به کتابخانه رفت.\",\n",
    "    # \"پول در جیب من است.\",\n",
    "    # \"سهم قیمتش پایین است\",\n",
    "    # \"حضور تو موجب خوشحالی من در هوای بارانی است\",\n",
    "]\n",
    "\n",
    "terms = [\n",
    "    'ضرر',\n",
    "    'سود',\n",
    "    'اطلاعیه',\n",
    "    'افزایش سرمایه',\n",
    "    'تقسیم سود',\n",
    "    'دامنه نوسان',\n",
    "    'نوسان شدید',\n",
    "    'سهم رانتی',\n",
    "    'عرضه اولیه',\n",
    "    'افشا',\n",
    "    'فعالیت ماهانه',\n",
    "    'فعالیت سالانه',\n",
    "    'کاهش',\n",
    "    'افزایش',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: ...\n",
      "Normalized Text: برکت همین افشای ب باعث می‌شود سهم سه درصد مثبت بشود . بخاطر همین میگم پیگیر باشید .\n",
      "x: (0, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    برکت\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " همین افشای ب باعث می‌شود سهم سه درصد مثبت بشود . بخاطر همین میگم پیگیر باشید .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Text 2: ...\n",
      "Normalized Text: نماد برکت امروز عرضه اولیه خیلی خوبی داره .\n",
      "x: (1, 3)\n",
      "x: (5, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">نماد \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    برکت\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " امروز عرضه اولیه خیلی خوبی داره .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Text 3: ...\n",
      "Normalized Text: روز چهارشنبه یک دفعه برای خودشون افشا زدن .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">روز چهارشنبه یک دفعه برای خودشون افشا زدن .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Text 4: ...\n",
      "Normalized Text: سهام وغدیر و خزر کاهش یافت .\n",
      "x: (5, 10)\n",
      "x: (13, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">سهام \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    وغدیر\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " و \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    خزر\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " کاهش یافت .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Text 5: ...\n",
      "Normalized Text: برای خودشون ی افشایی زدن .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">برای خودشون ی افشایی زدن .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Text 6: ...\n",
      "Normalized Text: آ س پ امروز بالا رفت .\n",
      "x: (0, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    آ س پ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SYMBOL</span>\n",
       "</mark>\n",
       " امروز بالا رفت .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for text_index,text in enumerate(texts):\n",
    "    text = normalizer1.normalize(text)\n",
    "    text = normalizer2.normalize(text)\n",
    "\n",
    "    print(f'Text {text_index + 1}: ...')\n",
    "    print(f\"Normalized Text: {text}\")\n",
    "        \n",
    "    doc = stanza_nlp(text)\n",
    "    \n",
    "    expression = \"|\".join(symbols)\n",
    "\n",
    "    # symbol_ents = [doc.char_span(*match.span(), label=\"SYMBOL\") for match in re.finditer(expression, doc.text)]  \n",
    "    symbol_ents = []\n",
    "    for match in re.finditer(expression, doc.text): \n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end, label=\"SYMBOL\")\n",
    "        print(f\"x: {match.span()}\")\n",
    "        if span is not None: \n",
    "            symbol_ents.append(span)\n",
    "                \n",
    "    term_ents = []\n",
    "    for term in terms: \n",
    "        expression = f\"{term}\"\n",
    "\n",
    "        for match in re.finditer(expression, doc.text):\n",
    "            \n",
    "            pass        \n",
    "    \n",
    "    doc.ents = symbol_ents + term_ents\n",
    "    displacy.render(doc, \"ent\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol Tagging Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: برکت همین افشای ب باعث می‌شود سهم سه درصد مثبت بشود . بخاطر همین میگم پیگیر باشید .\n",
      "Text 1: ...\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">برکت همین افشای ب باعث می‌شود سهم سه درصد مثبت بشود . بخاطر همین میگم پیگیر باشید .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Normalized Text: نماد برکت امروز عرضه اولیه خیلی خوبی داره .\n",
      "Text 2: ...\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">نماد برکت امروز عرضه اولیه خیلی خوبی داره .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Normalized Text: روز چهارشنبه یک دفعه برای خودشون افشا زدن .\n",
      "Text 3: ...\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">روز چهارشنبه یک دفعه برای خودشون افشا زدن .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Normalized Text: سهام وغدیر و خزر کاهش یافت .\n",
      "Text 4: ...\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">سهام وغدیر و خزر کاهش یافت .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Normalized Text: برای خودشون ی افشایی زدن .\n",
      "Text 5: ...\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">برای خودشون ی افشایی زدن .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Normalized Text: آ س پ امروز بالا رفت .\n",
      "Text 6: ...\n",
      "()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: rtl\">آ س پ امروز بالا رفت .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t_index, text in enumerate(texts): \n",
    "    text = normalizer1.normalize(text)\n",
    "    text = normalizer2.normalize(text)\n",
    "    \n",
    "    print(f\"Normalized Text: {text}\")\n",
    "    doc = stanza_nlp(text)\n",
    "    print(f'Text {t_index + 1}: ...')\n",
    "    \n",
    "    # for m_index, (match_id, start, end) in enumerate(matches):\n",
    "    #     span:Span = Span(doc, start, end, label=match_id)\n",
    "    #     print(f\"Match {m_index + 1}: {span.text}, {span.label_}\")\n",
    "    #     if span.label_ == \"term_pattern\":\n",
    "    #         print(list(span.subtree))\n",
    "        \n",
    "    # for s_index, sentence in enumerate(doc.sents):\n",
    "    #     print(f'Sentence {s_index + 1}: ...')\n",
    "\n",
    "    print(doc.ents)\n",
    "\n",
    "    displacy.render(doc, style=\"ent\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "    \n",
    "# # stanza_nlp.add_pipe(\"merge_noun_chunks\")\n",
    "\n",
    "\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#             # print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         for child1 in sentence.root.children:\n",
    "#             print(f\"{child1.text}, {child1.pos_}, {child1.tag_} {child1.dep_}\")\n",
    "            \n",
    "            \n",
    "        \n",
    "#         displacy.render(sentence, style=\"dep\")\n",
    "        \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dadma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def children_string(children): \n",
    "#     return \" \".join([ch.text for ch in children])\n",
    "\n",
    "    \n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = dadma_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         # for token in sentence:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "\n",
    "#         print(f'\\n\\n')\n",
    "#         print(f\"sentence root: {sentence.root.text}\")\n",
    "#         # for child1 in sentence.root.children:\n",
    "#             # print(f\"{child1.text} Span is: {children_string(child1.subtree)}\")\n",
    "            \n",
    "#         # print(f\"constituency: {doc._.constituency}\")\n",
    "#         print(f\"chunks: {doc._.chunks}\")\n",
    "        \n",
    "    \n",
    "                \n",
    "#     print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display Matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "# def collect_sents(matcher, doc, i, matches):\n",
    "#     match_id, start, end = matches[i]\n",
    "#     span = doc[start:end]  # Matched span\n",
    "#     sent = span.sent  # Sentence containing matched span\n",
    "#     # Append mock entity for match in displaCy style to matched_sents\n",
    "#     # get the match span by ofsetting the start and end of the span with the\n",
    "#     # start and end of the sentence in the doc\n",
    "#     match_ents = [{\n",
    "#         \"start\": span.start_char - sent.start_char,\n",
    "#         \"end\": span.end_char - sent.start_char,\n",
    "#         \"label\": \"MATCH\",\n",
    "#     }]\n",
    "#     matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "# pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "#            {\"POS\": \"ADJ\"}]\n",
    "# matcher.add(\"FacebookIs\", [pattern], on_match=collect_sents)  # add pattern\n",
    "# doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
    "# matches = matcher(doc)\n",
    "\n",
    "# # Serve visualization of sentences containing match with displaCy\n",
    "# # set manual=True to make displaCy render straight from a dictionary\n",
    "# # (if you're not running the code within a Jupyer environment, you can\n",
    "# # use displacy.serve instead)\n",
    "# displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# matcher = PhraseMatcher(nlp.vocab)\n",
    "# terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# # Only run nlp.make_doc to speed things up\n",
    "# patterns = [nlp.make_doc(text) for text in terms]\n",
    "# matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "# doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "#           \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "# matches = matcher(doc)\n",
    "# for match_id, start, end in matches:\n",
    "#     span = doc[start:end]\n",
    "#     print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# nlp = English()\n",
    "# matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "# patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
    "# matcher.add(\"Names\", patterns)\n",
    "\n",
    "# doc = nlp(\"angela merkel and us president barack Obama\")\n",
    "# for match_id, start, end in matcher(doc):\n",
    "#     print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher: Matcher = Matcher(dadma_nlp.vocab, validate=True)\n",
    "\n",
    "# for t_index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     doc = stanza_nlp(text)\n",
    "#     print(f'Text {t_index + 1}: ...')\n",
    "        \n",
    "#     for s_index, sentence in enumerate(doc.sents):\n",
    "#         print(f'Sentence {s_index + 1}: ...')\n",
    "#         for token in sentence:\n",
    "#             print(f\"{token.text:10}, {token.pos_:10}, {token.tag_:10}, {token.dep_}\")\n",
    "                \n",
    "        \n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "# for index, text in enumerate(texts):\n",
    "#     text = normalizer.normalize(text)\n",
    "#     try:\n",
    "        \n",
    "#         doc = dadma_nlp(text)\n",
    "        \n",
    "#         # print(f'sentence {index + 1}: ...')\n",
    "#         # for token in doc:\n",
    "#         #     print(f'word: {token.text:12}, pos: {token.pos_:10}, tag: {token.tag_:10}, dep: {token.dep_:15}')\n",
    "#         #     print(\"\\n\")\n",
    "        \n",
    "#         sentences = doc._.sentences\n",
    "        \n",
    "#         # for sentence in sentences:\n",
    "#         #     sentence_text = sentence.text\n",
    "#         #     for token in sentence:\n",
    "#         #         token_text = token.text\n",
    "#         #         lemma = token.lemma_ ## this has value only if lem is called\n",
    "#         #         pos_tag = token.pos_ ## this has value only if pos is called\n",
    "#         #         dep = token.dep_ ## this has value only if dep is called\n",
    "#         #         dep_arc = token._.dep_arc ## this has value only if dep is called\n",
    "#         #         print(token_text, pos_tag, dep, dep_arc)\n",
    "#         #         if token.pos_ == \"AUX\":\n",
    "#         #             token.pos_ = \"VERB\"\n",
    "                \n",
    "#         sent_constituency = doc._.constituency ## this has value only if cons is called\n",
    "#         sent_chunks = doc._.chunks ## this has value only if cons is called\n",
    "#         # ners = doc._.ners ## this has value only if ner is called\n",
    "#         # print(sent_constituency)\n",
    "#         print(sent_chunks)\n",
    "        \n",
    "#         print(\"\\n\\n\\n\")\n",
    "        \n",
    "#         displacy.render(doc, style=\"dep\")\n",
    "        \n",
    "#     except Exception:\n",
    "        \n",
    "#         print(f\"ERRR {text}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pattern Matching](https://spacy.io/usage/spacy-101#architecture-matchers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94bffb7bb37b548aa5e0061bb472f4819fbdd606d7ef446f7f021a1efc0be190"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('DataEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
