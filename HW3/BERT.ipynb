{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Oj8ZYJiEa3V",
        "outputId": "2d48e730-f915-4b25-8a9f-f013af39b3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (1.2.3)\n",
            "Requirement already satisfied: spacy-stanza in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.3)\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.7/dist-packages (22.3.0)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black) (1.5.4)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black) (0.4.3)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black) (0.9.0)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black) (2.5.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from black) (8.1.3)\n",
            "Installing collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.3\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy torch stanza spacy-stanza transformers nltk hazm black pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a0UeQ0EdU1",
        "outputId": "129c2681-1797-4ba2-8f4b-809d9fb54e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=8fbf11e7d5811ffb6e3e21a58ad9f39023b730d23a9be3170ba9184175f05539\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rzqldani/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "X4u6yjSTBUVz",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g6k69CHtBUV4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import torch\n",
        "\n",
        "import editdistance\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    BertTokenizer,\n",
        "    BertForMaskedLM,\n",
        "    AlbertTokenizer,\n",
        "    AlbertForMaskedLM,\n",
        "    RobertaTokenizer,\n",
        "    RobertaModel,\n",
        ")\n",
        "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
        "from spacy.tokens.token import Token\n",
        "from spacy.tokens.doc import Doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNA20jkGBUV6",
        "outputId": "fc95099d-3278-4f05-8f96-0d12f5026107",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch Device: cpu\n",
            "len vocab: 39620\n",
            "en roberta Model Loaded ...\n"
          ]
        }
      ],
      "source": [
        "# torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch_device = \"cpu\"\n",
        "print(f\"Torch Device: {torch_device}\")\n",
        "\n",
        "# language = 'fa'\n",
        "# model_type = 'bert'\n",
        "\n",
        "language = \"en\"\n",
        "model_type = \"roberta\"\n",
        "\n",
        "if language == \"en\":\n",
        "    if model_type == \"bert\":\n",
        "        model_name = \"bert-large-uncased\"  # Bert large\n",
        "        # model_name = \"bert-base-uncased\" # Bert base\n",
        "    elif model_type == \"roberta\":\n",
        "        model_name = \"roberta-large\"  # Roberta\n",
        "    else:\n",
        "        raise f\"{model_type} model not found.\"\n",
        "\n",
        "elif language == \"fa\":\n",
        "    if model_type == \"bert\":\n",
        "        # model_name = \"HooshvareLab/bert-fa-base-uncased\"  # BERT V2\n",
        "        model_name = \"HooshvareLab/bert-fa-zwnj-base\"  # BERT V3\n",
        "    elif model_type == \"albert\":\n",
        "        model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\"  # Albert\n",
        "    else:\n",
        "        raise f\"{model_type} model not found.\"\n",
        "\n",
        "else:\n",
        "    raise f\"{language} language not found.\"\n",
        "\n",
        "if model_type == \"bert\":\n",
        "    MASK = \"[MASK]\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
        "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "elif model_type == \"albert\":\n",
        "    MASK = \"[MASK]\"\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "    model = AlbertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
        "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "elif model_type == \"roberta\":\n",
        "    MASK = \"<mask>\"\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
        "    unmasker = pipeline(\"fill-mask\", model=\"roberta-large\", tokenizer=tokenizer)\n",
        "\n",
        "else:\n",
        "    print(f\"{model_type} not found.\")\n",
        "\n",
        "vocab: set = set(tokenizer.get_vocab().keys())\n",
        "\n",
        "if model_type == \"roberta\":\n",
        "    vocab = set(map(lambda s: s[1:], vocab))\n",
        "\n",
        "print(f\"len vocab: {len(vocab)}\")\n",
        "print(f\"{language} {model_type} Model Loaded ...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "svt6XdEUBUV-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Stanza\n",
        "\n",
        "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CZJviZP6BUV-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "import spacy\n",
        "import spacy_stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t9K0jKLgBUV-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "if language == \"fa\":\n",
        "    stanza.install_corenlp()\n",
        "    stanza.download(\"fa\")\n",
        "    nlp = spacy_stanza.load_pipeline(\"fa\")\n",
        "\n",
        "elif language == \"en\":\n",
        "    spacy.prefer_gpu()\n",
        "    nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"{language} not supported.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WM_OSihaBUV_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def half_space_case(predicted: str, current: str):\n",
        "    wo_half_space_current = current.replace(\"‌\", \"\")\n",
        "    return wo_half_space_current == predicted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eU-cld58BUWD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Correction Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PC6m3_-NBUWD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "class SpellCorrector:\n",
        "    def __init__(self, alpha=5, max_edit_distance=2, verbose=False, top_k=50):\n",
        "        self.alpha = alpha\n",
        "        self.max_edit_distance = max_edit_distance\n",
        "        self.verbose = verbose\n",
        "        self.top_k = top_k\n",
        "        self.spell_checker = SpellChecker()\n",
        "\n",
        "    def print_summary(self, type):\n",
        "        if not self.verbose:\n",
        "            return\n",
        "        text = self.text\n",
        "        current_token = self.current_token\n",
        "        start_char_index = self.start_char_index\n",
        "        end_char_index = self.end_char_index\n",
        "\n",
        "        print(\"*\" * 50)\n",
        "        print(f\"Token: {current_token.text}\")\n",
        "\n",
        "        print(\"Filtered Predicts: \\n\")\n",
        "        print(self.filtered_predicts[[\"token_str\", \"score\", \"total_score\"]])\n",
        "\n",
        "        print(f\"{current_token.text} -> {self.selected_predict} : {type}\")\n",
        "\n",
        "        if self.some_token_corrected:\n",
        "            typo_correction_details = {\n",
        "                \"raw\": current_token.text,\n",
        "                \"corrected\": self.selected_predict,\n",
        "                \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
        "                \"around\": text[start_char_index - 10 : end_char_index + 10],\n",
        "                \"type\": \"contextual\",\n",
        "            }\n",
        "\n",
        "            print(typo_correction_details)\n",
        "\n",
        "    def set_predictions(self):\n",
        "        start_char_index: int = self.current_token.idx\n",
        "        end_char_index = start_char_index + len(self.current_token)\n",
        "\n",
        "        masked_text = (\n",
        "            self.doc.text[:start_char_index] + MASK + self.doc.text[end_char_index:]\n",
        "        )\n",
        "\n",
        "        predicts = unmasker(masked_text, top_k=self.top_k)\n",
        "        predicts = pd.DataFrame(predicts)\n",
        "\n",
        "        self.predicts = predicts\n",
        "        self.start_char_index = start_char_index\n",
        "        self.end_char_index = end_char_index\n",
        "        self.masked_text = masked_text\n",
        "        return predicts\n",
        "\n",
        "    def set_filtered_predictions(self):\n",
        "        predicts = self.predicts\n",
        "        predicts.loc[:, \"token_str\"] = predicts[\"token_str\"].apply(\n",
        "            lambda tk: tk.replace(\" \", \"\")\n",
        "        )\n",
        "        predicts.loc[:, \"edit_distance\"] = predicts[\"token_str\"].apply(\n",
        "            lambda tk: editdistance.eval(self.current_token.text, tk)\n",
        "        )\n",
        "\n",
        "        # Filter tokens with at most 3 edit distance\n",
        "        filtered_predicts = predicts.loc[\n",
        "            predicts[\"edit_distance\"] <= self.max_edit_distance, :\n",
        "        ].copy()\n",
        "\n",
        "        # Apply total score function\n",
        "        # e: edit distance + 1\n",
        "        # l: token length\n",
        "        filtered_predicts.loc[:, \"e_to_l\"] = (\n",
        "            filtered_predicts.loc[:, \"edit_distance\"] + 1\n",
        "        ) / len(self.current_token.text)\n",
        "\n",
        "        filtered_predicts.loc[:, \"total_score\"] = (\n",
        "            filtered_predicts.loc[:, \"score\"]\n",
        "            / filtered_predicts.loc[:, \"e_to_l\"] ** self.alpha\n",
        "        )\n",
        "\n",
        "        filtered_predicts = filtered_predicts.sort_values(\n",
        "            \"total_score\", ascending=False\n",
        "        )\n",
        "        self.filtered_predicts = filtered_predicts\n",
        "\n",
        "    def correct_predict(self, selected_predict):\n",
        "        if selected_predict != self.current_token.text:\n",
        "            if not half_space_case(selected_predict, self.current_token.text):\n",
        "                self.some_token_corrected = True\n",
        "                self.text = self.masked_text.replace(MASK, selected_predict, 1)\n",
        "                self.doc = nlp(self.text)\n",
        "\n",
        "            else:\n",
        "                vocab.add(self.current_token.text)\n",
        "                self.selected_predict = self.current_token.text\n",
        "\n",
        "    def correct_lexico_typo(self):\n",
        "        self.doc = nlp(self.text)\n",
        "        while True:\n",
        "            self.some_token_corrected = False\n",
        "            for index, current_token in enumerate(self.doc):\n",
        "                self.current_token: Token = current_token\n",
        "\n",
        "                if current_token.text not in vocab:\n",
        "                    self.set_predictions()\n",
        "\n",
        "                    try:\n",
        "                        if current_token.text in string.punctuation:\n",
        "                            selected_predict = self.predicts[\"token_str\"].iloc[0]\n",
        "                        elif any(c.isdigit() for c in current_token.text):\n",
        "                            print(\"DIGIT\")\n",
        "                            selected_predict = current_token.text\n",
        "                        else:\n",
        "                            self.set_filtered_predictions()\n",
        "                            selected_predict_row = self.filtered_predicts.iloc[0, :]\n",
        "                            selected_predict = selected_predict_row[\"token_str\"]\n",
        "                    except Exception as e:\n",
        "                        print(\n",
        "                            f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(self.filtered_predicts)}\"\n",
        "                        )\n",
        "                        selected_predict = self.spell_checker.correction(\n",
        "                            self.current_token.text\n",
        "                        )\n",
        "\n",
        "                    self.correct_predict(selected_predict)\n",
        "                    self.print_summary(\"lexical\")\n",
        "\n",
        "                    if self.some_token_corrected:\n",
        "                        break\n",
        "\n",
        "            if not self.some_token_corrected:\n",
        "                break\n",
        "\n",
        "    def correct_contextual_typo(self):\n",
        "        doc = nlp(self.text)\n",
        "        self.doc = doc\n",
        "        for index, current_token in enumerate(doc):\n",
        "            self.current_token: Token = current_token\n",
        "            self.set_predictions()\n",
        "\n",
        "            try:\n",
        "                if current_token.text in string.punctuation:\n",
        "                    self.filtered_predicts = self.predicts.loc[\n",
        "                        self.predicts[\"token_str\"].apply(\n",
        "                            lambda tk: tk in string.punctuation\n",
        "                        ),\n",
        "                        :,\n",
        "                    ].copy()\n",
        "                    selected_predict = self.filtered_predicts[\"token_str\"].iloc[0]\n",
        "                elif any(c.isdigit() for c in current_token.text):\n",
        "                    selected_predict = current_token.text\n",
        "                else:\n",
        "                    self.set_filtered_predictions()\n",
        "                    selected_predict_row = self.filtered_predicts.iloc[0, :]\n",
        "                    selected_predict = selected_predict_row[\"token_str\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                selected_predict = current_token.text\n",
        "                print(\n",
        "                    f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(self.filtered_predicts)}\"\n",
        "                )\n",
        "\n",
        "            self.correct_predict(selected_predict)\n",
        "            self.print_summary(\"contexual\")\n",
        "\n",
        "    def correction_pipeline(self):\n",
        "        print(\n",
        "            f\"Lexico Correction ... . text = {self.text}\"\n",
        "        ) if self.verbose else print()\n",
        "        self.correct_lexico_typo()\n",
        "\n",
        "        print(\n",
        "            f\"Contextual Correction ... . text = {self.text}\"\n",
        "        ) if self.verbose else print()\n",
        "        self.correct_contextual_typo()\n",
        "\n",
        "    def __call__(self, text, *args, **kwargs):\n",
        "        self.text = text\n",
        "        self.correction_pipeline()\n",
        "        return self.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cY2Jwvz4BUWE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Sample Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urftUEFFBUWE",
        "outputId": "dea22317-0243-44d8-b443-3fdde5331d6c",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Token: fotball\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "28   football  0.004468   100.608445\n",
            "225  Football  0.000294     0.258717\n",
            "fotball -> football : lexical\n",
            "{'raw': 'fotball', 'corrected': 'football', 'span': '[13, 20]', 'around': 'm playing football', 'type': 'contextual'}\n",
            "**************************************************\n",
            "Token: I\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score   total_score\n",
            "0           I  0.986282  9.862816e-01\n",
            "2           I  0.000673  6.725948e-04\n",
            "1           i  0.005520  2.156073e-05\n",
            "7           1  0.000185  7.232717e-07\n",
            "11         II  0.000111  4.331160e-07\n",
            "..        ...       ...           ...\n",
            "212        14  0.000004  6.737125e-10\n",
            "213        Am  0.000004  6.729984e-10\n",
            "217        PS  0.000004  6.668498e-10\n",
            "233        30  0.000004  6.073729e-10\n",
            "238        it  0.000004  5.928574e-10\n",
            "\n",
            "[98 rows x 3 columns]\n",
            "I -> I : contexual\n",
            "**************************************************\n",
            "Token: am\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score   total_score\n",
            "12         am  0.012947  3.314549e+00\n",
            "8          'm  0.022866  2.286567e-02\n",
            "135        am  0.000037  9.451339e-03\n",
            "4         was  0.047790  1.864689e-03\n",
            "65         is  0.000099  3.856303e-06\n",
            "98       came  0.000058  2.259440e-06\n",
            "99        are  0.000058  2.255187e-06\n",
            "100       had  0.000058  2.252796e-06\n",
            "121       saw  0.000048  1.859842e-06\n",
            "151        go  0.000030  1.155741e-06\n",
            "153        'd  0.000029  1.126262e-06\n",
            "154        be  0.000029  1.118291e-06\n",
            "173        AM  0.000024  9.181491e-07\n",
            "178        's  0.000022  8.575868e-07\n",
            "180       can  0.000022  8.465810e-07\n",
            "182        do  0.000022  8.419434e-07\n",
            "191         ,  0.000020  7.615470e-07\n",
            "204       sat  0.000018  6.846152e-07\n",
            "213       Was  0.000016  6.407562e-07\n",
            "224       ran  0.000015  5.827190e-07\n",
            "227       ate  0.000015  5.665363e-07\n",
            "am -> am : contexual\n",
            "**************************************************\n",
            "Token: playing\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score    total_score\n",
            "1     playing  0.134393  774751.601598\n",
            "123    paying  0.000460      10.350106\n",
            "88    blaming  0.000678       0.595680\n",
            "158  planning  0.000348       0.305700\n",
            "244    saying  0.000192       0.168385\n",
            "playing -> playing : contexual\n",
            "**************************************************\n",
            "Token: football\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score   total_score\n",
            "28   football  0.004468  74956.617188\n",
            "225  Football  0.000294     19.297035\n",
            "football -> football : contexual\n",
            "True\n",
            "I am playing football\n",
            "\n",
            "\n",
            "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
            " * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if language == \"en\":\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            I am playing fotball\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            I am playing football\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The quantity thoery of money also assume that the quantity of money in an economy has a large influense on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the sopply of gods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The quantity theory of money also assumes that the quantity of money in an economy has a large influence on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the supply of goods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Does it privent Iran from getting nuclear weapens. Many exports say that if all parties adhered to their pledges, the deal almost certainly could have achieved that goal for longer than a dekade!\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Does it prevent Iran from getting nuclear weapons? Many experts say that if all parties adhere to their pledges, the deal almost certainly could have achieved that goal for longer than a decade.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The Federal Reserve monitor risks to the financal system and works to help insure the system supports a haelthy economy for US households, communities, and busineses.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The Federal Reserve monitors risks to the financial system and works to help ensure the system supports a healthy economy for US households, communities, and businesses.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Bitcoin is a decentrallized digital curency that can be transfered on the peer-to-peer bitcoin network. Bitcoin transactions are veryfied by network nodes throgh cryptography and recorded in a public distributed ledger called a blockchain. The criptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The curency began use in 2009 when its implemntation was released as open-source software.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Bitcoin is a decentralized digital currency that can be transferred on the peer-to-peer bitcoin network. Bitcoin transactions are verified by network nodes through cryptography and recorded in a public distributed ledger called a blockchain. The cryptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The 2022 FILA World Cup is scheduled to be the 22nd running of the FILA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 Novamber to 18 Decamber 2022.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The 2022 FIFA World Cup is scheduled to be the 22nd running of the FIFA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 November to 18 December 2022.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            President Daneld Trump annonced on Tuesday he well withdraw the United States from the Iran nuclear deal and restore far-reaching sanktions aimed at withdrawal Iran from the global finansial system.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            President Donald Trump announced on Tuesday he will withdraw the United States from the Iran nuclear deal and restore far-reaching sanctions aimed at withdrawal Iran from the global financial system.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Cars has very sweet features. It has two beautifull eye, adorable tiny paws, sharp claws, and two fury ear which are very sensitive to sounds. It has a tiny body covered with sot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Cat has very sweet features. It has two beautiful eyes, adorable tiny paws with sharp claws, and two furry ears which are very sensitive to sounds. It has a tiny body covered with soft fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
        "        \"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if model_type != \"roberta\":\n",
        "        for test_case in test_cases:\n",
        "            test_case[\"input_text\"] = test_case[\"input_text\"].lower()\n",
        "            test_case[\"true_text\"] = test_case[\"true_text\"].lower()\n",
        "\n",
        "elif language == \"fa\":\n",
        "    test_cases = [\n",
        "        # {\n",
        "        #     \"input_text\": \"\"\"\n",
        "        #\n",
        "        # \"\"\",\n",
        "        #     \"true_text\": \"\"\"\n",
        "        #\n",
        "        #  \"\"\"\n",
        "        # },\n",
        "        {\n",
        "            \"input_text\": \"پس از سال‌ها تلاش، رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"وقتی قیمت گوست قرمز یا صفید در کشورهای دیگر بیشتر شده است، ممکن است در جیران هم گرا شود.\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"در هفته گذشته قیمت تلا تغییر چندانی نداشت، و در همان محدوده 1850 دلاری کار خود را به پایان رساند. \",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"بر اساس مسوبه سران قوا، معاملات فردایی طلا همانند معاملات فردایی ارض، ممنوع و غیرقانونی شناخته شد و فعالان این بازار به جرم اخلال اقتصادی، تحت پیگرد قرار خواهند گرفت. در نتیجه تانک مرکزی در بازار فردایی مداخله نخواهد کرد\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        با نزدیک شدن قیمت دار غیر رسمی به سفف خود در روز قبل، تحلیلگران در بازار برای هفته بعد هشدار میدادند که باید احطیاط کرد و اقدامات امنیتی در بازار افزایش خواهد یافت.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        با تولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تأمین کندم جهان داشتند، بازار کندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان کندم بود، در همه جای جهان افزایش یافت.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        علت واقعی تعویق در مزاکرات وین چیست.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "else:\n",
        "    raise f\"{language} language not found.\"\n",
        "\n",
        "ALPHA = 8 if language == \"en\" else 30\n",
        "MAX_EDIT_DISTANCE = 2 if language == \"en\" else 2\n",
        "TOP_K = 250 if language == \"en\" else 5000\n",
        "VERBOSE = True\n",
        "\n",
        "for test_case in test_cases:\n",
        "    test_case[\"input_text\"] = test_case[\"input_text\"].strip()\n",
        "    test_case[\"true_text\"] = test_case[\"true_text\"].strip()\n",
        "\n",
        "spell_corrector = SpellCorrector(ALPHA, MAX_EDIT_DISTANCE, VERBOSE, TOP_K)\n",
        "from spacy import displacy\n",
        "\n",
        "for idx in range(len(test_cases)):\n",
        "    test_case = test_cases[idx]\n",
        "\n",
        "    input_text = test_case[\"input_text\"]\n",
        "\n",
        "    output_text = spell_corrector(input_text)\n",
        "\n",
        "    print(output_text == test_case[\"true_text\"])\n",
        "\n",
        "    print(output_text)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* \" * 50)\n",
        "    print(\" *\" * 50)\n",
        "    print(\"\\n\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-0Swt0WBUWG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\"الکل\" in vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nMe4SeZSoqj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BERT.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('data_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
