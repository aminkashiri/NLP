{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Oj8ZYJiEa3V",
        "outputId": "2d48e730-f915-4b25-8a9f-f013af39b3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (1.2.3)\n",
            "Requirement already satisfied: spacy-stanza in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.3)\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.7/dist-packages (22.3.0)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black) (1.5.4)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black) (0.4.3)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black) (0.9.0)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black) (2.5.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from black) (8.1.3)\n",
            "Installing collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.3\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy torch stanza spacy-stanza transformers nltk hazm black pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a0UeQ0EdU1",
        "outputId": "129c2681-1797-4ba2-8f4b-809d9fb54e27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=8fbf11e7d5811ffb6e3e21a58ad9f39023b730d23a9be3170ba9184175f05539\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rzqldani/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "X4u6yjSTBUVz",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g6k69CHtBUV4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import torch\n",
        "\n",
        "import editdistance\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    BertTokenizer,\n",
        "    BertForMaskedLM,\n",
        "    AlbertTokenizer,\n",
        "    AlbertForMaskedLM,\n",
        "    RobertaTokenizer,\n",
        "    RobertaModel,\n",
        ")\n",
        "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
        "from spacy.tokens.token import Token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VNA20jkGBUV6",
        "outputId": "fc95099d-3278-4f05-8f96-0d12f5026107",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch Device: cpu\n",
            "len vocab: 39620\n",
            "en roberta Model Loaded ...\n"
          ]
        }
      ],
      "source": [
        "# torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch_device = \"cpu\"\n",
        "print(f\"Torch Device: {torch_device}\")\n",
        "\n",
        "# language = 'fa'\n",
        "# model_type = 'bert'\n",
        "\n",
        "language = \"en\"\n",
        "model_type = \"roberta\"\n",
        "\n",
        "if language == \"en\":\n",
        "    if model_type == \"bert\":\n",
        "        model_name = \"bert-large-uncased\"  # Bert large\n",
        "        # model_name = \"bert-base-uncased\" # Bert base\n",
        "    elif model_type == \"roberta\":\n",
        "        model_name = \"roberta-large\"  # Roberta\n",
        "    else:\n",
        "        raise f\"{model_type} model not found.\"\n",
        "\n",
        "elif language == \"fa\":\n",
        "    if model_type == \"bert\":\n",
        "        # model_name = \"HooshvareLab/bert-fa-base-uncased\"  # BERT V2\n",
        "        model_name = \"HooshvareLab/bert-fa-zwnj-base\"  # BERT V3\n",
        "    elif model_type == \"albert\":\n",
        "        model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\"  # Albert\n",
        "    else:\n",
        "        raise f\"{model_type} model not found.\"\n",
        "\n",
        "else:\n",
        "    raise f\"{language} language not found.\"\n",
        "\n",
        "if model_type == \"bert\":\n",
        "    MASK = \"[MASK]\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
        "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "elif model_type == \"albert\":\n",
        "    MASK = \"[MASK]\"\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "    model = AlbertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
        "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "elif model_type == \"roberta\":\n",
        "    MASK = \"<mask>\"\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
        "    unmasker = pipeline(\"fill-mask\", model=\"roberta-large\", tokenizer=tokenizer)\n",
        "\n",
        "else:\n",
        "    print(f\"{model_type} not found.\")\n",
        "\n",
        "vocab: set = set(tokenizer.get_vocab().keys())\n",
        "\n",
        "if model_type == \"roberta\":\n",
        "    vocab = set(map(lambda s: s[1:], vocab))\n",
        "\n",
        "print(f\"len vocab: {len(vocab)}\")\n",
        "print(f\"{language} {model_type} Model Loaded ...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "svt6XdEUBUV-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Stanza\n",
        "\n",
        "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CZJviZP6BUV-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "import spacy\n",
        "import spacy_stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t9K0jKLgBUV-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "if language == \"fa\":\n",
        "    stanza.install_corenlp()\n",
        "    stanza.download(\"fa\")\n",
        "    nlp = spacy_stanza.load_pipeline(\"fa\")\n",
        "\n",
        "elif language == \"en\":\n",
        "    spacy.prefer_gpu()\n",
        "    nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"{language} not supported.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WM_OSihaBUV_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def half_space_case(predicted: str, current: str):\n",
        "    wo_half_space_current = current.replace(\"‌\", \"\")\n",
        "    return wo_half_space_current == predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "tokenizer = English().tokenizer\n",
        "for i in range(100):\n",
        "    tokens = tokenizer('He was, between all other things, a real man!')\n",
        "    a = list(tokens)\n",
        "\n",
        "# print(list(tokens))\n",
        "# tokens[1].idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruDvtvPtZMTS",
        "outputId": "49b6face-19d7-4226-cd44-941604767dd8"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[He, was, ,, between, all, other, things, ,, a, real, man, !]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "tokenizer = English().tokenizer\n",
        "for i in range(100):\n",
        "    tokens = nlp('He was, between all other things, a real man!')\n",
        "    a = list(tokens)\n",
        "\n",
        "# print(list(tokens))\n",
        "# tokens[1].idx"
      ],
      "metadata": {
        "id": "B2lIbsYHbjju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eU-cld58BUWD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Correction Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "PC6m3_-NBUWD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "from spacy.lang.en import English\n",
        "\n",
        "class SpellCorrector:\n",
        "    def __init__(self, alpha=5, max_edit_distance=2, verbose=False, top_k=50):\n",
        "        self.alpha = alpha\n",
        "        self.max_edit_distance = max_edit_distance\n",
        "        self.verbose = verbose\n",
        "        self.top_k = top_k\n",
        "        self.spell_checker = SpellChecker()\n",
        "        # Create a Tokenizer with the default settings for English\n",
        "        # including punctuation rules and exceptions\n",
        "        self.tokenizer = English().tokenizer\n",
        "\n",
        "    def print_summary(self, type):\n",
        "        text = self.text\n",
        "        current_token = self.current_token\n",
        "        start_char_index = self.start_char_index\n",
        "        end_char_index = self.end_char_index\n",
        "\n",
        "        if self.verbose:\n",
        "          print(\"*\" * 50)\n",
        "          print(f\"Token: {current_token.text}\")\n",
        "\n",
        "          print(\"Filtered Predicts: \\n\")\n",
        "\n",
        "          if current_token.text in string.punctuation:\n",
        "              print(self.filtered_predicts[[\"token_str\", \"score\"]])\n",
        "          else:\n",
        "              print(self.filtered_predicts[[\"token_str\", \"score\", \"total_score\"]])\n",
        "\n",
        "        if self.some_token_corrected:\n",
        "            print(f\"{current_token.text} -> {self.selected_predict} : {type}\")\n",
        "            typo_correction_details = {\n",
        "                \"raw\": current_token.text,\n",
        "                \"corrected\": self.selected_predict,\n",
        "                \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
        "                \"around\": text[start_char_index - 10 : end_char_index + 10],\n",
        "                \"type\": \"contextual\",\n",
        "            }\n",
        "\n",
        "            print(typo_correction_details)\n",
        "        if self.verbose:\n",
        "            print(\"#\" * 50)\n",
        "\n",
        "    def set_predictions(self):\n",
        "        start_char_index: int = self.current_token.idx\n",
        "        end_char_index = start_char_index + len(self.current_token)\n",
        "\n",
        "        masked_text = (\n",
        "            self.text[:start_char_index] + MASK + self.text[end_char_index:]\n",
        "        )\n",
        "\n",
        "        predicts = unmasker(masked_text, top_k=self.top_k)\n",
        "        predicts = pd.DataFrame(predicts)\n",
        "        \n",
        "        self.predicts = predicts\n",
        "        self.start_char_index = start_char_index\n",
        "        self.end_char_index = end_char_index\n",
        "        self.masked_text = masked_text\n",
        "        return predicts\n",
        "\n",
        "    def set_filtered_predictions(self):\n",
        "        predicts = self.predicts\n",
        "        predicts.loc[:, \"token_str\"] = predicts[\"token_str\"].apply(\n",
        "            lambda tk: tk.replace(\" \", \"\")\n",
        "        )\n",
        "        predicts.loc[:, \"edit_distance\"] = predicts[\"token_str\"].apply(\n",
        "            lambda tk: editdistance.eval(self.current_token.text, tk)\n",
        "        )\n",
        "\n",
        "        # Filter tokens with at most 3 edit distance\n",
        "        filtered_predicts = predicts.loc[\n",
        "            predicts[\"edit_distance\"] <= self.max_edit_distance, :\n",
        "        ].copy()\n",
        "\n",
        "        # Apply total score function\n",
        "        # e: edit distance + 1\n",
        "        # l: token length\n",
        "        filtered_predicts.loc[:, \"e_to_l\"] = (\n",
        "            filtered_predicts.loc[:, \"edit_distance\"] + 1\n",
        "        ) / len(self.current_token.text)\n",
        "\n",
        "        filtered_predicts.loc[:, \"total_score\"] = (\n",
        "            filtered_predicts.loc[:, \"score\"]\n",
        "            / filtered_predicts.loc[:, \"e_to_l\"] ** self.alpha\n",
        "        )\n",
        "\n",
        "        filtered_predicts = filtered_predicts.sort_values(\n",
        "            \"total_score\", ascending=False\n",
        "        )\n",
        "        self.filtered_predicts = filtered_predicts\n",
        "\n",
        "    def correct_predict(self, selected_predict):\n",
        "        if selected_predict != self.current_token.text:\n",
        "            if not half_space_case(selected_predict, self.current_token.text):\n",
        "                self.some_token_corrected = True\n",
        "                self.text = self.masked_text.replace(MASK, selected_predict, 1)\n",
        "\n",
        "            else:\n",
        "                vocab.add(self.current_token.text)\n",
        "\n",
        "        self.selected_predict = selected_predict\n",
        "\n",
        "\n",
        "    def correct_lexico_typo(self):\n",
        "        while True:\n",
        "            self.some_token_corrected = False\n",
        "            self.tokens = list(self.tokenizer(self.text))\n",
        "            for index, current_token in enumerate(self.tokens):\n",
        "                self.current_token: Token = current_token\n",
        "\n",
        "                if current_token.text not in vocab:\n",
        "                    self.set_predictions()\n",
        "\n",
        "                    try:\n",
        "                        if current_token.text in string.punctuation:\n",
        "                            selected_predict = self.predicts[\"token_str\"].iloc[0]\n",
        "                        elif any(c.isdigit() for c in current_token.text):\n",
        "                            print(\"DIGIT\")\n",
        "                            selected_predict = current_token.text\n",
        "                        else:\n",
        "                            self.set_filtered_predictions()\n",
        "                            selected_predict_row = self.filtered_predicts.iloc[0, :]\n",
        "                            selected_predict = selected_predict_row[\"token_str\"]\n",
        "                    except Exception as e:\n",
        "                        print(\n",
        "                            f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(self.filtered_predicts)}\"\n",
        "                        )\n",
        "                        selected_predict = self.spell_checker.correction(self.current_token.text)\n",
        "\n",
        "                    self.correct_predict(selected_predict)\n",
        "                    self.print_summary('lexical')\n",
        "\n",
        "                    if self.some_token_corrected:\n",
        "                        break\n",
        "\n",
        "            if not self.some_token_corrected:\n",
        "                break\n",
        "\n",
        "\n",
        "    def correct_contextual_typo(self):\n",
        "        index = 0\n",
        "        while True:\n",
        "            self.some_token_corrected = False\n",
        "            self.tokens = list(self.tokenizer(self.text))\n",
        "            for j in range(index, len(self.tokens)):\n",
        "                current_token: Token = self.tokens[j]\n",
        "                self.current_token = current_token\n",
        "                self.set_predictions()\n",
        "\n",
        "                try:\n",
        "                    if current_token.text in string.punctuation:\n",
        "                        self.filtered_predicts = self.predicts.loc[\n",
        "                            self.predicts[\"token_str\"].apply(lambda tk: tk in string.punctuation), :\n",
        "                        ].copy()\n",
        "                        selected_predict = self.filtered_predicts[\"token_str\"].iloc[0]\n",
        "                    elif any(c.isdigit() for c in current_token.text):\n",
        "                        selected_predict = current_token.text\n",
        "                    else:\n",
        "                        self.set_filtered_predictions()\n",
        "                        selected_predict_row = self.filtered_predicts.iloc[0, :]\n",
        "                        selected_predict = selected_predict_row[\"token_str\"]\n",
        "\n",
        "                except Exception as e:\n",
        "                    selected_predict = current_token.text\n",
        "                    print(\n",
        "                        f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(self.filtered_predicts)}\"\n",
        "                    )\n",
        "\n",
        "                self.correct_predict(selected_predict)\n",
        "                self.print_summary('contexual')\n",
        "                index += 1\n",
        "                if self.some_token_corrected:\n",
        "                    break\n",
        "\n",
        "            if not self.some_token_corrected:\n",
        "                break\n",
        "\n",
        "\n",
        "    def correction_pipeline(self):\n",
        "        print(f\"Lexico Correction ... . text = {self.text}\") if self.verbose else print()\n",
        "        self.correct_lexico_typo()\n",
        "\n",
        "        print(f\"Contextual Correction ... . text = {self.text}\") if self.verbose else print()\n",
        "        self.correct_contextual_typo()\n",
        "\n",
        "\n",
        "    def __call__(self, text, *args, **kwargs):\n",
        "        self.text = text\n",
        "        self.correction_pipeline()\n",
        "        return self.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cY2Jwvz4BUWE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Sample Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urftUEFFBUWE",
        "outputId": "75fea60c-b0e3-4207-904f-ea569c6357d7",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lexico Correction ... . text = The quantity thoery of money also assume that the quantity of money in an economy has a large influense on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the sopply of gods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
            "**************************************************\n",
            "Token: thoery\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "1      theory  0.317603    10.163292\n",
            "6      Theory  0.000769     0.005841\n",
            "285     theor  0.000001     0.000009\n",
            "thoery -> theory : lexical\n",
            "{'raw': 'thoery', 'corrected': 'theory', 'span': '[13, 19]', 'around': ' quantity theory of money ', 'type': 'contextual'}\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: influense\n",
            "Filtered Predicts: \n",
            "\n",
            "      token_str         score  total_score\n",
            "2     influence  2.733823e-01   504.467189\n",
            "20    Influence  4.641749e-05     0.011279\n",
            "47   influences  1.355756e-05     0.003294\n",
            "177  influenced  1.294747e-06     0.000315\n",
            "217   inference  9.282696e-07     0.000054\n",
            "influense -> influence : lexical\n",
            "{'raw': 'influense', 'corrected': 'influence', 'span': '[94, 103]', 'around': 's a large influence on its le', 'type': 'contextual'}\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: sopply\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "0      supply  0.339622    82.528057\n",
            "319    Supply  0.000026     0.000836\n",
            "sopply -> supply : lexical\n",
            "{'raw': 'sopply', 'corrected': 'supply', 'span': '[238, 244]', 'around': 'ge in the supply of gods a', 'type': 'contextual'}\n",
            "##################################################\n",
            "Contextual Correction ... . text = The quantity theory of money also assume that the quantity of money in an economy has a large influence on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the supply of gods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
            "**************************************************\n",
            "Token: The\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score   total_score\n",
            "0         The  0.898594  2.183583e+02\n",
            "1           A  0.079585  1.888578e-02\n",
            "63        The  0.000033  7.964768e-03\n",
            "12        the  0.000528  4.010519e-03\n",
            "2        This  0.002411  2.410746e-03\n",
            "..        ...       ...           ...\n",
            "482       Com  0.000001  3.020523e-07\n",
            "484        Li  0.000001  3.007324e-07\n",
            "491       Red  0.000001  2.944897e-07\n",
            "496         -  0.000001  2.870135e-07\n",
            "497         …  0.000001  2.868909e-07\n",
            "\n",
            "[161 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: quantity\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "3    quantity  0.036462  1194.776855\n",
            "194  Quantity  0.000154     0.157331\n",
            "280   quality  0.000089     0.012007\n",
            "137   quantum  0.000251     0.008018\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: theory\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score  total_score\n",
            "1      theory  1.558185e-01  1211.644479\n",
            "0    theories  8.207021e-01     6.232207\n",
            "8      Theory  3.448786e-04     0.083805\n",
            "360     theor  4.482009e-07     0.000109\n",
            "351   theorem  4.795615e-07     0.000015\n",
            "396   theoret  3.682653e-07     0.000012\n",
            "239  theorist  1.001083e-06     0.000008\n",
            "390    thesis  3.741218e-07     0.000003\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: of\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          of  9.905010e-01  3.169603e+01\n",
            "16         of  1.657717e-05  5.304693e-04\n",
            "3          on  3.708406e-04  3.708406e-04\n",
            "1         and  7.438395e-03  2.324498e-04\n",
            "4          or  1.453247e-04  1.453247e-04\n",
            "..        ...           ...           ...\n",
            "449       why  1.386008e-07  4.331275e-09\n",
            "461      some  1.350873e-07  4.221478e-09\n",
            "473      good  1.313393e-07  4.104353e-09\n",
            "475       tax  1.309800e-07  4.093126e-09\n",
            "498       ist  1.247758e-07  3.899243e-09\n",
            "\n",
            "[81 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: money\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "0       money  0.875531  2736.035362\n",
            "184     money  0.000028     0.089001\n",
            "25      Money  0.000653     0.063754\n",
            "39      power  0.000378     0.001153\n",
            "66   monetary  0.000165     0.000502\n",
            "472     monet  0.000004     0.000421\n",
            "348      many  0.000008     0.000106\n",
            "191    models  0.000027     0.000083\n",
            "248     bonds  0.000014     0.000043\n",
            "294      mind  0.000011     0.000034\n",
            "368       men  0.000007     0.000021\n",
            "389       man  0.000006     0.000018\n",
            "402    modern  0.000006     0.000017\n",
            "412     today  0.000005     0.000016\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: also\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "14       also  0.003612     3.698655\n",
            "4        must  0.033398     0.033398\n",
            "75        all  0.000499     0.002102\n",
            "38         is  0.001274     0.001274\n",
            "39       laws  0.001259     0.001259\n",
            "58         to  0.000739     0.000739\n",
            "74        use  0.000504     0.000504\n",
            "91         do  0.000373     0.000373\n",
            "105       has  0.000312     0.000312\n",
            "209       and  0.000094     0.000094\n",
            "469        as  0.000021     0.000087\n",
            "226        's  0.000080     0.000080\n",
            "293         s  0.000051     0.000051\n",
            "303      acts  0.000047     0.000047\n",
            "318     value  0.000043     0.000043\n",
            "328        is  0.000042     0.000042\n",
            "336      flow  0.000040     0.000040\n",
            "389       ors  0.000030     0.000030\n",
            "428     sales  0.000025     0.000025\n",
            "447       two  0.000023     0.000023\n",
            "452     deals  0.000022     0.000022\n",
            "468      ness  0.000021     0.000021\n",
            "476       ism  0.000020     0.000020\n",
            "498      aims  0.000019     0.000019\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: assume\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "0     assumes  0.691179   167.956492\n",
            "43     assume  0.000319     2.482488\n",
            "31    assumed  0.000614     0.149153\n",
            "121   assures  0.000019     0.000603\n",
            "143     argue  0.000011     0.000084\n",
            "150  assuming  0.000010     0.000078\n",
            "240    assert  0.000005     0.000035\n",
            "398   presume  0.000001     0.000010\n",
            "423      sums  0.000001     0.000009\n",
            "assume -> assumes : contexual\n",
            "{'raw': 'assume', 'corrected': 'assumes', 'span': '[34, 40]', 'around': 'oney also assumes that the', 'type': 'contextual'}\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: that\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0        that  9.998802e-01  1.023877e+03\n",
            "4        that  5.357045e-06  5.485614e-03\n",
            "2        than  1.057601e-05  3.384324e-04\n",
            "1         the  5.504816e-05  2.319725e-04\n",
            "17       That  7.510820e-07  2.403463e-05\n",
            "..        ...           ...           ...\n",
            "422      past  4.478705e-09  4.478705e-09\n",
            "434       Mac  4.278291e-09  4.278291e-09\n",
            "453       let  4.083231e-09  4.083231e-09\n",
            "470       top  3.954344e-09  3.954344e-09\n",
            "480      Root  3.813823e-09  3.813823e-09\n",
            "\n",
            "[74 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: the\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0         the  9.994556e-01  2.428677e+02\n",
            "12        the  5.485751e-06  1.333037e-03\n",
            "9         The  1.064842e-05  8.086145e-05\n",
            "1           a  1.758088e-04  4.172026e-05\n",
            "4        this  2.585363e-05  2.585363e-05\n",
            "..        ...           ...           ...\n",
            "458        up  3.298837e-08  7.828295e-09\n",
            "459         .  3.295516e-08  7.820415e-09\n",
            "468         X  3.228091e-08  7.660412e-09\n",
            "476     trade  3.154167e-08  7.484985e-09\n",
            "480         s  3.114293e-08  7.390364e-09\n",
            "\n",
            "[92 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: quantity\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str     score   total_score\n",
            "0     quantity  0.427253  14000.221680\n",
            "43     quality  0.000183      0.024744\n",
            "40     quantum  0.000205      0.006549\n",
            "57  quantities  0.000114      0.003644\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: of\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          of  9.999288e-01  3.199772e+01\n",
            "5          of  3.408066e-06  1.090581e-04\n",
            "2          or  7.674630e-06  7.674630e-06\n",
            "3          on  6.282205e-06  6.282205e-06\n",
            "1          in  1.911147e-05  2.516737e-06\n",
            "..        ...           ...           ...\n",
            "302       its  7.556557e-09  2.361424e-10\n",
            "333       ing  6.782833e-09  2.119635e-10\n",
            "378       net  5.684374e-09  1.776367e-10\n",
            "387       are  5.546451e-09  1.733266e-10\n",
            "497      form  4.165668e-09  1.301771e-10\n",
            "\n",
            "[96 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: money\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0       money  9.854983e-01  3.079682e+03\n",
            "151     money  2.853518e-06  8.917245e-03\n",
            "39      Money  3.567679e-05  3.484062e-03\n",
            "36      power  3.896671e-05  1.189170e-04\n",
            "59   monetary  1.957942e-05  5.975165e-05\n",
            "481     Money  2.514150e-07  2.455225e-05\n",
            "125     bonds  3.985273e-06  1.216209e-05\n",
            "337       one  6.044377e-07  7.773118e-06\n",
            "166     notes  2.243053e-06  6.845254e-06\n",
            "233      some  1.171265e-06  3.574417e-06\n",
            "237       men  1.107156e-06  3.378772e-06\n",
            "330      mint  6.257048e-07  1.909500e-06\n",
            "366       any  4.758387e-07  1.452144e-06\n",
            "498       new  2.314819e-07  7.064267e-07\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: in\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          in  9.869292e-01  3.158173e+01\n",
            "24         in  1.572929e-05  5.033371e-04\n",
            "11         on  6.120354e-05  6.120354e-05\n",
            "3          of  4.630880e-04  6.098279e-05\n",
            "34         In  9.152927e-06  9.152927e-06\n",
            "..        ...           ...           ...\n",
            "418      runs  7.398784e-08  2.312120e-09\n",
            "423      even  7.295451e-08  2.279828e-09\n",
            "436       out  6.976654e-08  2.180204e-09\n",
            "495      this  5.486974e-08  1.714680e-09\n",
            "499     iting  5.413872e-08  1.691835e-09\n",
            "\n",
            "[68 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: an\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          an  6.002265e-01  1.920725e+01\n",
            "1         the  3.945311e-01  1.232910e-02\n",
            "2         any  4.136193e-03  4.136193e-03\n",
            "4           a  2.475954e-04  2.475954e-04\n",
            "14         an  6.825248e-06  2.184079e-04\n",
            "..        ...           ...           ...\n",
            "436      Sage  2.681790e-08  8.380594e-10\n",
            "470      near  2.527028e-08  7.896963e-10\n",
            "475      able  2.514096e-08  7.856549e-10\n",
            "487      past  2.469554e-08  7.717356e-10\n",
            "493       GDP  2.422028e-08  7.568839e-10\n",
            "\n",
            "[112 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: economy\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str         score   total_score\n",
            "0      economy  9.915640e-01  1.666522e+04\n",
            "13     Economy  1.513220e-05  7.947716e-03\n",
            "12    economic  1.834695e-05  1.268960e-03\n",
            "10   economies  2.331843e-05  3.827273e-04\n",
            "85      econom  1.562010e-07  8.203969e-05\n",
            "124     econom  7.978757e-08  4.190593e-05\n",
            "40   economics  9.463779e-07  1.553298e-05\n",
            "78     ecology  2.050718e-07  1.418371e-05\n",
            "64   economist  2.920279e-07  4.793079e-06\n",
            "313   economic  8.654975e-09  5.986180e-07\n",
            "431   Economic  4.051665e-09  6.650032e-08\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: has\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0         has  9.889457e-01  2.403138e+02\n",
            "27        has  2.968034e-05  7.212322e-03\n",
            "8         had  3.986272e-04  3.027075e-03\n",
            "2          is  1.804234e-03  1.804234e-03\n",
            "3        have  1.503944e-03  1.503944e-03\n",
            "..        ...           ...           ...\n",
            "462        on  6.011386e-08  1.426530e-08\n",
            "478       but  5.470110e-08  1.298083e-08\n",
            "486      ties  5.313130e-08  1.260831e-08\n",
            "489     plans  5.292033e-08  1.255824e-08\n",
            "491     names  5.257123e-08  1.247540e-08\n",
            "\n",
            "[120 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: a\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0           a  9.989552e-01  9.989552e-01\n",
            "15          a  5.013995e-06  5.013995e-06\n",
            "4          an  5.317314e-05  1.661661e-06\n",
            "3          no  8.921009e-05  3.671197e-07\n",
            "21          A  2.474034e-06  7.731357e-08\n",
            "..        ...           ...           ...\n",
            "443      take  9.999386e-09  9.765026e-12\n",
            "458       und  9.406248e-09  9.185789e-12\n",
            "460      bear  9.348762e-09  9.129650e-12\n",
            "461       now  9.337820e-09  9.118965e-12\n",
            "470       own  8.791232e-09  8.585187e-12\n",
            "\n",
            "[97 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: large\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "6       large  0.023442    73.257700\n",
            "118    larger  0.000164     0.016016\n",
            "25       huge  0.002289     0.006984\n",
            "77     marked  0.000389     0.001188\n",
            "129      true  0.000144     0.000440\n",
            "206      free  0.000056     0.000171\n",
            "477   largest  0.000009     0.000118\n",
            "355     force  0.000016     0.000048\n",
            "374      core  0.000015     0.000045\n",
            "390    varied  0.000013     0.000041\n",
            "434      base  0.000011     0.000034\n",
            "471      pure  0.000010     0.000030\n",
            "476      sure  0.000009     0.000028\n",
            "492     loose  0.000008     0.000026\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: influence\n",
            "Filtered Predicts: \n",
            "\n",
            "       token_str         score   total_score\n",
            "1      influence  3.126397e-01  18461.060670\n",
            "20     Influence  3.331276e-05      0.061471\n",
            "52    influences  7.183206e-06      0.013255\n",
            "216   influenced  4.803296e-07      0.000886\n",
            "206    inference  5.271024e-07      0.000128\n",
            "113  influencing  1.810250e-06      0.000104\n",
            "148    incidence  1.037744e-06      0.000060\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: on\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          on  8.839580e-01  2.828666e+01\n",
            "1        over  1.058189e-01  3.306840e-03\n",
            "3          in  2.357888e-03  2.357888e-03\n",
            "2        upon  6.404320e-03  8.433673e-04\n",
            "4          of  3.725458e-04  3.725458e-04\n",
            "..        ...           ...           ...\n",
            "411      most  2.979232e-08  9.310101e-10\n",
            "459      fore  2.521747e-08  7.880460e-10\n",
            "475       yet  2.392109e-08  7.475342e-10\n",
            "485       ...  2.327879e-08  7.274623e-10\n",
            "498      mine  2.242555e-08  7.007984e-10\n",
            "\n",
            "[141 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: its\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "1         its  3.120422e-03  7.582626e-01\n",
            "0         the  9.963103e-01  2.364291e-01\n",
            "16         it  4.712980e-06  3.578919e-05\n",
            "3           a  6.062079e-05  1.438560e-05\n",
            "4         any  5.574007e-05  1.322738e-05\n",
            "..        ...           ...           ...\n",
            "435     rates  2.486616e-08  5.900856e-09\n",
            "458         T  2.357219e-08  5.593791e-09\n",
            "465      cost  2.266483e-08  5.378470e-09\n",
            "476       sum  2.194489e-08  5.207626e-09\n",
            "491       put  2.100445e-08  4.984454e-09\n",
            "\n",
            "[90 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: level\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "0       level  0.668393  2088.728361\n",
            "1      levels  0.116275    11.355001\n",
            "198     level  0.000026     0.082272\n",
            "93      Level  0.000108     0.010514\n",
            "40      model  0.000532     0.001623\n",
            "475    levels  0.000004     0.000412\n",
            "325    levers  0.000009     0.000114\n",
            "413    Levels  0.000005     0.000070\n",
            "220      wave  0.000020     0.000061\n",
            "285     lines  0.000011     0.000035\n",
            "338      life  0.000008     0.000025\n",
            "384      line  0.000006     0.000019\n",
            "403     waves  0.000006     0.000018\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: of\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          of  9.997057e-01  3.199058e+01\n",
            "1          or  8.803527e-05  8.803527e-05\n",
            "11         of  2.417054e-06  7.734573e-05\n",
            "3          on  4.310675e-05  4.310675e-05\n",
            "4          in  3.807178e-05  5.013567e-06\n",
            "..        ...           ...           ...\n",
            "415      onto  8.804282e-09  2.751338e-10\n",
            "416      both  8.778259e-09  2.743206e-10\n",
            "430      only  8.456022e-09  2.642507e-10\n",
            "477       fir  7.569410e-09  2.365441e-10\n",
            "478       dev  7.566552e-09  2.364547e-10\n",
            "\n",
            "[147 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: economic\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str     score   total_score\n",
            "0     economic  0.967512  31703.419922\n",
            "51    economic  0.000048      1.560493\n",
            "77    Economic  0.000023      0.023839\n",
            "33  economical  0.000090      0.012097\n",
            "56     economy  0.000040      0.005372\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: activity\n",
            "Filtered Predicts: \n",
            "\n",
            "      token_str     score   total_score\n",
            "0      activity  0.907471  29736.007812\n",
            "125    activity  0.000008      0.259826\n",
            "5    activities  0.002968      0.094965\n",
            "96     Activity  0.000013      0.012801\n",
            "49      utility  0.000036      0.001138\n",
            "130     ability  0.000007      0.000238\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: .\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score\n",
            "0           .  9.991990e-01\n",
            "1           .  2.882220e-04\n",
            "2           :  2.475906e-04\n",
            "3           ,  9.676351e-05\n",
            "6           !  2.354077e-05\n",
            "9           ;  6.345346e-06\n",
            "22          ?  1.014795e-06\n",
            "41          -  1.982080e-07\n",
            "69         -.  5.178401e-08\n",
            "72          ,  4.622063e-08\n",
            "80          /  3.562931e-08\n",
            "81          !  3.559141e-08\n",
            "102         ?  1.882833e-08\n",
            "113         '  1.277600e-08\n",
            "122         *  1.134251e-08\n",
            "134        ./  9.112465e-09\n",
            "162         \\  7.239335e-09\n",
            "170         ~  6.906478e-09\n",
            "174         )  6.746155e-09\n",
            "186         |  6.091714e-09\n",
            "239         `  4.298303e-09\n",
            "247         ]  4.141546e-09\n",
            "265         \"  3.878767e-09\n",
            "356         >  2.408592e-09\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: So\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "8          So  1.513556e-02  4.843380e-01\n",
            "182        So  2.361956e-06  7.558260e-05\n",
            "52        For  1.328066e-04  1.748893e-05\n",
            "40       Also  4.215275e-04  1.317273e-05\n",
            "66        Say  6.266546e-05  8.252241e-06\n",
            "..        ...           ...           ...\n",
            "438       and  2.545459e-07  7.954561e-09\n",
            "439      Safe  2.545032e-07  7.953226e-09\n",
            "456       May  2.415985e-07  7.549953e-09\n",
            "470      Jobs  2.235188e-07  6.984962e-09\n",
            "477       Way  2.158269e-07  6.744590e-09\n",
            "\n",
            "[84 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: ,\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score\n",
            "0           ,  8.977565e-01\n",
            "2           :  1.663693e-03\n",
            "7           ;  1.457745e-04\n",
            "41          ,  1.443423e-05\n",
            "68          .  7.453327e-06\n",
            "114         -  3.571060e-06\n",
            "194         '  1.536908e-06\n",
            "216         )  1.375853e-06\n",
            "228         \"  1.278270e-06\n",
            "320         ]  7.120806e-07\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: a\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0           a  7.571718e-01  7.571718e-01\n",
            "1         any  1.265496e-01  5.207803e-04\n",
            "2         the  6.283136e-02  6.135875e-05\n",
            "4        each  2.528893e-02  2.469622e-05\n",
            "18          a  1.701921e-05  1.701921e-05\n",
            "..        ...           ...           ...\n",
            "430       sum  1.868465e-07  1.824673e-10\n",
            "436       key  1.853373e-07  1.809935e-10\n",
            "447      same  1.794262e-07  1.752209e-10\n",
            "463       not  1.699894e-07  1.660053e-10\n",
            "478       THE  1.650179e-07  1.611503e-10\n",
            "\n",
            "[79 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: change\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str         score  total_score\n",
            "0       change  9.777452e-01  7602.946484\n",
            "5      changes  8.245748e-04     0.200372\n",
            "134     change  2.051736e-06     0.015954\n",
            "42      Change  1.492587e-05     0.003627\n",
            "44     changed  1.399046e-05     0.003400\n",
            "17    changing  1.089703e-04     0.000827\n",
            "371     charge  3.720815e-07     0.000090\n",
            "155      range  1.686863e-06     0.000054\n",
            "498     chance  2.181779e-07     0.000053\n",
            "97      choice  3.646290e-06     0.000028\n",
            "112     plunge  2.790305e-06     0.000021\n",
            "178      large  1.343402e-06     0.000010\n",
            "184      share  1.295199e-06     0.000010\n",
            "348      phase  4.288468e-07     0.000003\n",
            "379       case  3.569656e-07     0.000003\n",
            "381      shape  3.567900e-07     0.000003\n",
            "388  challenge  3.468141e-07     0.000003\n",
            "398       hand  3.334020e-07     0.000003\n",
            "416      thing  3.026917e-07     0.000002\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: in\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          in  9.940101e-01  3.180832e+01\n",
            "1          to  3.464842e-03  4.562754e-04\n",
            "6          in  1.024939e-05  3.279805e-04\n",
            "2          of  2.275873e-03  2.997034e-04\n",
            "3          on  9.350674e-05  9.350674e-05\n",
            "..        ...           ...           ...\n",
            "444     index  6.589582e-09  2.059244e-10\n",
            "450      size  6.490638e-09  2.028324e-10\n",
            "458      trim  6.312601e-09  1.972688e-10\n",
            "472       tax  6.057283e-09  1.892901e-10\n",
            "482     ising  5.872645e-09  1.835201e-10\n",
            "\n",
            "[135 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: the\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0         the  9.992977e-01  2.428294e+02\n",
            "19        the  2.932333e-06  7.125568e-04\n",
            "2        this  1.168785e-04  1.168785e-04\n",
            "4        that  7.503818e-05  7.503818e-05\n",
            "1           a  1.283248e-04  3.045209e-05\n",
            "..        ...           ...           ...\n",
            "446        so  4.645944e-08  1.102504e-08\n",
            "453      area  4.495023e-08  1.066690e-08\n",
            "467        no  4.338152e-08  1.029464e-08\n",
            "478        GN  4.184088e-08  9.929038e-09\n",
            "488      open  4.065053e-08  9.646561e-09\n",
            "\n",
            "[97 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: money\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0       money  9.947546e-01  3.108608e+03\n",
            "30      money  6.111820e-06  1.909944e-02\n",
            "1    monetary  3.563516e-03  1.087499e-02\n",
            "6       Money  4.061904e-05  3.966704e-03\n",
            "224     monet  2.877178e-07  2.809744e-05\n",
            "23      power  8.996233e-06  2.745432e-05\n",
            "315     Money  1.773613e-07  1.732044e-05\n",
            "36        new  5.163186e-06  1.575679e-05\n",
            "42       mint  4.191576e-06  1.279168e-05\n",
            "75       bond  1.720213e-06  5.249674e-06\n",
            "359       one  1.404634e-07  1.806371e-06\n",
            "168       net  4.871446e-07  1.486647e-06\n",
            "213     penny  3.244390e-07  9.901091e-07\n",
            "229       man  2.783534e-07  8.494671e-07\n",
            "272       key  2.202427e-07  6.721273e-07\n",
            "280     token  2.087954e-07  6.371930e-07\n",
            "287      open  2.029884e-07  6.194715e-07\n",
            "352      long  1.453573e-07  4.435954e-07\n",
            "381     moral  1.246018e-07  3.802546e-07\n",
            "436      home  1.022959e-07  3.121822e-07\n",
            "445    moment  1.003617e-07  3.062795e-07\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: supply\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score  total_score\n",
            "0      supply  9.840922e-01  7652.301224\n",
            "61     Supply  1.980135e-05     0.004812\n",
            "7    supplies  3.840620e-04     0.002916\n",
            "58    surplus  2.185273e-05     0.000166\n",
            "62   supplied  1.968087e-05     0.000149\n",
            "469    simply  6.313313e-07     0.000020\n",
            "329  supplier  1.225495e-06     0.000009\n",
            "364    survey  1.016734e-06     0.000008\n",
            "402   support  8.830274e-07     0.000007\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: results\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str         score   total_score\n",
            "0      results  9.973837e-01  1.676303e+04\n",
            "1       result  1.477097e-03  7.757992e-01\n",
            "61     results  1.131509e-06  1.901727e-02\n",
            "3     resulted  1.865922e-04  1.290558e-02\n",
            "27     Results  3.970397e-06  2.085327e-03\n",
            "6    resulting  5.882369e-05  9.654783e-04\n",
            "375     result  3.201419e-08  1.681445e-05\n",
            "104      rules  3.694030e-07  6.063044e-06\n",
            "240      rests  8.522522e-08  5.894569e-06\n",
            "267     Result  6.227057e-08  4.306920e-06\n",
            "137   resolves  2.370921e-07  3.891414e-06\n",
            "139    resides  2.356252e-07  3.867336e-06\n",
            "176    returns  1.551629e-07  2.546702e-06\n",
            "185    reports  1.415979e-07  2.324059e-06\n",
            "190     reacts  1.365870e-07  2.241814e-06\n",
            "204      rents  1.206756e-07  1.980660e-06\n",
            "250   presents  7.313263e-08  1.200332e-06\n",
            "399    repeats  2.677223e-08  4.394149e-07\n",
            "402   recruits  2.645447e-08  4.341995e-07\n",
            "416    reduces  2.449300e-08  4.020057e-07\n",
            "487   defaults  1.658430e-08  2.721996e-07\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: in\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          in  9.847873e-01  3.151319e+01\n",
            "13         in  6.511812e-06  2.083780e-04\n",
            "3          is  5.137180e-05  5.137180e-05\n",
            "8          on  1.938477e-05  1.938477e-05\n",
            "11         In  9.980631e-06  9.980631e-06\n",
            "..        ...           ...           ...\n",
            "428       see  1.303223e-08  4.072573e-10\n",
            "429      side  1.300214e-08  4.063169e-10\n",
            "438     inate  1.267263e-08  3.960198e-10\n",
            "468       say  1.183624e-08  3.698823e-10\n",
            "486      once  1.114425e-08  3.482579e-10\n",
            "\n",
            "[155 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: either\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0      either  9.980497e-01  7.760835e+03\n",
            "3      either  8.369627e-05  6.508222e-01\n",
            "4      Either  2.891616e-05  7.026627e-03\n",
            "13    neither  8.200447e-06  1.992709e-03\n",
            "2     whether  1.928004e-04  1.464078e-03\n",
            "52     Either  1.740988e-06  4.230601e-04\n",
            "75      ether  1.156991e-06  2.811487e-04\n",
            "46     rather  2.175498e-06  6.961595e-05\n",
            "35        the  3.061269e-06  2.324651e-05\n",
            "37       then  2.834096e-06  2.152142e-05\n",
            "105      with  5.760584e-07  4.374443e-06\n",
            "150   further  2.990661e-07  2.271033e-06\n",
            "494     other  4.629975e-08  1.481592e-06\n",
            "220   another  1.658868e-07  1.259703e-06\n",
            "225    within  1.624463e-07  1.233577e-06\n",
            "231     later  1.547692e-07  1.175279e-06\n",
            "241      them  1.386432e-07  1.052822e-06\n",
            "384   whether  6.547369e-08  4.971909e-07\n",
            "451     after  5.269254e-08  4.001340e-07\n",
            "452     there  5.259013e-08  3.993563e-07\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: a\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0           a  9.993917e-01  9.993917e-01\n",
            "9           a  5.423789e-06  5.423789e-06\n",
            "2          an  7.293750e-05  2.279297e-06\n",
            "1         the  2.760351e-04  2.695655e-07\n",
            "7          as  6.886298e-06  2.151968e-07\n",
            "..        ...           ...           ...\n",
            "382       per  2.790508e-08  2.725106e-11\n",
            "400      each  2.608074e-08  2.546948e-11\n",
            "407      said  2.553555e-08  2.493706e-11\n",
            "414      part  2.506783e-08  2.448030e-11\n",
            "443      hard  2.338103e-08  2.283304e-11\n",
            "\n",
            "[83 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: change\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str         score  total_score\n",
            "0       change  9.923198e-01  7716.278475\n",
            "6      changes  4.493302e-04     0.109187\n",
            "50      change  5.421109e-06     0.042155\n",
            "19      Change  2.405824e-05     0.005846\n",
            "24     changed  1.695125e-05     0.004119\n",
            "13    changing  7.045450e-05     0.000535\n",
            "203     chance  5.030639e-07     0.000122\n",
            "272     charge  3.268421e-07     0.000079\n",
            "113      range  1.216058e-06     0.000039\n",
            "445     Change  1.408398e-07     0.000034\n",
            "101     plunge  1.372068e-06     0.000010\n",
            "115     choice  1.206562e-06     0.000009\n",
            "171      share  6.522162e-07     0.000005\n",
            "190       case  5.516568e-07     0.000004\n",
            "222      cause  4.207187e-07     0.000003\n",
            "262  challenge  3.439430e-07     0.000003\n",
            "301      thing  2.777304e-07     0.000002\n",
            "384      shape  1.809392e-07     0.000001\n",
            "401      phase  1.682041e-07     0.000001\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: in\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          in  9.950235e-01  3.184075e+01\n",
            "9          in  1.217856e-05  3.897140e-04\n",
            "1          of  2.755205e-03  3.628253e-04\n",
            "3          on  2.743028e-04  2.743028e-04\n",
            "2          to  1.480132e-03  1.949144e-04\n",
            "..        ...           ...           ...\n",
            "435     index  9.520075e-09  2.975023e-10\n",
            "454       nor  9.217112e-09  2.880347e-10\n",
            "469      trim  8.942191e-09  2.794435e-10\n",
            "475       car  8.794660e-09  2.748331e-10\n",
            "495      been  8.168930e-09  2.552790e-10\n",
            "\n",
            "[178 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: the\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "0         the  0.353972    86.015164\n",
            "5         its  0.024682     0.005857\n",
            "30      their  0.001918     0.001918\n",
            "193      then  0.000090     0.000685\n",
            "26       both  0.002413     0.000573\n",
            "84       true  0.000322     0.000322\n",
            "38        all  0.001230     0.000292\n",
            "39       base  0.001221     0.000290\n",
            "98      those  0.000258     0.000258\n",
            "61        net  0.000519     0.000123\n",
            "73        new  0.000397     0.000094\n",
            "74       home  0.000396     0.000094\n",
            "188     these  0.000092     0.000092\n",
            "81       item  0.000347     0.000082\n",
            "85        key  0.000320     0.000076\n",
            "90       wage  0.000299     0.000071\n",
            "216     other  0.000070     0.000070\n",
            "236       top  0.000059     0.000059\n",
            "114      core  0.000210     0.000050\n",
            "135      some  0.000155     0.000037\n",
            "144     share  0.000144     0.000034\n",
            "146       end  0.000143     0.000034\n",
            "150        in  0.000134     0.000032\n",
            "151       oil  0.000133     0.000032\n",
            "326        to  0.000031     0.000031\n",
            "361      that  0.000025     0.000025\n",
            "192        US  0.000090     0.000021\n",
            "194       our  0.000090     0.000021\n",
            "407       her  0.000021     0.000021\n",
            "202       GDP  0.000081     0.000019\n",
            "437      time  0.000019     0.000019\n",
            "204     trade  0.000079     0.000019\n",
            "208        of  0.000076     0.000018\n",
            "213     store  0.000071     0.000017\n",
            "215       and  0.000070     0.000017\n",
            "221       low  0.000064     0.000015\n",
            "224    either  0.000063     0.000015\n",
            "235         a  0.000059     0.000014\n",
            "281       gas  0.000044     0.000010\n",
            "288     state  0.000042     0.000010\n",
            "292      area  0.000041     0.000010\n",
            "348       set  0.000026     0.000006\n",
            "350      what  0.000026     0.000006\n",
            "352     whole  0.000026     0.000006\n",
            "360      sale  0.000025     0.000006\n",
            "371       pay  0.000024     0.000006\n",
            "380     trend  0.000024     0.000006\n",
            "389       job  0.000023     0.000005\n",
            "391        on  0.000023     0.000005\n",
            "400       any  0.000022     0.000005\n",
            "406       his  0.000021     0.000005\n",
            "409      shop  0.000021     0.000005\n",
            "428      fuel  0.000020     0.000005\n",
            "449         ,  0.000018     0.000004\n",
            "459        or  0.000017     0.000004\n",
            "460        it  0.000017     0.000004\n",
            "477       per  0.000015     0.000004\n",
            "490       CPI  0.000014     0.000003\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: price\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "1       price  0.197629   617.590407\n",
            "24     prices  0.001745     0.170379\n",
            "355     price  0.000003     0.009018\n",
            "49      trade  0.000232     0.000707\n",
            "52       rate  0.000202     0.000617\n",
            "65    pricing  0.000147     0.000450\n",
            "68    service  0.000141     0.000430\n",
            "393     Price  0.000002     0.000217\n",
            "100      risk  0.000055     0.000168\n",
            "178     crime  0.000012     0.000158\n",
            "495    priced  0.000001     0.000143\n",
            "151    policy  0.000020     0.000061\n",
            "202     grade  0.000009     0.000028\n",
            "227      paid  0.000007     0.000022\n",
            "264     force  0.000005     0.000015\n",
            "360     noise  0.000003     0.000008\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: levels\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str     score  total_score\n",
            "0       level  0.553504   134.501426\n",
            "6      levels  0.004571    35.540747\n",
            "351     Level  0.000008     0.000240\n",
            "446   leveled  0.000005     0.000145\n",
            "372      even  0.000007     0.000050\n",
            "455  leveling  0.000004     0.000032\n",
            "levels -> level : contexual\n",
            "{'raw': 'levels', 'corrected': 'level', 'span': '[213, 219]', 'around': 'the price level or a chang', 'type': 'contextual'}\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: or\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "1          or  4.551732e-01  1.456554e+01\n",
            "0           ,  5.435048e-01  7.157265e-02\n",
            "2          of  4.571580e-04  4.571580e-04\n",
            "17         or  7.220962e-06  2.310708e-04\n",
            "3           ,  2.395564e-04  3.154652e-05\n",
            "..        ...           ...           ...\n",
            "450      post  2.912981e-08  9.103066e-10\n",
            "474      both  2.738518e-08  8.557870e-10\n",
            "476      sear  2.695979e-08  8.424935e-10\n",
            "478       two  2.692310e-08  8.413470e-10\n",
            "492       end  2.643703e-08  8.261573e-10\n",
            "\n",
            "[215 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: a\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0           a  9.975417e-01  9.975417e-01\n",
            "12          a  4.530420e-06  4.530420e-06\n",
            "1         the  2.058981e-03  2.010724e-06\n",
            "2          an  5.857213e-05  1.830379e-06\n",
            "3          in  5.239408e-05  2.156135e-07\n",
            "..        ...           ...           ...\n",
            "398      near  3.690951e-08  3.604445e-11\n",
            "399      same  3.677249e-08  3.591063e-11\n",
            "417      vast  3.399096e-08  3.319430e-11\n",
            "466       kin  3.006852e-08  2.936379e-11\n",
            "469      mean  2.989342e-08  2.919280e-11\n",
            "\n",
            "[72 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: change\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str         score  total_score\n",
            "0       change  9.731494e-01  7567.209417\n",
            "8      changes  4.685308e-04     0.113853\n",
            "113     change  3.705376e-06     0.028813\n",
            "26     changed  2.924249e-05     0.007106\n",
            "39      Change  1.682358e-05     0.004088\n",
            "17    changing  1.224205e-04     0.000930\n",
            "189     chance  1.540488e-06     0.000374\n",
            "209      range  1.214000e-06     0.000039\n",
            "118      share  3.353869e-06     0.000025\n",
            "132     choice  2.760522e-06     0.000021\n",
            "142     plunge  2.355748e-06     0.000018\n",
            "224  challenge  1.106064e-06     0.000008\n",
            "327       case  6.287061e-07     0.000005\n",
            "353      shape  5.625890e-07     0.000004\n",
            "359      chain  5.417863e-07     0.000004\n",
            "404      large  4.488980e-07     0.000003\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: in\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          in  9.985505e-01  3.195362e+01\n",
            "6          in  1.426888e-05  4.566041e-04\n",
            "3          on  1.068372e-04  1.068372e-04\n",
            "1          of  5.871219e-04  7.731647e-05\n",
            "2          to  5.677528e-04  7.476579e-05\n",
            "..        ...           ...           ...\n",
            "388      seen  8.342340e-09  2.606981e-10\n",
            "389       set  8.338968e-09  2.605927e-10\n",
            "405       lax  7.861516e-09  2.456724e-10\n",
            "469     minus  6.387175e-09  1.995992e-10\n",
            "494     ining  5.930291e-09  1.853216e-10\n",
            "\n",
            "[133 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: the\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0         the  9.992593e-01  2.428200e+02\n",
            "26        the  3.113269e-06  7.565244e-04\n",
            "1           a  1.936965e-04  4.596508e-05\n",
            "35        The  2.004167e-06  1.521914e-05\n",
            "6       their  1.443697e-05  1.443697e-05\n",
            "..        ...           ...           ...\n",
            "467        un  6.618347e-08  1.570565e-08\n",
            "472      what  6.404843e-08  1.519899e-08\n",
            "475      core  6.379314e-08  1.513841e-08\n",
            "482       out  6.280244e-08  1.490331e-08\n",
            "489       end  6.108490e-08  1.449573e-08\n",
            "\n",
            "[78 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: supply\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str     score  total_score\n",
            "0       supply  0.393909  3063.040149\n",
            "25    supplies  0.002545     0.019324\n",
            "276     Supply  0.000027     0.006577\n",
            "74    supplier  0.000393     0.002985\n",
            "77     surplus  0.000359     0.002723\n",
            "327  supplying  0.000019     0.000145\n",
            "341   supplied  0.000018     0.000133\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: of\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0          of  9.996017e-01  3.198725e+01\n",
            "9          of  3.033605e-06  9.707536e-05\n",
            "2          or  4.182813e-05  4.182813e-05\n",
            "1         for  2.031857e-04  2.675696e-05\n",
            "5          on  2.022615e-05  2.022615e-05\n",
            "..        ...           ...           ...\n",
            "445       exp  9.786309e-09  3.058221e-10\n",
            "458      core  9.295122e-09  2.904726e-10\n",
            "467       apt  9.009176e-09  2.815367e-10\n",
            "474       rad  8.886638e-09  2.777074e-10\n",
            "480       her  8.817179e-09  2.755368e-10\n",
            "\n",
            "[134 rows x 3 columns]\n",
            "##################################################\n",
            "**************************************************\n",
            "Token: gods\n",
            "Filtered Predicts: \n",
            "\n",
            "    token_str         score   total_score\n",
            "0       goods  9.919076e-01  3.174104e+01\n",
            "3        good  1.732111e-04  7.299103e-04\n",
            "12      Goods  3.236539e-05  1.363875e-04\n",
            "13       jobs  2.879033e-05  1.213222e-04\n",
            "23       food  6.012538e-06  6.012538e-06\n",
            "..        ...           ...           ...\n",
            "475     games  5.803738e-09  5.803738e-09\n",
            "480     flows  5.709772e-09  5.709772e-09\n",
            "483      sees  5.579548e-09  5.579548e-09\n",
            "484      hits  5.557901e-09  5.557901e-09\n",
            "490      some  5.512209e-09  5.512209e-09\n",
            "\n",
            "[65 rows x 3 columns]\n",
            "gods -> goods : contexual\n",
            "{'raw': 'gods', 'corrected': 'goods', 'span': '[248, 252]', 'around': 'supply of goods and serv', 'type': 'contextual'}\n",
            "##################################################\n"
          ]
        }
      ],
      "source": [
        "if language == \"en\":\n",
        "    test_cases = [\n",
        "        # {\n",
        "        #     \"input_text\": \"\"\"\n",
        "        #     When he was walking on the roog, he saw a start that was very shining.\n",
        "        # \"\"\",\n",
        "        #     \"true_text\": \"\"\"\n",
        "        #     When he was walking on the roof, he saw a star that was very shining.\n",
        "        # \"\"\",\n",
        "        # },    \n",
        "        # {\n",
        "        #     \"input_text\": \"\"\"\n",
        "        #     Being one of the larges cities in word, tehran is always crowded with pollution walking left and write.\n",
        "        # \"\"\",\n",
        "        #     \"true_text\": \"\"\"\n",
        "        #     Being one of the larges cities in world, tehran is always crowded with population walking left and right.\n",
        "        # \"\"\",\n",
        "        # },    \n",
        "        # {\n",
        "        #     \"input_text\": \"\"\"\n",
        "        #     I was playing fotball, but then I broke my legg. The goal keeper saved a very powerfull shout. It as a very good hatch.\n",
        "        # \"\"\",\n",
        "        #     \"true_text\": \"\"\"\n",
        "        #     I was playing football, but then I broke my leg. The goal keeper saved a very powerfull shot. It was a very good match.\n",
        "        # \"\"\",\n",
        "        # },    \n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The quantity thoery of money also assume that the quantity of money in an economy has a large influense on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the sopply of gods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The quantity theory of money also assumes that the quantity of money in an economy has a large influence on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the supply of goods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Does it privent Iran from getting nuclear weapens. Many exports say that if all parties adhered to their pledges, the deal almost certainly could have achieved that goal for longer than a dekade!\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Does it prevent Iran from getting nuclear weapons? Many experts say that if all parties adhere to their pledges, the deal almost certainly could have achieved that goal for longer than a decade.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The Federal Reserve monitor risks to the financal system and works to help insure the system supports a haelthy economy for US households, communities, and busineses.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The Federal Reserve monitors risks to the financial system and works to help ensure the system supports a healthy economy for US households, communities, and businesses.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Bitcoin is a decentrallized digital curency that can be transfered on the peer-to-peer bitcoin network. Bitcoin transactions are veryfied by network nodes throgh cryptography and recorded in a public distributed ledger called a blockchain. The criptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The curency began use in 2009 when its implemntation was released as open-source software.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Bitcoin is a decentralized digital currency that can be transferred on the peer-to-peer bitcoin network. Bitcoin transactions are verified by network nodes through cryptography and recorded in a public distributed ledger called a blockchain. The cryptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The 2022 FILA World Cup is scheduled to be the 22nd running of the FILA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 Novamber to 18 Decamber 2022.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The 2022 FIFA World Cup is scheduled to be the 22nd running of the FIFA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 November to 18 December 2022.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            President Daneld Trump annonced on Tuesday he well withdraw the United States from the Iran nuclear deal and restore far-reaching sanktions aimed at withdrawal Iran from the global finansial system.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            President Donald Trump announced on Tuesday he will withdraw the United States from the Iran nuclear deal and restore far-reaching sanctions aimed at withdrawal Iran from the global financial system.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Cars has very sweet features. It has two beautifull eye, adorable tiny paws, sharp claws, and two fury ear which are very sensitive to sounds. It has a tiny body covered with sot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Cat has very sweet features. It has two beautiful eyes, adorable tiny paws with sharp claws, and two furry ears which are very sensitive to sounds. It has a tiny body covered with soft fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
        "        \"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if model_type != \"roberta\":\n",
        "        for test_case in test_cases:\n",
        "            test_case[\"input_text\"] = test_case[\"input_text\"].lower()\n",
        "            test_case[\"true_text\"] = test_case[\"true_text\"].lower()\n",
        "\n",
        "elif language == \"fa\":\n",
        "    test_cases = [\n",
        "        # {\n",
        "        #     \"input_text\": \"\"\"\n",
        "        #\n",
        "        # \"\"\",\n",
        "        #     \"true_text\": \"\"\"\n",
        "        #\n",
        "        #  \"\"\"\n",
        "        # },\n",
        "        {\n",
        "            \"input_text\": \"پس از سال‌ها تلاش، رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"وقتی قیمت گوست قرمز یا صفید در کشورهای دیگر بیشتر شده است، ممکن است در جیران هم گرا شود.\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"در هفته گذشته قیمت تلا تغییر چندانی نداشت، و در همان محدوده 1850 دلاری کار خود را به پایان رساند. \",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"بر اساس مسوبه سران قوا، معاملات فردایی طلا همانند معاملات فردایی ارض، ممنوع و غیرقانونی شناخته شد و فعالان این بازار به جرم اخلال اقتصادی، تحت پیگرد قرار خواهند گرفت. در نتیجه تانک مرکزی در بازار فردایی مداخله نخواهد کرد\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        با نزدیک شدن قیمت دار غیر رسمی به سفف خود در روز قبل، تحلیلگران در بازار برای هفته بعد هشدار میدادند که باید احطیاط کرد و اقدامات امنیتی در بازار افزایش خواهد یافت.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        با تولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تأمین کندم جهان داشتند، بازار کندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان کندم بود، در همه جای جهان افزایش یافت.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        علت واقعی تعویق در مزاکرات وین چیست.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "else:\n",
        "    raise f\"{language} language not found.\"\n",
        "\n",
        "# ALPHA = 8 if language == \"en\" else 30\n",
        "ALPHA = 5 if language == \"en\" else 30\n",
        "MAX_EDIT_DISTANCE = 3 if language == \"en\" else 2\n",
        "TOP_K = 500 if language == \"en\" else 5000\n",
        "VERBOSE = True\n",
        "\n",
        "for test_case in test_cases:\n",
        "    test_case[\"input_text\"] = test_case[\"input_text\"].strip()\n",
        "    test_case[\"true_text\"] = test_case[\"true_text\"].strip()\n",
        "\n",
        "spell_corrector = SpellCorrector(ALPHA, MAX_EDIT_DISTANCE, VERBOSE, TOP_K)\n",
        "from spacy import displacy\n",
        "\n",
        "for idx in range(len(test_cases)):\n",
        "    test_case = test_cases[idx]\n",
        "\n",
        "    input_text = test_case[\"input_text\"]\n",
        "\n",
        "    output_text = spell_corrector(input_text)\n",
        "\n",
        "    print('Is output corrected: ', output_text == test_case[\"true_text\"])\n",
        "\n",
        "    print('Corrected text: ', output_text)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* \" * 50)\n",
        "    print(\" *\" * 50)\n",
        "    print(\"\\n\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-0Swt0WBUWG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "f\"الکل\" in vocab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1nMe4SeZSoqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BERT.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('data_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}