{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Oj8ZYJiEa3V",
        "outputId": "615cb2b1-b716-4be1-d161-a341b585ca5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting spacy-stanza\n",
            "  Downloading spacy_stanza-1.0.2-py3-none-any.whl (9.7 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 75.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from stanza) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 72.5 MB/s \n",
            "\u001b[?25hCollecting spacy\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 36.1 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 40.0 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 35.4 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 47.4 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 39.3 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.1.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 51.6 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 42.9 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.1.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 41.0 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 51.3 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 47.9 MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 59.1 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 32.0 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 35.8 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 55.7 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 67 kB/s \n",
            "\u001b[?25hCollecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting spacy\n",
            "  Downloading spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 15.3 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.4-cp37-cp37m-manylinux2014_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 40.8 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.3-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 14.0 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.2-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 29.2 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.1-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 30.8 MB/s \n",
            "\u001b[?25h  Downloading spacy-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 32.2 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of spacy-stanza to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting spacy-stanza\n",
            "  Downloading spacy_stanza-1.0.1-py3-none-any.whl (9.7 kB)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
            "\u001b[K     |████████████████████████████████| 432 kB 40.1 MB/s \n",
            "\u001b[?25hCollecting spacy-stanza\n",
            "  Downloading spacy_stanza-1.0.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.2.3-py3-none-any.whl (342 kB)\n",
            "\u001b[K     |████████████████████████████████| 342 kB 37.3 MB/s \n",
            "\u001b[?25hCollecting spacy-stanza\n",
            "  Downloading spacy_stanza-0.2.5-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 21.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 37.7 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 41.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394489 sha256=6eaafb5b07df2d7554c588a86b0c2ed16e77b94fd1c06ad69b50d928b3886bd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153379 sha256=24a56cce860e38766544eae6e6179a3e2300a19f7cffb5af2fd5a13e8bb55ec6\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: pyyaml, tokenizers, stanza, nltk, libwapiti, huggingface-hub, transformers, spacy-stanza, hazm\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 huggingface-hub-0.7.0 libwapiti-0.2.1 nltk-3.3 pyyaml-6.0 spacy-stanza-0.2.5 stanza-1.2.3 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy torch stanza spacy-stanza transformers nltk hazm black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a0UeQ0EdU1",
        "outputId": "997b84bc-d919-47f0-ba38-b8750fe11e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=866bf1f4450c4b7f844253b5dd40d33c5cc27f99e2736e68c558ce30aee5448c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jobtrx_7/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "X4u6yjSTBUVz",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6k69CHtBUV4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import torch\n",
        "\n",
        "import editdistance\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    BertTokenizer,\n",
        "    BertForMaskedLM,\n",
        "    AlbertTokenizer,\n",
        "    AlbertForMaskedLM,\n",
        "    RobertaTokenizer,\n",
        "    RobertaModel,\n",
        ")\n",
        "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
        "from spacy.tokens.token import Token\n",
        "from spacy.tokens.doc import Doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNA20jkGBUV6",
        "outputId": "af692c5e-c42d-4ced-d87b-514be5dc65b6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch Device: cpu\n",
            "fa bert Model Loaded ...\n"
          ]
        }
      ],
      "source": [
        "# torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch_device = \"cpu\"\n",
        "print(f\"Torch Device: {torch_device}\")\n",
        "\n",
        "# language = 'fa'\n",
        "# model_type = 'bert'\n",
        "\n",
        "language = \"en\"\n",
        "model_type = \"roberta\"\n",
        "\n",
        "if language == \"en\":\n",
        "    if model_type == \"bert\":\n",
        "        model_name = \"bert-large-uncased\"  # Bert large\n",
        "        # model_name = \"bert-base-uncased\" # Bert base\n",
        "    elif model_type == \"roberta\":\n",
        "        model_name = \"roberta-large\"  # Roberta\n",
        "    else:\n",
        "        raise f\"{model_type} model not found.\"\n",
        "\n",
        "elif language == \"fa\":\n",
        "    if model_type == \"bert\":\n",
        "        # model_name = \"HooshvareLab/bert-fa-base-uncased\"  # BERT V2\n",
        "        model_name = \"HooshvareLab/bert-fa-zwnj-base\"  # BERT V3\n",
        "    elif model_type == \"albert\":\n",
        "        model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\"  # Albert\n",
        "    else:\n",
        "        raise f\"{model_type} model not found.\"\n",
        "\n",
        "else:\n",
        "    raise f\"{language} language not found.\"\n",
        "\n",
        "if model_type == \"bert\":\n",
        "    MASK = \"[MASK]\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
        "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "elif model_type == \"albert\":\n",
        "    MASK = \"[MASK]\"\n",
        "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
        "    model = AlbertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
        "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "elif model_type == \"roberta\":\n",
        "    MASK = \"<mask>\"\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
        "    unmasker = pipeline(\"fill-mask\", model=\"roberta-large\", tokenizer=tokenizer)\n",
        "\n",
        "else:\n",
        "    print(f\"{model_type} not found.\")\n",
        "\n",
        "vocab: set = set(tokenizer.get_vocab().keys())\n",
        "\n",
        "if model_type == \"roberta\":\n",
        "    vocab = set(map(lambda s: s[1:], vocab))\n",
        "\n",
        "print(f\"len vocab: {len(vocab)}\")\n",
        "print(f\"{language} {model_type} Model Loaded ...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "svt6XdEUBUV-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Stanza\n",
        "\n",
        "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZJviZP6BUV-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import stanza\n",
        "import spacy\n",
        "import spacy_stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4658a1df5ec34c9e8455fdaf30e48551",
            "8fbffa83d9154316b0f4d6a693eb041c"
          ]
        },
        "id": "t9K0jKLgBUV-",
        "outputId": "8591b58b-8ea0-4919-8b02-196931829de3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-06 10:31:38 WARNING: Directory /home/ahur4/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4658a1df5ec34c9e8455fdaf30e48551",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-06 10:31:39 INFO: Downloading default packages for language: fa (Persian)...\n",
            "2022-06-06 10:31:40 INFO: File exists: /home/ahur4/stanza_resources/fa/default.zip\n",
            "2022-06-06 10:31:42 INFO: Finished downloading models and saved to /home/ahur4/stanza_resources.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fbffa83d9154316b0f4d6a693eb041c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-06 10:31:43 INFO: Loading these models for language: fa (Persian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | perdt   |\n",
            "| mwt       | perdt   |\n",
            "| pos       | perdt   |\n",
            "| lemma     | perdt   |\n",
            "| depparse  | perdt   |\n",
            "| ner       | arman   |\n",
            "=======================\n",
            "\n",
            "2022-06-06 10:31:43 INFO: Use device: cpu\n",
            "2022-06-06 10:31:43 INFO: Loading: tokenize\n",
            "2022-06-06 10:31:43 INFO: Loading: mwt\n",
            "2022-06-06 10:31:43 INFO: Loading: pos\n",
            "2022-06-06 10:31:43 INFO: Loading: lemma\n",
            "2022-06-06 10:31:43 INFO: Loading: depparse\n",
            "2022-06-06 10:31:44 INFO: Loading: ner\n",
            "2022-06-06 10:31:44 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "if language == \"fa\":\n",
        "    stanza.install_corenlp()\n",
        "    stanza.download(\"fa\")\n",
        "    nlp = spacy_stanza.load_pipeline(\"fa\")\n",
        "\n",
        "elif language == \"en\":\n",
        "    spacy.prefer_gpu()\n",
        "    nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"{language} not supported.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM_OSihaBUV_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def half_space_case(predicted: str, current: str):\n",
        "    wo_half_space_current = current.replace(\"‌\", \"\")\n",
        "    return wo_half_space_current == predicted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-OvcIbayBUWA",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Correct Lexico Typo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8sXXpJTBUWB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def lexico_typo_correction(\n",
        "    text,\n",
        "    alpha=10,\n",
        "    max_edit_distance=2,\n",
        "    top_k=10,\n",
        "    verbose=False,\n",
        "):\n",
        "    while True:\n",
        "        some_token_corrected = False\n",
        "        doc = nlp(text)\n",
        "        for index, current_token in enumerate(doc):\n",
        "            current_token: Token\n",
        "            start_char_index: int = current_token.idx\n",
        "            end_char_index = start_char_index + len(current_token)\n",
        "\n",
        "            if current_token.text not in vocab:\n",
        "                masked_text = (\n",
        "                    doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
        "                )\n",
        "\n",
        "                predicts = unmasker(masked_text, top_k=top_k)\n",
        "\n",
        "                # Select token from predicts\n",
        "                predicts = pd.DataFrame(predicts)\n",
        "\n",
        "                try:\n",
        "                    if current_token.text in string.punctuation:\n",
        "                        selected_predict = predicts[\"token_str\"].iloc[0]\n",
        "\n",
        "                    elif any(c.isdigit() for c in current_token.text):\n",
        "                        print(\"DIGIT\")\n",
        "                        selected_predict = current_token.text\n",
        "\n",
        "                    else:\n",
        "                        predicts.loc[:, \"token_str\"] = predicts[\"token_str\"].apply(\n",
        "                            lambda tk: tk.replace(\" \", \"\")\n",
        "                        )\n",
        "                        predicts.loc[:, \"edit_distance\"] = predicts[\"token_str\"].apply(\n",
        "                            lambda tk: editdistance.eval(current_token.text, tk)\n",
        "                        )\n",
        "\n",
        "                        # Filter tokens with at most 3 edit distance\n",
        "                        filtered_predicts = predicts.loc[\n",
        "                            predicts[\"edit_distance\"] <= max_edit_distance, :\n",
        "                        ].copy()\n",
        "\n",
        "                        # Apply total score function\n",
        "                        # e: edit distance + 1\n",
        "                        # l: token length\n",
        "                        filtered_predicts.loc[:, \"e_to_l\"] = (\n",
        "                            filtered_predicts.loc[:, \"edit_distance\"] + 1\n",
        "                        ) / len(current_token.text)\n",
        "\n",
        "                        filtered_predicts.loc[:, \"total_score\"] = (\n",
        "                            filtered_predicts.loc[:, \"score\"]\n",
        "                            / filtered_predicts.loc[:, \"e_to_l\"] ** alpha\n",
        "                        )\n",
        "\n",
        "                        filtered_predicts = filtered_predicts.sort_values(\n",
        "                            \"total_score\", ascending=False\n",
        "                        )\n",
        "                        selected_predict_row = filtered_predicts.iloc[0, :]\n",
        "\n",
        "                        selected_predict = selected_predict_row[\"token_str\"]\n",
        "\n",
        "                except Exception as e:\n",
        "\n",
        "                    print(\n",
        "                        f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(filtered_predicts)}\"\n",
        "                    )\n",
        "                    from spellchecker import SpellChecker\n",
        "\n",
        "                    spell = SpellChecker()\n",
        "                    selected_predict = spell.correction(current_token.text)\n",
        "\n",
        "                if selected_predict != current_token.text:\n",
        "                    if not half_space_case(selected_predict, current_token.text):\n",
        "                        some_token_corrected = True\n",
        "                        result_text = masked_text.replace(MASK, selected_predict, 1)\n",
        "                        text = result_text\n",
        "\n",
        "                    else:\n",
        "                        vocab.add(current_token.text)\n",
        "                        selected_predict = current_token.text\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"*\" * 50)\n",
        "                    print(f\"Token: {current_token.text}\")\n",
        "\n",
        "                    print(\"Filtered Predicts: \\n\")\n",
        "                    print(filtered_predicts[[\"token_str\", \"score\", \"total_score\"]])\n",
        "\n",
        "                    print(f\"{current_token.text} -> {selected_predict} : lexical\")\n",
        "\n",
        "                    if some_token_corrected:\n",
        "                        typo_correction_details = {\n",
        "                            \"raw\": current_token.text,\n",
        "                            \"corrected\": selected_predict,\n",
        "                            \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
        "                            \"type\": \"lexical\",\n",
        "                        }\n",
        "\n",
        "                        print(typo_correction_details)\n",
        "\n",
        "                if some_token_corrected:\n",
        "                    break\n",
        "\n",
        "        if not some_token_corrected:\n",
        "            break\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vbL5MvBrBUWC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Correct Contextual Typo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYyAc6HMBUWC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def contextual_typo_correction(\n",
        "    text,\n",
        "    alpha=10,\n",
        "    max_edit_distance=2,\n",
        "    top_k=10,\n",
        "    verbose=False,\n",
        "):\n",
        "    doc = nlp(text)\n",
        "    for index in range(len(doc)):\n",
        "\n",
        "        current_token: Token = doc[index]\n",
        "\n",
        "        print(\"*\" * 50)\n",
        "        print(f\"Token: {current_token.text}\")\n",
        "\n",
        "        start_char_index = current_token.idx\n",
        "        end_char_index = start_char_index + len(current_token)\n",
        "\n",
        "        masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
        "\n",
        "        predicts = unmasker(masked_text, top_k=top_k)\n",
        "        ### Select Token From Predicts\n",
        "        predicts = pd.DataFrame(predicts)\n",
        "\n",
        "        try:\n",
        "            if current_token.text in string.punctuation:\n",
        "                filtered_predicts = predicts.loc[\n",
        "                    predicts[\"token_str\"].apply(lambda tk: tk in string.punctuation), :\n",
        "                ].copy()\n",
        "                selected_predict = filtered_predicts[\"token_str\"].iloc[0]\n",
        "\n",
        "            elif any(c.isdigit() for c in current_token.text):\n",
        "                selected_predict = current_token.text\n",
        "\n",
        "            else:\n",
        "                predicts.loc[:, \"token_str\"] = predicts[\"token_str\"].apply(\n",
        "                    lambda tk: tk.replace(\" \", \"\")\n",
        "                )\n",
        "                predicts.loc[:, \"edit_distance\"] = predicts[\"token_str\"].apply(\n",
        "                    lambda tk: editdistance.eval(current_token.text, tk)\n",
        "                )\n",
        "\n",
        "                # Filter tokens with at most 3 edit distance\n",
        "                filtered_predicts = predicts.loc[\n",
        "                    predicts[\"edit_distance\"] <= max_edit_distance, :\n",
        "                ].copy()\n",
        "\n",
        "                # Apply total score function\n",
        "                # e: edit distance + 1\n",
        "                # l: token length\n",
        "                filtered_predicts.loc[:, \"e_to_l\"] = (\n",
        "                    filtered_predicts.loc[:, \"edit_distance\"] + 1\n",
        "                ) / len(current_token.text)\n",
        "\n",
        "                filtered_predicts.loc[:, \"total_score\"] = (\n",
        "                    filtered_predicts.loc[:, \"score\"]\n",
        "                    / filtered_predicts.loc[:, \"e_to_l\"] ** alpha\n",
        "                )\n",
        "\n",
        "                filtered_predicts = filtered_predicts.sort_values(\n",
        "                    \"total_score\", ascending=False\n",
        "                )\n",
        "                selected_predict_row = filtered_predicts.iloc[0, :]\n",
        "\n",
        "                selected_predict = selected_predict_row[\"token_str\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(filtered_predicts)}\"\n",
        "            )\n",
        "            selected_predict = current_token.text\n",
        "\n",
        "        if selected_predict != current_token.text:\n",
        "            if not half_space_case(selected_predict, current_token.text):\n",
        "                text = masked_text.replace(MASK, selected_predict, 1)\n",
        "                doc = nlp(text)\n",
        "\n",
        "            else:\n",
        "                vocab.add(current_token.text)\n",
        "                selected_predict = current_token.text\n",
        "\n",
        "        if verbose:\n",
        "            if current_token.text != selected_predict:\n",
        "                print(\"Filtered Predicts: \\n\")\n",
        "                print(filtered_predicts[[\"token_str\", \"score\", \"total_score\"]])\n",
        "\n",
        "                print(f\"{current_token.text} -> {selected_predict} : contextual\")\n",
        "\n",
        "                typo_correction_details = {\n",
        "                    \"raw\": current_token.text,\n",
        "                    \"corrected\": selected_predict,\n",
        "                    \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
        "                    \"around\": text[start_char_index - 10 : end_char_index + 10],\n",
        "                    \"type\": \"contextual\",\n",
        "                }\n",
        "\n",
        "                print(typo_correction_details)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eU-cld58BUWD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Correction Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC6m3_-NBUWD",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class SpellCorrector:\n",
        "    def __init__(self, alpha=5, max_edit_distance=2, verbose=False, top_k=50):\n",
        "        self.alpha = alpha\n",
        "        self.max_edit_distance = max_edit_distance\n",
        "        self.verbose = verbose\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def _lexico_typo_correction(self, text):\n",
        "        return lexico_typo_correction(\n",
        "            text,\n",
        "            self.alpha,\n",
        "            self.max_edit_distance,\n",
        "            self.top_k,\n",
        "            self.verbose,\n",
        "        )\n",
        "\n",
        "    def _contextual_typo_correction(self, text):\n",
        "        return contextual_typo_correction(\n",
        "            text,\n",
        "            self.alpha,\n",
        "            self.max_edit_distance,\n",
        "            self.top_k,\n",
        "            self.verbose,\n",
        "        )\n",
        "\n",
        "    def correction_pipeline(self, text):\n",
        "        # print(\"Lexico Correction ...\") if self.verbose else print()\n",
        "        corrected_text = self._lexico_typo_correction(text)\n",
        "\n",
        "        # print(\"Contextual Correction ...\") if self.verbose else print()\n",
        "        corrected_text = self._contextual_typo_correction(corrected_text)\n",
        "\n",
        "        return corrected_text\n",
        "\n",
        "    def __call__(self, text, *args, **kwargs):\n",
        "        return self.correction_pipeline(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cY2Jwvz4BUWE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Sample Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urftUEFFBUWE",
        "outputId": "b174a35e-5a3d-42c4-b8f3-9b2bc4855860",
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**************************************************\n",
            "Token: کسف\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str     score   total_score\n",
            "2          کشف  0.065814  12619.911998\n",
            "160        کسب  0.000415     79.552601\n",
            "1898       کسر  0.000008      1.567860\n",
            "3022       کسی  0.000004      0.753784\n",
            "3215        کف  0.000004      0.685134\n",
            "...        ...       ...           ...\n",
            "4920        دس  0.000002      0.000002\n",
            "4939        نف  0.000002      0.000002\n",
            "4940       کلک  0.000002      0.000002\n",
            "4952        سگ  0.000002      0.000002\n",
            "4992        جس  0.000002      0.000002\n",
            "\n",
            "[120 rows x 3 columns]\n",
            "کسف -> کشف : lexical\n",
            "{'raw': 'کسف', 'corrected': 'کشف', 'span': '[32, 35]', 'type': 'lexical'}\n",
            "**************************************************\n",
            "Token: تیرانی\n",
            "Filtered Predicts: \n",
            "\n",
            "     token_str     score   total_score\n",
            "24      ایرانی  0.006808  1.401631e+12\n",
            "1216    تهرانی  0.000036  7.443975e+09\n",
            "69     تایوانی  0.001803  1.936309e+06\n",
            "344     نورانی  0.000211  2.263658e+05\n",
            "406     شیرازی  0.000172  1.843384e+05\n",
            "633     کیهانی  0.000091  9.744183e+04\n",
            "665      ایران  0.000084  9.063041e+04\n",
            "723     شیطانی  0.000076  8.150461e+04\n",
            "991     حیوانی  0.000048  5.149391e+04\n",
            "1084    بیرونی  0.000042  4.524209e+04\n",
            "1348    میدانی  0.000031  3.366329e+04\n",
            "1506     میانی  0.000027  2.860571e+04\n",
            "2437    میراثی  0.000013  1.403591e+04\n",
            "2831     کیانی  0.000010  1.101377e+04\n",
            "2985    ایمانی  0.000009  1.020002e+04\n",
            "3084     قرانی  0.000009  9.704528e+03\n",
            "3803    تمرینی  0.000007  6.990530e+03\n",
            "3816    هیجانی  0.000006  6.956379e+03\n",
            "3985     تراپی  0.000006  6.501799e+03\n",
            "4087    گیلانی  0.000006  6.240923e+03\n",
            "4156     ترانه  0.000006  6.098743e+03\n",
            "4919    شیرینی  0.000004  4.668734e+03\n",
            "4942    شیبانی  0.000004  4.636539e+03\n",
            "4985     بیانی  0.000004  4.563315e+03\n",
            "تیرانی -> ایرانی : lexical\n",
            "{'raw': 'تیرانی', 'corrected': 'ایرانی', 'span': '[57, 63]', 'type': 'lexical'}\n",
            "**************************************************\n",
            "Token: پس\n",
            "**************************************************\n",
            "Token: از\n",
            "**************************************************\n",
            "Token: سال‌ها\n",
            "**************************************************\n",
            "Token: تلاش\n",
            "**************************************************\n",
            "Token: ،\n"
          ]
        }
      ],
      "source": [
        "if language == \"en\":\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The quantity thoery of money also assume that the quantity of money in an economy has a large influense on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the sopply of gods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The quantity theory of money also assumes that the quantity of money in an economy has a large influence on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the supply of goods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Does it privent Iran from getting nuclear weapens. Many exports say that if all parties adhered to their pledges, the deal almost certainly could have achieved that goal for longer than a dekade!\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Does it prevent Iran from getting nuclear weapons? Many experts say that if all parties adhere to their pledges, the deal almost certainly could have achieved that goal for longer than a decade.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The Federal Reserve monitor risks to the financal system and works to help insure the system supports a haelthy economy for US households, communities, and busineses.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The Federal Reserve monitors risks to the financial system and works to help ensure the system supports a healthy economy for US households, communities, and businesses.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Bitcoin is a decentrallized digital curency that can be transfered on the peer-to-peer bitcoin network. Bitcoin transactions are veryfied by network nodes throgh cryptography and recorded in a public distributed ledger called a blockchain. The criptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The curency began use in 2009 when its implemntation was released as open-source software.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Bitcoin is a decentralized digital currency that can be transferred on the peer-to-peer bitcoin network. Bitcoin transactions are verified by network nodes through cryptography and recorded in a public distributed ledger called a blockchain. The cryptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            The 2022 FILA World Cup is scheduled to be the 22nd running of the FILA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 Novamber to 18 Decamber 2022.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            The 2022 FIFA World Cup is scheduled to be the 22nd running of the FIFA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 November to 18 December 2022.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            President Daneld Trump annonced on Tuesday he well withdraw the United States from the Iran nuclear deal and restore far-reaching sanktions aimed at withdrawal Iran from the global finansial system.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            President Donald Trump announced on Tuesday he will withdraw the United States from the Iran nuclear deal and restore far-reaching sanctions aimed at withdrawal Iran from the global financial system.\n",
        "        \"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "            Cars has very sweet features. It has two beautifull eye, adorable tiny paws, sharp claws, and two fury ear which are very sensitive to sounds. It has a tiny body covered with sot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\n",
        "            Cat has very sweet features. It has two beautiful eyes, adorable tiny paws with sharp claws, and two furry ears which are very sensitive to sounds. It has a tiny body covered with soft fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
        "        \"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if model_type != \"roberta\":\n",
        "        for test_case in test_cases:\n",
        "            test_case[\"input_text\"] = test_case[\"input_text\"].lower()\n",
        "            test_case[\"true_text\"] = test_case[\"true_text\"].lower()\n",
        "\n",
        "elif language == \"fa\":\n",
        "    test_cases = [\n",
        "        # {\n",
        "        #     \"input_text\": \"\"\"\n",
        "        #\n",
        "        # \"\"\",\n",
        "        #     \"true_text\": \"\"\"\n",
        "        #\n",
        "        #  \"\"\"\n",
        "        # },\n",
        "        {\n",
        "            \"input_text\": \"پس از سال‌ها تلاش، رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"وقتی قیمت گوست قرمز یا صفید در کشورهای دیگر بیشتر شده است، ممکن است در جیران هم گرا شود.\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"در هفته گذشته قیمت تلا تغییر چندانی نداشت، و در همان محدوده 1850 دلاری کار خود را به پایان رساند. \",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"بر اساس مسوبه سران قوا، معاملات فردایی طلا همانند معاملات فردایی ارض، ممنوع و غیرقانونی شناخته شد و فعالان این بازار به جرم اخلال اقتصادی، تحت پیگرد قرار خواهند گرفت. در نتیجه تانک مرکزی در بازار فردایی مداخله نخواهد کرد\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        با نزدیک شدن قیمت دار غیر رسمی به سفف خود در روز قبل، تحلیلگران در بازار برای هفته بعد هشدار میدادند که باید احطیاط کرد و اقدامات امنیتی در بازار افزایش خواهد یافت.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        با تولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تأمین کندم جهان داشتند، بازار کندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان کندم بود، در همه جای جهان افزایش یافت.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "        {\n",
        "            \"input_text\": \"\"\"\n",
        "        علت واقعی تعویق در مزاکرات وین چیست.\n",
        "        \"\"\",\n",
        "            \"true_text\": \"\"\"\"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "else:\n",
        "    raise f\"{language} language not found.\"\n",
        "\n",
        "ALPHA = 8 if language == \"en\" else 30\n",
        "MAX_EDIT_DISTANCE = 2 if language == \"en\" else 2\n",
        "TOP_K = 250 if language == \"en\" else 5000\n",
        "VERBOSE = True\n",
        "\n",
        "for test_case in test_cases:\n",
        "    test_case[\"input_text\"] = test_case[\"input_text\"].strip()\n",
        "    test_case[\"true_text\"] = test_case[\"true_text\"].strip()\n",
        "\n",
        "spell_corrector = SpellCorrector(ALPHA, MAX_EDIT_DISTANCE, VERBOSE, TOP_K)\n",
        "from spacy import displacy\n",
        "\n",
        "for idx in range(len(test_cases)):\n",
        "    test_case = test_cases[idx]\n",
        "\n",
        "    input_text = test_case[\"input_text\"]\n",
        "\n",
        "    output_text = spell_corrector(input_text)\n",
        "\n",
        "    print(output_text == test_case[\"true_text\"])\n",
        "\n",
        "    print(output_text)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"* \" * 50)\n",
        "    print(\" *\" * 50)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-0Swt0WBUWG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "\"الکل\" in vocab"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sam_copy.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('data_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
