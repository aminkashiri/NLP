{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM\n",
    "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en Model Loaded ...\n",
      "Torch Device: cpu\n",
      "en Model Loaded ...\n"
     ]
    }
   ],
   "source": [
    "# torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_device = 'cpu'\n",
    "\n",
    "print(f\"Torch Device: {torch_device}\")\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "# EN\n",
    "model_name = \"bert-large-uncased\"\n",
    "# model_name = \"bert-base-uncased\"\n",
    "\n",
    "# FA\n",
    "## Lite BERT\n",
    "# model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\"\n",
    "\n",
    "## BERT \n",
    "# model_name = \"HooshvareLab/bert-fa-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if language == 'en':\n",
    "    model = BertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "else:\n",
    "    model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "print(f\"{language} Model Loaded ...\")\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "30522"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "30522"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[[{'score': 0.5726480484008789,\n   'token': 4092,\n   'token_str': 's p e a k i n g',\n   'sequence': '[CLS] i am speaking english very [MASK]. [SEP]'},\n  {'score': 0.30466437339782715,\n   'token': 4083,\n   'token_str': 'l e a r n i n g',\n   'sequence': '[CLS] i am learning english very [MASK]. [SEP]'},\n  {'score': 0.018297268077731133,\n   'token': 2635,\n   'token_str': 't a k i n g',\n   'sequence': '[CLS] i am taking english very [MASK]. [SEP]'},\n  {'score': 0.017190691083669662,\n   'token': 5702,\n   'token_str': 's t u d y i n g',\n   'sequence': '[CLS] i am studying english very [MASK]. [SEP]'},\n  {'score': 0.011624318547546864,\n   'token': 2478,\n   'token_str': 'u s i n g',\n   'sequence': '[CLS] i am using english very [MASK]. [SEP]'}],\n [{'score': 0.8824039697647095,\n   'token': 2092,\n   'token_str': 'w e l l',\n   'sequence': '[CLS] i am [MASK] english very well. [SEP]'},\n  {'score': 0.023844681680202484,\n   'token': 2411,\n   'token_str': 'o f t e n',\n   'sequence': '[CLS] i am [MASK] english very often. [SEP]'},\n  {'score': 0.020263176411390305,\n   'token': 2172,\n   'token_str': 'm u c h',\n   'sequence': '[CLS] i am [MASK] english very much. [SEP]'},\n  {'score': 0.00927507970482111,\n   'token': 2855,\n   'token_str': 'q u i c k l y',\n   'sequence': '[CLS] i am [MASK] english very quickly. [SEP]'},\n  {'score': 0.008535384200513363,\n   'token': 2210,\n   'token_str': 'l i t t l e',\n   'sequence': '[CLS] i am [MASK] english very little. [SEP]'}]]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "[[{'score': 0.5726480484008789,\n   'token': 4092,\n   'token_str': 's p e a k i n g',\n   'sequence': '[CLS] i am speaking english very [MASK]. [SEP]'},\n  {'score': 0.30466437339782715,\n   'token': 4083,\n   'token_str': 'l e a r n i n g',\n   'sequence': '[CLS] i am learning english very [MASK]. [SEP]'},\n  {'score': 0.018297268077731133,\n   'token': 2635,\n   'token_str': 't a k i n g',\n   'sequence': '[CLS] i am taking english very [MASK]. [SEP]'},\n  {'score': 0.017190691083669662,\n   'token': 5702,\n   'token_str': 's t u d y i n g',\n   'sequence': '[CLS] i am studying english very [MASK]. [SEP]'},\n  {'score': 0.011624318547546864,\n   'token': 2478,\n   'token_str': 'u s i n g',\n   'sequence': '[CLS] i am using english very [MASK]. [SEP]'}],\n [{'score': 0.8824039697647095,\n   'token': 2092,\n   'token_str': 'w e l l',\n   'sequence': '[CLS] i am [MASK] english very well. [SEP]'},\n  {'score': 0.023844681680202484,\n   'token': 2411,\n   'token_str': 'o f t e n',\n   'sequence': '[CLS] i am [MASK] english very often. [SEP]'},\n  {'score': 0.020263176411390305,\n   'token': 2172,\n   'token_str': 'm u c h',\n   'sequence': '[CLS] i am [MASK] english very much. [SEP]'},\n  {'score': 0.00927507970482111,\n   'token': 2855,\n   'token_str': 'q u i c k l y',\n   'sequence': '[CLS] i am [MASK] english very quickly. [SEP]'},\n  {'score': 0.008535384200513363,\n   'token': 2210,\n   'token_str': 'l i t t l e',\n   'sequence': '[CLS] i am [MASK] english very little. [SEP]'}]]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The capital of Iran is [MASK].\"\n",
    "text = \"Yesterday, I played [MASK] in the [MASK].\"\n",
    "text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "text = \"I'm a very [MASK] player in football.\"\n",
    "text = \"He drived a [MASK].\"\n",
    "text = \"I love playing [MASK].\"\n",
    "text = \"I am [MASK] english very [MASK]. \"\n",
    "\n",
    "# text = \"امروز در استادیوم آزادی، تیم ملی [MASK] ایران و سوریه مسابقه می‌دهند.\"\n",
    "# text = \"امروز در استادیوم آزادی [ماسک] ملی ایران و روسیه مسایقه می‌دهند.\"\n",
    "# text = \"پس از سال‌ها تلاش رازی موفق به [ماسک] الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ [ماسک] است.\"\n",
    "# text = \"اهل کدام کشور هستی[ماسک]\"\n",
    "# text = text.replace(\"[ماسک]\", \"[MASK]\")\n",
    "\n",
    "unmasker(text, )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stanza\n",
    "\n",
    "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
    "\n",
    "## Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import spacy_stanza"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "if language == 'fa':\n",
    "    stanza.install_corenlp()\n",
    "    stanza.download('en')\n",
    "    nlp = spacy_stanza.load_pipeline(\"en\")\n",
    "\n",
    "elif language == 'en':\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Correct Lexico Typo:\n",
      "\n",
      "\n",
      "Text: i am speeking english very god.\n",
      "\n",
      "\n",
      "[speeking] is not in vocab\n",
      "Tokens: [i, am, speeking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "      score  token token_str                         sequence  \\\n",
      "0  0.335115   2019        an        i am an english very god.   \n",
      "1  0.158579   2025       not       i am not english very god.   \n",
      "2  0.138133   1996       the       i am the english very god.   \n",
      "3  0.133903   4092  speaking  i am speaking english very god.   \n",
      "4  0.053356   1999        in        i am in english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                       0.875  \n",
      "1                       1.000  \n",
      "2                       0.875  \n",
      "3                       0.125  \n",
      "4                       0.750  \n",
      "Filtered Predicts: \n",
      "\n",
      "      score  token token_str                         sequence  \\\n",
      "3  0.133903   4092  speaking  i am speaking english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "3                       0.125  \n",
      "speeking -> speaking\n",
      "Text: i am speaking english very god.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
    "# tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "# tokenizer.get_vocab()\n",
    "\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "\n",
    "text = \"The capitan of Iran is tehran.\"\n",
    "text = \"i am speeking english very wall.\"\n",
    "# text = \"Yesterday, I played [MASK] in the [MASK].\"\n",
    "# text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "# text = \"I'm a very [MASK] player in football.\"\n",
    "# text = \"He drove a cat.\"\n",
    "# text = \"do you want to watch tv.\"\n",
    "# text = \"I love playing [MASK].\"\n",
    "\n",
    "text = text.lower()\n",
    "vocab: dict = tokenizer.get_vocab()\n",
    "MASK = \"[MASK]\"\n",
    "\n",
    "MAX_EDIT_DISTANCE_TO_LEN_RATIO = 0.5\n",
    "MIN_SCORE = 0.01\n",
    "\n",
    "### Correct Lexico Typo\n",
    "print(\"\\n\\nCorrect Lexico Typo:\\n\\n\")\n",
    "print(f\"Text: {text}\\n\\n\")\n",
    "while True:\n",
    "    some_token_corrected = False\n",
    "    doc = nlp(text)\n",
    "    for index in range(len(doc)):\n",
    "        current_token: Token = doc[index]\n",
    "        start_char_index = current_token.idx\n",
    "        end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "        if current_token.text not in vocab:\n",
    "            print(f\"[{current_token.text}] is not in vocab\")\n",
    "\n",
    "            tokens = list(doc)\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "\n",
    "            masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "            predicts = unmasker(masked_text)\n",
    "\n",
    "            ### Select Token From Predicts\n",
    "            predicts = pd.DataFrame(predicts)\n",
    "\n",
    "\n",
    "            predicts['token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "            predicts['edit_distance_to_len_ratio'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk) / len(current_token.text))\n",
    "\n",
    "            print(\"Predicts: \\n\")\n",
    "            print(predicts)\n",
    "\n",
    "            predicts = predicts[predicts['edit_distance_to_len_ratio'] <= MAX_EDIT_DISTANCE_TO_LEN_RATIO]\n",
    "\n",
    "            print(\"Filtered Predicts: \\n\")\n",
    "            print(predicts)\n",
    "\n",
    "            selected_predict = predicts['token_str'].iloc[0]\n",
    "            ###\n",
    "\n",
    "            result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "\n",
    "            print(f\"{current_token.text} -> {selected_predict}\")\n",
    "            print(f\"Text: {result_text}\\n\\n\")\n",
    "\n",
    "            text = result_text\n",
    "            some_token_corrected = True\n",
    "            break\n",
    "\n",
    "    if not some_token_corrected:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Correct Contextual Typo:\n",
      "\n",
      "\n",
      "Text: i am speaking english very god.\n",
      "\n",
      "\n",
      "Tokens: [i, am, speaking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "          score  token token_str                           sequence  \\\n",
      "0  9.999995e-01   1045         i    i am speaking english very god.   \n",
      "1  1.979739e-07   1998       and  and am speaking english very god.   \n",
      "2  5.421951e-08   1000         \"    \" am speaking english very god.   \n",
      "3  5.051135e-08   2021       but  but am speaking english very god.   \n",
      "4  2.728065e-08   2026        my   my am speaking english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                         0.0  \n",
      "1                         3.0  \n",
      "2                         1.0  \n",
      "3                         3.0  \n",
      "4                         2.0  \n",
      "Filtered Predicts: \n",
      "\n",
      "   score  token token_str                         sequence  \\\n",
      "0    1.0   1045         i  i am speaking english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                         0.0  \n",
      "Tokens: [i, am, speaking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "      score  token token_str                              sequence  \\\n",
      "0  0.515845   2001       was      i was speaking english very god.   \n",
      "1  0.254389   2572        am       i am speaking english very god.   \n",
      "2  0.088084   2318   started  i started speaking english very god.   \n",
      "3  0.023678   2211     began    i began speaking english very god.   \n",
      "4  0.023339   2707     start    i start speaking english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                         1.0  \n",
      "1                         0.0  \n",
      "2                         3.0  \n",
      "3                         2.0  \n",
      "4                         2.0  \n",
      "Filtered Predicts: \n",
      "\n",
      "      score  token token_str                         sequence  \\\n",
      "1  0.254389   2572        am  i am speaking english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "1                         0.0  \n",
      "Tokens: [i, am, speaking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "      score  token token_str                         sequence  \\\n",
      "0  0.335115   2019        an        i am an english very god.   \n",
      "1  0.158579   2025       not       i am not english very god.   \n",
      "2  0.138133   1996       the       i am the english very god.   \n",
      "3  0.133903   4092  speaking  i am speaking english very god.   \n",
      "4  0.053356   1999        in        i am in english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                       0.750  \n",
      "1                       1.000  \n",
      "2                       0.875  \n",
      "3                       0.000  \n",
      "4                       0.750  \n",
      "Filtered Predicts: \n",
      "\n",
      "      score  token token_str                         sequence  \\\n",
      "3  0.133903   4092  speaking  i am speaking english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "3                         0.0  \n",
      "Tokens: [i, am, speaking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "      score  token token_str                      sequence  \\\n",
      "0  0.974783   1996       the   i am speaking the very god.   \n",
      "1  0.008398   2023      this  i am speaking this very god.   \n",
      "2  0.002698   1997        of    i am speaking of very god.   \n",
      "3  0.002194   2007      with  i am speaking with very god.   \n",
      "4  0.002032   2008      that  i am speaking that very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                    1.000000  \n",
      "1                    0.714286  \n",
      "2                    1.000000  \n",
      "3                    0.714286  \n",
      "4                    1.000000  \n",
      "Filtered Predicts: \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [score, token, token_str, sequence, edit_distance_to_len_ratio]\n",
      "Index: []\n",
      "Tokens: [i, am, speaking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "      score  token token_str                          sequence  \\\n",
      "0  0.646184   1010         ,       i am speaking english, god.   \n",
      "1  0.092546   4067     thank  i am speaking english thank god.   \n",
      "2  0.069617   2133       ...     i am speaking english... god.   \n",
      "3  0.043801   1012         .       i am speaking english. god.   \n",
      "4  0.028404   1011         -      i am speaking english - god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                        1.00  \n",
      "1                        1.25  \n",
      "2                        1.00  \n",
      "3                        1.00  \n",
      "4                        1.00  \n",
      "Filtered Predicts: \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [score, token, token_str, sequence, edit_distance_to_len_ratio]\n",
      "Index: []\n",
      "Tokens: [i, am, speaking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "      score  token token_str                             sequence  \\\n",
      "0  0.971681   2092      well     i am speaking english very well.   \n",
      "1  0.003122   2210    little   i am speaking english very little.   \n",
      "2  0.002586   2855   quickly  i am speaking english very quickly.   \n",
      "3  0.002316   9996    poorly   i am speaking english very poorly.   \n",
      "4  0.002086   3435      fast     i am speaking english very fast.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                    1.333333  \n",
      "1                    2.000000  \n",
      "2                    2.333333  \n",
      "3                    1.666667  \n",
      "4                    1.333333  \n",
      "Filtered Predicts: \n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [score, token, token_str, sequence, edit_distance_to_len_ratio]\n",
      "Index: []\n",
      "Tokens: [i, am, speaking, english, very, god, .]\n",
      "Predicts: \n",
      "\n",
      "      score  token token_str                           sequence  \\\n",
      "0  0.713696   1012         .    i am speaking english very god.   \n",
      "1  0.274192    999         !    i am speaking english very god!   \n",
      "2  0.006241   1025         ;   i am speaking english very god ;   \n",
      "3  0.005289   1029         ?    i am speaking english very god?   \n",
      "4  0.000248   2133       ...  i am speaking english very god...   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                         0.0  \n",
      "1                         1.0  \n",
      "2                         1.0  \n",
      "3                         1.0  \n",
      "4                         2.0  \n",
      "Filtered Predicts: \n",
      "\n",
      "      score  token token_str                         sequence  \\\n",
      "0  0.713696   1012         .  i am speaking english very god.   \n",
      "\n",
      "   edit_distance_to_len_ratio  \n",
      "0                         0.0  \n"
     ]
    }
   ],
   "source": [
    "### Correct Contextual Typo\n",
    "print(\"\\n\\nCorrect Contextual Typo:\\n\\n\")\n",
    "print(f\"Text: {text}\\n\\n\")\n",
    "while True:\n",
    "    some_token_corrected = False\n",
    "    doc = nlp(text)\n",
    "    for index in range(len(doc)):\n",
    "        current_token: Token = doc[index]\n",
    "        start_char_index = current_token.idx\n",
    "        end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "        tokens = list(doc)\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "\n",
    "        masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "        predicts = unmasker(masked_text)\n",
    "\n",
    "        ### Select Token From Predicts\n",
    "        predicts = pd.DataFrame(predicts)\n",
    "\n",
    "        predicts['token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "        predicts['edit_distance_to_len_ratio'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk) / len(current_token.text))\n",
    "\n",
    "        print(\"Predicts: \\n\")\n",
    "        print(predicts)\n",
    "\n",
    "        predicts = predicts[(predicts['edit_distance_to_len_ratio'] <= MAX_EDIT_DISTANCE_TO_LEN_RATIO) & (predicts['score'] >= MIN_SCORE)]\n",
    "\n",
    "        print(\"Filtered Predicts: \\n\")\n",
    "        print(predicts)\n",
    "\n",
    "        # selected_predict = predicts.loc[0, 'token_str']\n",
    "        ###\n",
    "        #\n",
    "        # result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "        #\n",
    "        # print(f\"{current_token.text} -> {selected_predict}\")\n",
    "        # print(f\"Text: {result_text}\\n\\n\")\n",
    "        #\n",
    "        # text = result_text\n",
    "        # some_token_corrected = True\n",
    "        # break\n",
    "\n",
    "    break\n",
    "    if not some_token_corrected:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# # tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
    "# # tokens\n",
    "# # tokenizer.convert_tokens_to_string(tokens)\n",
    "#\n",
    "# from spacy.tokens.token import Token\n",
    "# from spacy.tokens.doc import Doc\n",
    "#\n",
    "# text = \"پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\"\n",
    "# doc = stanza_nlp(text)\n",
    "#\n",
    "#\n",
    "# # for index in range(len(doc)):\n",
    "# #     token_str: Token = doc[index]\n",
    "# #     # print(f\"{index}: {token_str.text}\")\n",
    "#\n",
    "# #     print(doc)\n",
    "# #     break\n",
    "#\n",
    "#\n",
    "# len(stanza_nlp.vocab.strings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# text = \"The capital of Iran is [MASK].\"\n",
    "# text = \"Yesterday, I played [MASK] in the [MASK].\"\n",
    "# text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "# text = \"I'm a very [MASK] player in football.\"\n",
    "# text = \"He drived a [MASK].\"\n",
    "# text = \"I love playing [MASK].\"\n",
    "# text = \"I am [MASK] english very [MASK]. \"\n",
    "#\n",
    "# # text = \"امروز در استادیوم آزادی، تیم ملی [MASK] ایران و سوریه مسابقه می‌دهند.\"\n",
    "# # text = \"امروز در استادیوم آزادی [ماسک] ملی ایران و روسیه مسایقه می‌دهند.\"\n",
    "# # text = \"پس از سال‌ها تلاش رازی موفق به [ماسک] الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ [ماسک] است.\"\n",
    "# # text = \"اهل کدام کشور هستی[ماسک]\"\n",
    "# # text = text.replace(\"[ماسک]\", \"[MASK]\")\n",
    "#\n",
    "# unmasker(text, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
    "# tokens\n",
    "# tokenizer.convert_tokens_to_string(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ContextualSpellCheck Lib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import contextualSpellCheck\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "contextualSpellCheck.add_to_pipe(nlp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "i am speaking this very wall.\n",
      "True\n",
      "i am speaking this very wall.\n"
     ]
    }
   ],
   "source": [
    "text = 'i am speaking english very wall.'\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc._.performed_spellCheck) #Should be True\n",
    "print(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}