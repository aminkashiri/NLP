{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Required Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahur4/anaconda3/envs/data_env/lib/python3.10/site-packages/torch/cuda/__init__.py:82: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM, AlbertTokenizer, AlbertForMaskedLM, RobertaTokenizer, RobertaModel\n",
    "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cpu\n",
      "en roberta Model Loaded ...\n"
     ]
    }
   ],
   "source": [
    "# torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_device = 'cpu'\n",
    "print(f\"Torch Device: {torch_device}\")\n",
    "\n",
    "language = 'en'\n",
    "model_type = 'roberta'\n",
    "\n",
    "# EN\n",
    "\n",
    "if model_type == 'bert':\n",
    "    model_name = \"bert-large-uncased\"  # Bert large\n",
    "    # model_name = \"bert-base-uncased\" # Bert base\n",
    "\n",
    "if model_type == 'roberta':\n",
    "    model_name = \"roberta-large\"  # Roberta\n",
    "\n",
    "# FA\n",
    "# model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\" # Albert\n",
    "# model_name = \"HooshvareLab/bert-fa-base-uncased\" # BERT V2\n",
    "# model_name = \"HooshvareLab/bert-fa-zwnj-base\" # BERT V3\n",
    "\n",
    "if model_type == 'bert':\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForMaskedLM.from_pretrained(model_name).to(\n",
    "        torch_device) if language == 'en' else BertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
    "    MASK = \"[MASK]\"\n",
    "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "elif model_type == 'albert':\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    model = AlbertForMaskedLM.from_pretrained(model_name).to(\n",
    "        torch_device) if language == 'en' else AlbertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
    "    MASK = \"[MASK]\"\n",
    "    unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "elif model_type == 'roberta':\n",
    "    MASK = \"<mask>\"\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    unmasker = pipeline('fill-mask', model='roberta-large')\n",
    "\n",
    "else:\n",
    "    print(f\"{model_type} not found.\")\n",
    "\n",
    "vocab: set = set(tokenizer.get_vocab().keys())\n",
    "\n",
    "if model_type == 'roberta':\n",
    "    vocab = set(map(lambda s: s[1:], vocab))\n",
    "\n",
    "print(f\"{language} {model_type} Model Loaded ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "39620"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stanza\n",
    "\n",
    "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import spacy_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if language == 'fa':\n",
    "    stanza.install_corenlp()\n",
    "    stanza.download('fa')\n",
    "    nlp = spacy_stanza.load_pipeline(\"fa\")\n",
    "\n",
    "elif language == 'en':\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"{language} not supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "alpha = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correct Lexico Typo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def lexico_typo_correction(\n",
    "        text,\n",
    "        max_edit_distance_to_length_ratio=0.45,\n",
    "        max_edit_distance=2,\n",
    "        min_score=1e-7,\n",
    "        top_k=10,\n",
    "        verbose=False,\n",
    "):\n",
    "    while True:\n",
    "        some_token_corrected = False\n",
    "        doc = nlp(text)\n",
    "        for index, current_token in enumerate(doc):\n",
    "            current_token: Token\n",
    "            start_char_index: int = current_token.idx\n",
    "            end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "            if current_token.text not in vocab:\n",
    "                masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "                predicts = unmasker(masked_text, top_k=top_k)\n",
    "\n",
    "                # Select token from predicts\n",
    "                predicts = pd.DataFrame(predicts)\n",
    "\n",
    "                predicts.loc[:, 'token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "                predicts.loc[:, 'edit_distance'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk))\n",
    "\n",
    "                # Filter tokens with at most 3 edit distance\n",
    "                filtered_predicts = predicts.loc[predicts['edit_distance'] <= 3, :].copy()\n",
    "\n",
    "                # Apply total score function\n",
    "                # e: edit distance + 1\n",
    "                # l: token length\n",
    "                filtered_predicts.loc[:, 'e_to_l'] = (filtered_predicts.loc[:, 'edit_distance'] + 1) / len(current_token.text)\n",
    "\n",
    "                filtered_predicts.loc[:, 'total_score'] = filtered_predicts.loc[:, 'score'] / filtered_predicts.loc[:, 'e_to_l'] * alpha\n",
    "\n",
    "                filtered_predicts = filtered_predicts.sort_values('total_score', ascending=False)\n",
    "\n",
    "                try:\n",
    "                    selected_predict_row = filtered_predicts.iloc[0, :]\n",
    "                    selected_predict = selected_predict_row['token_str']\n",
    "                except:\n",
    "                    print(f\"\\n ** filtered tokens size is 0. ** \\n\")\n",
    "\n",
    "                    from spellchecker import SpellChecker\n",
    "                    spell = SpellChecker()\n",
    "                    selected_predict = spell.correction(current_token.text)\n",
    "\n",
    "                if selected_predict != current_token.text:\n",
    "                    some_token_corrected = True\n",
    "                    result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "                    text = result_text\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"*\" * 50)\n",
    "                    print(f\"Token: {current_token.text}\")\n",
    "\n",
    "                    print(\"Filtered Predicts: \\n\")\n",
    "                    print(filtered_predicts[['token_str', 'score', 'total_score']])\n",
    "\n",
    "                    print(f\"{current_token.text} -> {selected_predict} : lexical\")\n",
    "\n",
    "\n",
    "                    typo_correction_details = {\n",
    "                        \"raw\": current_token.text,\n",
    "                        \"corrected\": selected_predict,\n",
    "                        \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
    "                        \"type\": \"lexical\"\n",
    "                    }\n",
    "\n",
    "                    # print(typo_correction_details)\n",
    "\n",
    "                if some_token_corrected:\n",
    "                    break\n",
    "\n",
    "        if not some_token_corrected:\n",
    "            break\n",
    "\n",
    "    return text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correct Contextual Typo\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def contextual_typo_correction(\n",
    "        text,\n",
    "        max_edit_distance_to_length_ratio=0.45,\n",
    "        max_edit_distance=2,\n",
    "        min_score=1e-7,\n",
    "        top_k=10,\n",
    "        verbose=False,\n",
    "):\n",
    "    while True:\n",
    "        some_token_corrected = False\n",
    "        doc = nlp(text)\n",
    "        for index in range(len(doc)):\n",
    "            current_token: Token = doc[index]\n",
    "\n",
    "            start_char_index = current_token.idx\n",
    "            end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "            masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "            predicts = unmasker(masked_text, top_k=top_k)\n",
    "\n",
    "            try:\n",
    "                if current_token.text in string.punctuation:\n",
    "                    selected_predict = predicts['token_str'].iloc[0]\n",
    "\n",
    "                elif current_token.text.isdigit():\n",
    "                    selected_predict = current_token.text\n",
    "\n",
    "                else:\n",
    "                    ### Select Token From Predicts\n",
    "                    predicts = pd.DataFrame(predicts)\n",
    "\n",
    "                    predicts.loc[:, 'token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "                    predicts.loc[:, 'edit_distance'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk))\n",
    "\n",
    "                    # Filter tokens with at most 3 edit distance\n",
    "                    filtered_predicts = predicts.loc[predicts['edit_distance'] <= 3, :].copy()\n",
    "\n",
    "                    # Apply total score function\n",
    "                    # e: edit distance + 1\n",
    "                    # l: token length\n",
    "                    filtered_predicts.loc[:, 'e_to_l'] = (filtered_predicts.loc[:, 'edit_distance'] + 1) / len(current_token.text)\n",
    "\n",
    "                    filtered_predicts.loc[:, 'total_score'] = filtered_predicts.loc[:, 'score'] / filtered_predicts.loc[:, 'e_to_l'] * alpha\n",
    "\n",
    "                    filtered_predicts = filtered_predicts.sort_values('total_score', ascending=False)\n",
    "                    selected_predict_row = filtered_predicts.iloc[0, :]\n",
    "\n",
    "                    selected_predict = selected_predict_row['token_str']\n",
    "\n",
    "                    current_token_text_tot_score = filtered_predicts.loc[filtered_predicts['token_str'] == current_token.text, 'total_score']\n",
    "                    selected_token_text_tot_score = selected_predict['total_score']\n",
    "\n",
    "                    print(f\"current_token_text_total_score: {current_token_text_tot_score}, selected_token_total_score: {selected_token_text_tot_score}\")\n",
    "\n",
    "            except:\n",
    "                selected_predict = current_token.text\n",
    "\n",
    "            if selected_predict != current_token.text:\n",
    "                some_token_corrected = True\n",
    "                result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "                text = result_text\n",
    "\n",
    "            if verbose:\n",
    "                print(\"*\" * 50)\n",
    "                print(f\"Token: {current_token.text}\")\n",
    "\n",
    "                print(\"Filtered Predicts: \\n\")\n",
    "                print(filtered_predicts[['token_str', 'score', 'total_score']])\n",
    "\n",
    "                print(f\"{current_token.text} -> {selected_predict} : contextual\")\n",
    "\n",
    "                if current_token.text != selected_predict:\n",
    "                    typo_correction_details = {\n",
    "                        \"raw\": current_token.text,\n",
    "                        \"corrected\": selected_predict,\n",
    "                        \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
    "                        \"type\": \"contextual\"\n",
    "                    }\n",
    "\n",
    "                    # print(typo_correction_details)\n",
    "\n",
    "            if some_token_corrected:\n",
    "                break\n",
    "\n",
    "        if not some_token_corrected:\n",
    "            break\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correction Pipeline Class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_edit_distance_to_length_ratio=0.45,\n",
    "            max_edit_distance=2,\n",
    "            min_score=1e-7,\n",
    "            verbose=False,\n",
    "            top_k=50\n",
    "    ):\n",
    "        self.max_edit_distance_to_length_ratio = max_edit_distance_to_length_ratio\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.min_score = min_score\n",
    "        self.verbose = verbose\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def _lexico_typo_correction(self, text):\n",
    "        return lexico_typo_correction(text, self.max_edit_distance_to_length_ratio, self.max_edit_distance,\n",
    "                                      self.min_score, self.top_k, self.verbose, )\n",
    "\n",
    "    def _contextual_typo_correction(self, text):\n",
    "        return contextual_typo_correction(text, self.max_edit_distance_to_length_ratio, self.max_edit_distance,\n",
    "                                          self.min_score, self.top_k, self.verbose, )\n",
    "\n",
    "    def correction_pipeline(self, text):\n",
    "        print(f\"raw       : {text}\")\n",
    "        # print(\"Lexico Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._lexico_typo_correction(text)\n",
    "\n",
    "        # print(\"Contextual Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._contextual_typo_correction(corrected_text)\n",
    "\n",
    "        print(f\"corrected : {corrected_text}\")\n",
    "        return corrected_text\n",
    "\n",
    "    def __call__(self, text, *args, **kwargs):\n",
    "        return self.correction_pipeline(text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test On Sample Texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spell Correction for text sentences:\n",
      "raw       : Cars have very sweet features. It has two beautifull eye, adorable tiny paws, sharp claws, and two perky ear which are very sensitive to sounds. It has a tiny body covered with smoot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose, a big mouth and a few whiskers under its nose.\n",
      "**************************************************\n",
      "Token: beautifull\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "14  beautiful  0.014907     0.745338\n",
      "beautifull -> beautiful : lexical\n",
      "**************************************************\n",
      "Token: perky\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "11      furry  0.010215     0.127687\n",
      "21       pink  0.004458     0.055723\n",
      "72       very  0.000765     0.012751\n",
      "63     pretty  0.000836     0.010456\n",
      "76      horny  0.000708     0.008844\n",
      "94        pet  0.000562     0.007021\n",
      "187      weak  0.000161     0.002008\n",
      "204     curly  0.000140     0.001753\n",
      "211      dark  0.000129     0.001610\n",
      "219     puppy  0.000124     0.001545\n",
      "245      sexy  0.000100     0.001245\n",
      "perky -> furry : lexical\n",
      "**************************************************\n",
      "Token: whiskers\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "150    whites  0.000083     0.001665\n",
      "154     whisk  0.000077     0.001536\n",
      "175  stickers  0.000061     0.001224\n",
      "whiskers -> whites : lexical\n",
      "**************************************************\n",
      "Token: Cars\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1         Cat  0.246920     3.292269\n",
      "2        cats  0.037513     0.500174\n",
      "5        Cats  0.002972     0.059446\n",
      "6         Its  0.002483     0.024832\n",
      "9        cats  0.001197     0.015960\n",
      "..        ...       ...          ...\n",
      "235      Sure  0.000018     0.000180\n",
      "237    Marcus  0.000018     0.000176\n",
      "238      Cook  0.000018     0.000176\n",
      "239        CA  0.000017     0.000174\n",
      "243      Fair  0.000017     0.000169\n",
      "\n",
      "[73 rows x 3 columns]\n",
      "Cars -> Cars : contextual\n",
      "**************************************************\n",
      "Token: have\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        have  0.823563    32.942514\n",
      "1         has  0.164081     2.187743\n",
      "2         are  0.002810     0.037465\n",
      "6         had  0.000745     0.009933\n",
      "18       have  0.000098     0.003917\n",
      "..        ...       ...          ...\n",
      "212      even  0.000002     0.000016\n",
      "213       're  0.000002     0.000016\n",
      "215        be  0.000002     0.000016\n",
      "216     serve  0.000002     0.000016\n",
      "241      gain  0.000001     0.000013\n",
      "\n",
      "[66 rows x 3 columns]\n",
      "have -> have : contextual\n",
      "**************************************************\n",
      "Token: very\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        very  0.425764    17.030560\n",
      "1        many  0.155030     1.550296\n",
      "17        few  0.004012     0.040118\n",
      "29     lovely  0.001938     0.019379\n",
      "36       tiny  0.001615     0.016153\n",
      "38       real  0.001410     0.014097\n",
      "42       more  0.001074     0.010739\n",
      "50       only  0.000741     0.007408\n",
      "127      Very  0.000126     0.002524\n",
      "91      furry  0.000235     0.002347\n",
      "108       any  0.000178     0.001779\n",
      "164       her  0.000081     0.001078\n",
      "140      rare  0.000103     0.001034\n",
      "153        my  0.000094     0.000942\n",
      "155      even  0.000092     0.000920\n",
      "233     every  0.000045     0.000893\n",
      "160      less  0.000087     0.000867\n",
      "163      pure  0.000083     0.000826\n",
      "171       new  0.000077     0.000767\n",
      "179       ten  0.000071     0.000714\n",
      "182      neat  0.000069     0.000693\n",
      "183       our  0.000069     0.000693\n",
      "186     weird  0.000067     0.000670\n",
      "224      sexy  0.000048     0.000636\n",
      "194      baby  0.000062     0.000620\n",
      "199       are  0.000060     0.000603\n",
      "202     fairy  0.000059     0.000594\n",
      "216       pet  0.000053     0.000530\n",
      "very -> very : contextual\n",
      "**************************************************\n",
      "Token: sweet\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "30      sweet  0.001391     0.069538\n",
      "35      smart  0.001257     0.015708\n",
      "39       neat  0.001139     0.014241\n",
      "48       soft  0.000759     0.009493\n",
      "75      short  0.000370     0.004628\n",
      "102     sleek  0.000232     0.003867\n",
      "90      great  0.000302     0.003773\n",
      "95       sexy  0.000258     0.003229\n",
      "167      same  0.000082     0.001025\n",
      "sweet -> sweet : contextual\n",
      "**************************************************\n",
      "Token: features\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score  total_score\n",
      "8     features  0.024031     1.922451\n",
      "13      nature  0.006077     0.121530\n",
      "46   creatures  0.000734     0.019577\n",
      "49    pictures  0.000680     0.013592\n",
      "93    creature  0.000224     0.004481\n",
      "137    feature  0.000106     0.004229\n",
      "117   gestures  0.000151     0.004038\n",
      "features -> features : contextual\n",
      "**************************************************\n",
      "Token: .\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score  total_score\n",
      "8     features  0.024031     1.922451\n",
      "13      nature  0.006077     0.121530\n",
      "46   creatures  0.000734     0.019577\n",
      "49    pictures  0.000680     0.013592\n",
      "93    creature  0.000224     0.004481\n",
      "137    feature  0.000106     0.004229\n",
      "117   gestures  0.000151     0.004038\n",
      ". -> . : contextual\n",
      "**************************************************\n",
      "Token: It\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0          It  0.904243    18.084863\n",
      "3         Cat  0.011469     0.076457\n",
      "2         One  0.014487     0.072433\n",
      "4        Cats  0.005797     0.028985\n",
      "6         Car  0.004973     0.024866\n",
      "..        ...       ...          ...\n",
      "234       Are  0.000017     0.000083\n",
      "238       Kid  0.000016     0.000080\n",
      "241       car  0.000016     0.000079\n",
      "243       All  0.000016     0.000079\n",
      "249      Cart  0.000015     0.000075\n",
      "\n",
      "[72 rows x 3 columns]\n",
      "It -> It : contextual\n",
      "**************************************************\n",
      "Token: has\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0         has  9.777597e-01    29.332790\n",
      "1        have  1.822253e-02     0.182225\n",
      "2         had  6.441690e-04     0.009663\n",
      "4          is  4.184690e-04     0.004185\n",
      "6       bears  1.935696e-04     0.001452\n",
      "..        ...           ...          ...\n",
      "240    shapes  4.513723e-07     0.000003\n",
      "244        by  4.480494e-07     0.000003\n",
      "245        so  4.469637e-07     0.000003\n",
      "246      they  4.433433e-07     0.000003\n",
      "248      pair  4.396013e-07     0.000003\n",
      "\n",
      "[102 rows x 3 columns]\n",
      "has -> has : contextual\n",
      "**************************************************\n",
      "Token: two\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0           a  0.944216     7.081619\n",
      "1         one  0.043871     0.329031\n",
      "5         two  0.000666     0.019991\n",
      "4         the  0.000953     0.009528\n",
      "3         big  0.001171     0.008783\n",
      "..        ...       ...          ...\n",
      "229       our  0.000002     0.000012\n",
      "234       Big  0.000002     0.000012\n",
      "239       for  0.000002     0.000011\n",
      "242      cool  0.000002     0.000011\n",
      "244       fur  0.000001     0.000011\n",
      "\n",
      "[61 rows x 3 columns]\n",
      "two -> two : contextual\n",
      "**************************************************\n",
      "Token: beautiful\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "17  beautiful  0.012374     1.113616\n",
      "beautiful -> beautiful : contextual\n",
      "**************************************************\n",
      "Token: eye\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        eyes  0.718743    10.781138\n",
      "1        ears  0.108516     0.813873\n",
      "3        legs  0.060135     0.451016\n",
      "4        feet  0.011693     0.087699\n",
      "5        face  0.005236     0.039267\n",
      "18        eye  0.000450     0.013499\n",
      "11       toes  0.001482     0.011113\n",
      "15       nose  0.000931     0.006981\n",
      "27       head  0.000184     0.001380\n",
      "40       cute  0.000072     0.000539\n",
      "63       eyed  0.000030     0.000444\n",
      "64        ear  0.000029     0.000289\n",
      "80       neck  0.000017     0.000127\n",
      "88        big  0.000014     0.000107\n",
      "90        fur  0.000014     0.000106\n",
      "124      Eyes  0.000007     0.000072\n",
      "182      eyes  0.000004     0.000053\n",
      "128      wide  0.000007     0.000051\n",
      "170         ,  0.000004     0.000031\n",
      "171       jaw  0.000004     0.000030\n",
      "180      Face  0.000004     0.000027\n",
      "185     sexes  0.000003     0.000026\n",
      "187       red  0.000003     0.000025\n",
      "193      more  0.000003     0.000025\n",
      "194      Legs  0.000003     0.000024\n",
      "199       and  0.000003     0.000023\n",
      "202      rays  0.000003     0.000023\n",
      "205     types  0.000003     0.000022\n",
      "218       shy  0.000003     0.000020\n",
      "219      size  0.000003     0.000020\n",
      "220      sets  0.000003     0.000020\n",
      "237       dim  0.000002     0.000017\n",
      "238      huge  0.000002     0.000017\n",
      "249       fat  0.000002     0.000016\n",
      "eye -> eye : contextual\n",
      "**************************************************\n",
      "Token: ,\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        eyes  0.718743    10.781138\n",
      "1        ears  0.108516     0.813873\n",
      "3        legs  0.060135     0.451016\n",
      "4        feet  0.011693     0.087699\n",
      "5        face  0.005236     0.039267\n",
      "18        eye  0.000450     0.013499\n",
      "11       toes  0.001482     0.011113\n",
      "15       nose  0.000931     0.006981\n",
      "27       head  0.000184     0.001380\n",
      "40       cute  0.000072     0.000539\n",
      "63       eyed  0.000030     0.000444\n",
      "64        ear  0.000029     0.000289\n",
      "80       neck  0.000017     0.000127\n",
      "88        big  0.000014     0.000107\n",
      "90        fur  0.000014     0.000106\n",
      "124      Eyes  0.000007     0.000072\n",
      "182      eyes  0.000004     0.000053\n",
      "128      wide  0.000007     0.000051\n",
      "170         ,  0.000004     0.000031\n",
      "171       jaw  0.000004     0.000030\n",
      "180      Face  0.000004     0.000027\n",
      "185     sexes  0.000003     0.000026\n",
      "187       red  0.000003     0.000025\n",
      "193      more  0.000003     0.000025\n",
      "194      Legs  0.000003     0.000024\n",
      "199       and  0.000003     0.000023\n",
      "202      rays  0.000003     0.000023\n",
      "205     types  0.000003     0.000022\n",
      "218       shy  0.000003     0.000020\n",
      "219      size  0.000003     0.000020\n",
      "220      sets  0.000003     0.000020\n",
      "237       dim  0.000002     0.000017\n",
      "238      huge  0.000002     0.000017\n",
      "249       fat  0.000002     0.000016\n",
      ", -> , : contextual\n",
      "**************************************************\n",
      "Token: adorable\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "19   adorable  0.001248     0.099846\n",
      "100    double  0.000061     0.001219\n",
      "adorable -> adorable : contextual\n",
      "**************************************************\n",
      "Token: tiny\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "3        tiny  0.093628     3.745108\n",
      "5         big  0.036822     0.368218\n",
      "8         two  0.020015     0.200151\n",
      "19       hind  0.004689     0.062522\n",
      "17      hairy  0.005344     0.053435\n",
      "24       pink  0.003351     0.044683\n",
      "21       long  0.004437     0.044370\n",
      "28       baby  0.002670     0.026704\n",
      "34       tail  0.001753     0.017529\n",
      "53      shiny  0.000913     0.012171\n",
      "45        six  0.001207     0.012072\n",
      "46     skinny  0.001176     0.011765\n",
      "47       wide  0.001140     0.011403\n",
      "51       five  0.001025     0.010246\n",
      "52      giant  0.000939     0.009387\n",
      "56     sticky  0.000818     0.008179\n",
      "57       high  0.000778     0.007783\n",
      "62      thick  0.000676     0.006764\n",
      "79       thin  0.000461     0.006151\n",
      "92       mini  0.000398     0.005310\n",
      "80      dirty  0.000459     0.004585\n",
      "83       grey  0.000440     0.004400\n",
      "86      stick  0.000409     0.004093\n",
      "127      twin  0.000261     0.003475\n",
      "102     funny  0.000341     0.003411\n",
      "136       toy  0.000235     0.003139\n",
      "113      side  0.000309     0.003086\n",
      "116     bunny  0.000300     0.003001\n",
      "119       pig  0.000289     0.002891\n",
      "120     fancy  0.000286     0.002863\n",
      "149      hand  0.000180     0.001800\n",
      "150       one  0.000174     0.001736\n",
      "175      nine  0.000120     0.001605\n",
      "158      gray  0.000155     0.001545\n",
      "182      many  0.000114     0.001518\n",
      "183       tan  0.000113     0.001506\n",
      "167     tiger  0.000130     0.001301\n",
      "169      like  0.000128     0.001282\n",
      "170      size  0.000127     0.001273\n",
      "219       ten  0.000086     0.001142\n",
      "184      tall  0.000113     0.001127\n",
      "189      cozy  0.000111     0.001109\n",
      "192      with  0.000106     0.001055\n",
      "199      lion  0.000098     0.000984\n",
      "203       toe  0.000096     0.000962\n",
      "218       boy  0.000086     0.000862\n",
      "231       and  0.000081     0.000809\n",
      "249      toes  0.000071     0.000708\n",
      "tiny -> tiny : contextual\n",
      "**************************************************\n",
      "Token: paws\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1        ears  0.294395     3.925272\n",
      "5        paws  0.010549     0.421962\n",
      "4        face  0.014240     0.142402\n",
      "7        eyes  0.004282     0.042817\n",
      "18        paw  0.001928     0.038568\n",
      "11       tail  0.003197     0.031972\n",
      "12        ear  0.003141     0.031413\n",
      "13       legs  0.003031     0.030310\n",
      "16       toes  0.002199     0.021991\n",
      "19      hands  0.001788     0.017882\n",
      "23      claws  0.001308     0.017444\n",
      "20       lips  0.001502     0.015018\n",
      "26       hair  0.000784     0.007844\n",
      "28      faces  0.000425     0.004249\n",
      "33        jaw  0.000306     0.004082\n",
      "31       hand  0.000385     0.003849\n",
      "43       jaws  0.000187     0.003735\n",
      "35       arms  0.000267     0.002667\n",
      "46      tails  0.000173     0.001733\n",
      "51      hairs  0.000118     0.001178\n",
      "53      heads  0.000115     0.001153\n",
      "55      nails  0.000111     0.001115\n",
      "57       claw  0.000094     0.000938\n",
      "60        hat  0.000083     0.000833\n",
      "70      waist  0.000066     0.000657\n",
      "78        bow  0.000053     0.000526\n",
      "81       ribs  0.000051     0.000509\n",
      "89       ones  0.000040     0.000397\n",
      "93        cat  0.000034     0.000337\n",
      "94      penis  0.000034     0.000337\n",
      "95       fins  0.000034     0.000336\n",
      "101      mask  0.000028     0.000283\n",
      "118      palm  0.000020     0.000271\n",
      "127     palms  0.000019     0.000248\n",
      "120       arm  0.000020     0.000197\n",
      "125     spots  0.000019     0.000190\n",
      "134      hips  0.000017     0.000166\n",
      "137     balls  0.000016     0.000158\n",
      "158      dash  0.000011     0.000107\n",
      "165       pen  0.000010     0.000097\n",
      "168      dots  0.000009     0.000094\n",
      "172      Face  0.000009     0.000092\n",
      "174       bat  0.000009     0.000089\n",
      "186      bark  0.000008     0.000082\n",
      "230      yawn  0.000006     0.000076\n",
      "200       fat  0.000007     0.000072\n",
      "203      bits  0.000007     0.000070\n",
      "204     patch  0.000007     0.000070\n",
      "205       ant  0.000007     0.000069\n",
      "206      back  0.000007     0.000068\n",
      "208       two  0.000007     0.000067\n",
      "209       car  0.000007     0.000067\n",
      "210      gaze  0.000007     0.000066\n",
      "224       pet  0.000006     0.000059\n",
      "225      toys  0.000006     0.000059\n",
      "229      tips  0.000006     0.000057\n",
      "237      ball  0.000005     0.000055\n",
      "244       and  0.000005     0.000053\n",
      "paws -> paws : contextual\n",
      "**************************************************\n",
      "Token: ,\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1        ears  0.294395     3.925272\n",
      "5        paws  0.010549     0.421962\n",
      "4        face  0.014240     0.142402\n",
      "7        eyes  0.004282     0.042817\n",
      "18        paw  0.001928     0.038568\n",
      "11       tail  0.003197     0.031972\n",
      "12        ear  0.003141     0.031413\n",
      "13       legs  0.003031     0.030310\n",
      "16       toes  0.002199     0.021991\n",
      "19      hands  0.001788     0.017882\n",
      "23      claws  0.001308     0.017444\n",
      "20       lips  0.001502     0.015018\n",
      "26       hair  0.000784     0.007844\n",
      "28      faces  0.000425     0.004249\n",
      "33        jaw  0.000306     0.004082\n",
      "31       hand  0.000385     0.003849\n",
      "43       jaws  0.000187     0.003735\n",
      "35       arms  0.000267     0.002667\n",
      "46      tails  0.000173     0.001733\n",
      "51      hairs  0.000118     0.001178\n",
      "53      heads  0.000115     0.001153\n",
      "55      nails  0.000111     0.001115\n",
      "57       claw  0.000094     0.000938\n",
      "60        hat  0.000083     0.000833\n",
      "70      waist  0.000066     0.000657\n",
      "78        bow  0.000053     0.000526\n",
      "81       ribs  0.000051     0.000509\n",
      "89       ones  0.000040     0.000397\n",
      "93        cat  0.000034     0.000337\n",
      "94      penis  0.000034     0.000337\n",
      "95       fins  0.000034     0.000336\n",
      "101      mask  0.000028     0.000283\n",
      "118      palm  0.000020     0.000271\n",
      "127     palms  0.000019     0.000248\n",
      "120       arm  0.000020     0.000197\n",
      "125     spots  0.000019     0.000190\n",
      "134      hips  0.000017     0.000166\n",
      "137     balls  0.000016     0.000158\n",
      "158      dash  0.000011     0.000107\n",
      "165       pen  0.000010     0.000097\n",
      "168      dots  0.000009     0.000094\n",
      "172      Face  0.000009     0.000092\n",
      "174       bat  0.000009     0.000089\n",
      "186      bark  0.000008     0.000082\n",
      "230      yawn  0.000006     0.000076\n",
      "200       fat  0.000007     0.000072\n",
      "203      bits  0.000007     0.000070\n",
      "204     patch  0.000007     0.000070\n",
      "205       ant  0.000007     0.000069\n",
      "206      back  0.000007     0.000068\n",
      "208       two  0.000007     0.000067\n",
      "209       car  0.000007     0.000067\n",
      "210      gaze  0.000007     0.000066\n",
      "224       pet  0.000006     0.000059\n",
      "225      toys  0.000006     0.000059\n",
      "229      tips  0.000006     0.000057\n",
      "237      ball  0.000005     0.000055\n",
      "244       and  0.000005     0.000053\n",
      ", -> , : contextual\n",
      "**************************************************\n",
      "Token: sharp\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1       sharp  0.118085     5.904238\n",
      "5       small  0.052904     0.661294\n",
      "8       short  0.031258     0.520962\n",
      "63       hard  0.000726     0.012102\n",
      "64      shiny  0.000726     0.009071\n",
      "70      hairy  0.000661     0.008266\n",
      "92      smart  0.000457     0.007612\n",
      "100      dark  0.000414     0.005180\n",
      "151      have  0.000265     0.003311\n",
      "165       had  0.000232     0.002906\n",
      "226    square  0.000139     0.001738\n",
      "239      bear  0.000129     0.001615\n",
      "sharp -> sharp : contextual\n",
      "**************************************************\n",
      "Token: claws\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "2       claws  0.195976     9.798823\n",
      "3        ears  0.132143     1.651785\n",
      "9        jaws  0.003615     0.060243\n",
      "14       paws  0.002327     0.038779\n",
      "22       legs  0.001601     0.020008\n",
      "38       claw  0.000577     0.014424\n",
      "27        jaw  0.000940     0.011745\n",
      "30       lips  0.000849     0.010614\n",
      "43     elbows  0.000333     0.004159\n",
      "52        paw  0.000199     0.002494\n",
      "61       coat  0.000140     0.001754\n",
      "99       cuts  0.000056     0.000702\n",
      "118     blade  0.000040     0.000504\n",
      "130    blades  0.000033     0.000410\n",
      "155     heads  0.000024     0.000297\n",
      "195       saw  0.000016     0.000198\n",
      "196      cane  0.000015     0.000192\n",
      "232      rays  0.000011     0.000132\n",
      "234       cat  0.000010     0.000130\n",
      "241     black  0.000010     0.000123\n",
      "claws -> claws : contextual\n",
      "**************************************************\n",
      "Token: ,\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "2       claws  0.195976     9.798823\n",
      "3        ears  0.132143     1.651785\n",
      "9        jaws  0.003615     0.060243\n",
      "14       paws  0.002327     0.038779\n",
      "22       legs  0.001601     0.020008\n",
      "38       claw  0.000577     0.014424\n",
      "27        jaw  0.000940     0.011745\n",
      "30       lips  0.000849     0.010614\n",
      "43     elbows  0.000333     0.004159\n",
      "52        paw  0.000199     0.002494\n",
      "61       coat  0.000140     0.001754\n",
      "99       cuts  0.000056     0.000702\n",
      "118     blade  0.000040     0.000504\n",
      "130    blades  0.000033     0.000410\n",
      "155     heads  0.000024     0.000297\n",
      "195       saw  0.000016     0.000198\n",
      "196      cane  0.000015     0.000192\n",
      "232      rays  0.000011     0.000132\n",
      "234       cat  0.000010     0.000130\n",
      "241     black  0.000010     0.000123\n",
      ", -> , : contextual\n",
      "**************************************************\n",
      "Token: and\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0         and  0.983301    29.499021\n",
      "2           a  0.002757     0.027573\n",
      "4         and  0.000753     0.022603\n",
      "3         its  0.000794     0.005955\n",
      "5         has  0.000667     0.005003\n",
      "..        ...       ...          ...\n",
      "229      pink  0.000002     0.000015\n",
      "233      into  0.000002     0.000014\n",
      "237       Its  0.000002     0.000014\n",
      "239      than  0.000002     0.000014\n",
      "241       cat  0.000002     0.000013\n",
      "\n",
      "[86 rows x 3 columns]\n",
      "and -> and : contextual\n",
      "**************************************************\n",
      "Token: two\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1         two  0.209424     6.282714\n",
      "0           a  0.334200     2.506502\n",
      "2         one  0.110030     0.825227\n",
      "3        tiny  0.059906     0.449292\n",
      "5         big  0.043377     0.325328\n",
      "13        its  0.005925     0.044441\n",
      "17        red  0.003060     0.022950\n",
      "19        has  0.002877     0.021575\n",
      "23        six  0.002021     0.015159\n",
      "25          2  0.001762     0.013217\n",
      "31        the  0.001302     0.013018\n",
      "34          3  0.001020     0.007652\n",
      "36         an  0.000846     0.006346\n",
      "42        few  0.000658     0.004936\n",
      "45          4  0.000595     0.004465\n",
      "47       tall  0.000548     0.004112\n",
      "52      those  0.000496     0.003718\n",
      "64       twin  0.000370     0.003704\n",
      "57          1  0.000454     0.003404\n",
      "72          5  0.000297     0.002230\n",
      "79       thin  0.000259     0.001941\n",
      "80       also  0.000258     0.001936\n",
      "88        had  0.000235     0.001760\n",
      "96        and  0.000213     0.001601\n",
      "100         6  0.000180     0.001349\n",
      "103        is  0.000164     0.001231\n",
      "115       low  0.000133     0.000996\n",
      "121      good  0.000122     0.000912\n",
      "123        it  0.000118     0.000887\n",
      "151       ten  0.000082     0.000820\n",
      "132       old  0.000105     0.000789\n",
      "140        10  0.000095     0.000713\n",
      "142        on  0.000093     0.000698\n",
      "148       his  0.000084     0.000634\n",
      "153         8  0.000081     0.000611\n",
      "159      that  0.000078     0.000582\n",
      "161       fun  0.000075     0.000559\n",
      "167       pet  0.000070     0.000522\n",
      "173         ,  0.000065     0.000491\n",
      "183       are  0.000060     0.000452\n",
      "184        in  0.000060     0.000450\n",
      "191      this  0.000054     0.000407\n",
      "193       new  0.000053     0.000395\n",
      "196      cool  0.000052     0.000392\n",
      "230        no  0.000039     0.000389\n",
      "239       top  0.000037     0.000366\n",
      "204         7  0.000047     0.000354\n",
      "216       all  0.000043     0.000322\n",
      "228       fat  0.000039     0.000296\n",
      "242        of  0.000036     0.000271\n",
      "247        13  0.000033     0.000249\n",
      "two -> two : contextual\n",
      "**************************************************\n",
      "Token: furry\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "11      furry  0.010520     0.525989\n",
      "67       very  0.000859     0.010742\n",
      "68      hairy  0.000842     0.010522\n",
      "77      horny  0.000758     0.009481\n",
      "80     fluffy  0.000701     0.008768\n",
      "112     funny  0.000398     0.006637\n",
      "127     fuzzy  0.000306     0.005095\n",
      "132      full  0.000282     0.003523\n",
      "159     fancy  0.000221     0.002761\n",
      "199     curly  0.000157     0.002613\n",
      "216       fur  0.000135     0.002256\n",
      "205      ruby  0.000152     0.001906\n",
      "219     puppy  0.000133     0.001667\n",
      "220     bunny  0.000133     0.001664\n",
      "225       fun  0.000128     0.001602\n",
      "243    sturdy  0.000110     0.001376\n",
      "furry -> furry : contextual\n",
      "**************************************************\n",
      "Token: ear\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0        ears  9.919584e-01    14.879376\n",
      "1         ear  1.416664e-03     0.042500\n",
      "2        feet  1.406068e-03     0.010546\n",
      "5        legs  4.171802e-04     0.003129\n",
      "6        paws  3.564114e-04     0.002673\n",
      "10       eyes  1.503074e-04     0.001127\n",
      "15      heads  9.026079e-05     0.000677\n",
      "16       pads  5.550633e-05     0.000416\n",
      "29       head  2.072823e-05     0.000207\n",
      "27       jaws  2.662776e-05     0.000200\n",
      "30       arms  1.969394e-05     0.000148\n",
      "36       back  1.232989e-05     0.000092\n",
      "43       face  1.109439e-05     0.000083\n",
      "62       bars  6.075953e-06     0.000061\n",
      "59       ends  6.596341e-06     0.000049\n",
      "63       tail  5.956139e-06     0.000045\n",
      "85     hearts  3.097002e-06     0.000023\n",
      "89       fans  2.872717e-06     0.000022\n",
      "97      hairs  2.565439e-06     0.000019\n",
      "102         ,  2.337526e-06     0.000018\n",
      "131      hear  1.167486e-06     0.000018\n",
      "103      neck  2.306735e-06     0.000017\n",
      "111      tags  1.808755e-06     0.000014\n",
      "112     parts  1.696616e-06     0.000013\n",
      "115     areas  1.624056e-06     0.000012\n",
      "120      horn  1.457089e-06     0.000011\n",
      "122      bell  1.407204e-06     0.000011\n",
      "186      rear  5.844009e-07     0.000009\n",
      "135     marks  1.124474e-06     0.000008\n",
      "141      mask  1.081574e-06     0.000008\n",
      "160     leads  7.927254e-07     0.000006\n",
      "167     pairs  7.087077e-07     0.000005\n",
      "197     hears  5.251721e-07     0.000005\n",
      "215      deaf  3.957786e-07     0.000004\n",
      "204     seals  4.813389e-07     0.000004\n",
      "247       fur  3.239864e-07     0.000003\n",
      "214      rats  4.012512e-07     0.000003\n",
      "218       and  3.916498e-07     0.000003\n",
      "225      more  3.720024e-07     0.000003\n",
      "240     seats  3.323940e-07     0.000002\n",
      "ear -> ear : contextual\n",
      "**************************************************\n",
      "Token: which\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1       which  0.273621    13.681029\n",
      "7         who  0.000305     0.003810\n",
      "10       with  0.000149     0.002481\n",
      "23      which  0.000049     0.002446\n",
      "17      whose  0.000075     0.000933\n",
      "20      where  0.000057     0.000708\n",
      "53       what  0.000022     0.000274\n",
      "83       each  0.000014     0.000172\n",
      "101      will  0.000010     0.000131\n",
      "126      this  0.000008     0.000104\n",
      "193      mice  0.000005     0.000057\n",
      "196      when  0.000005     0.000057\n",
      "which -> which : contextual\n",
      "**************************************************\n",
      "Token: are\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0         are  7.081077e-01    21.243231\n",
      "1          is  2.842976e-01     2.132232\n",
      "4        were  6.707161e-04     0.006707\n",
      "3         get  7.143740e-04     0.005358\n",
      "5        seem  4.484119e-04     0.003363\n",
      "..        ...           ...          ...\n",
      "237       cut  9.591473e-07     0.000007\n",
      "238      hide  9.550510e-07     0.000007\n",
      "241       had  9.424994e-07     0.000007\n",
      "246        IS  9.026043e-07     0.000007\n",
      "248       ARE  8.997475e-07     0.000007\n",
      "\n",
      "[128 rows x 3 columns]\n",
      "are -> are : contextual\n",
      "**************************************************\n",
      "Token: very\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        very  0.876038    35.041521\n",
      "4        more  0.010844     0.108439\n",
      "23       less  0.000351     0.003508\n",
      "26       even  0.000231     0.002315\n",
      "27       real  0.000216     0.002162\n",
      "37       only  0.000148     0.001483\n",
      "89       very  0.000030     0.001206\n",
      "75       Very  0.000042     0.000842\n",
      "66     overly  0.000055     0.000732\n",
      "61       well  0.000069     0.000685\n",
      "63        are  0.000064     0.000640\n",
      "82       over  0.000038     0.000508\n",
      "111       way  0.000019     0.000193\n",
      "115       red  0.000019     0.000186\n",
      "119        be  0.000018     0.000180\n",
      "136      ever  0.000013     0.000179\n",
      "140      tiny  0.000013     0.000126\n",
      "141      hard  0.000013     0.000126\n",
      "145      uber  0.000011     0.000108\n",
      "151      semi  0.000010     0.000101\n",
      "157       ear  0.000009     0.000092\n",
      "160       yet  0.000009     0.000092\n",
      "178      warm  0.000007     0.000074\n",
      "183       pet  0.000007     0.000072\n",
      "187      many  0.000007     0.000066\n",
      "192      deep  0.000006     0.000063\n",
      "200      sure  0.000006     0.000061\n",
      "211       for  0.000005     0.000054\n",
      "238       far  0.000004     0.000044\n",
      "239      keen  0.000004     0.000044\n",
      "241      ears  0.000004     0.000044\n",
      "244     heavy  0.000004     0.000043\n",
      "very -> very : contextual\n",
      "**************************************************\n",
      "Token: sensitive\n",
      "Filtered Predicts: \n",
      "\n",
      "       token_str     score  total_score\n",
      "0      sensitive  0.980538    88.248405\n",
      "19   insensitive  0.000095     0.002853\n",
      "34     sensitive  0.000029     0.002621\n",
      "27      ensitive  0.000040     0.001809\n",
      "76   sensitivity  0.000011     0.000240\n",
      "196    selective  0.000002     0.000051\n",
      "sensitive -> sensitive : contextual\n",
      "**************************************************\n",
      "Token: to\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0          to  0.954481    19.089615\n",
      "1         for  0.016016     0.106776\n",
      "2          of  0.008119     0.054128\n",
      "3        with  0.006056     0.030279\n",
      "5          on  0.001442     0.009612\n",
      "..        ...       ...          ...\n",
      "210       bit  0.000001     0.000007\n",
      "212      post  0.000001     0.000007\n",
      "228      they  0.000001     0.000006\n",
      "233      nose  0.000001     0.000006\n",
      "239       ant  0.000001     0.000005\n",
      "\n",
      "[94 rows x 3 columns]\n",
      "to -> to : contextual\n",
      "**************************************************\n",
      "Token: sounds\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1       sound  0.147240     4.417196\n",
      "2      sounds  0.058598     3.515875\n",
      "102      loud  0.000135     0.002021\n",
      "109     words  0.000124     0.001863\n",
      "176       sun  0.000064     0.000967\n",
      "230     horns  0.000040     0.000594\n",
      "sounds -> sounds : contextual\n",
      "**************************************************\n",
      "Token: .\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "1       sound  0.147240     4.417196\n",
      "2      sounds  0.058598     3.515875\n",
      "102      loud  0.000135     0.002021\n",
      "109     words  0.000124     0.001863\n",
      "176       sun  0.000064     0.000967\n",
      "230     horns  0.000040     0.000594\n",
      ". -> . : contextual\n",
      "**************************************************\n",
      "Token: It\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0          It  0.900145    18.002909\n",
      "2         Cat  0.028242     0.188283\n",
      "1        Cats  0.029829     0.149144\n",
      "3         Car  0.006820     0.034100\n",
      "7          it  0.002012     0.020124\n",
      "..        ...       ...          ...\n",
      "224       Box  0.000011     0.000053\n",
      "225      Iris  0.000011     0.000053\n",
      "235       May  0.000010     0.000050\n",
      "237      Coat  0.000010     0.000049\n",
      "243      Goat  0.000010     0.000048\n",
      "\n",
      "[81 rows x 3 columns]\n",
      "It -> It : contextual\n",
      "**************************************************\n",
      "Token: has\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0         has  9.899018e-01    29.697053\n",
      "1        have  5.993068e-03     0.059931\n",
      "2          is  1.589902e-03     0.015899\n",
      "3         had  7.047429e-04     0.010571\n",
      "10        has  6.134412e-05     0.001840\n",
      "..        ...           ...          ...\n",
      "234       now  3.933131e-07     0.000003\n",
      "239      rows  3.771693e-07     0.000003\n",
      "240       cut  3.749729e-07     0.000003\n",
      "242       too  3.723232e-07     0.000003\n",
      "248      sits  3.590787e-07     0.000003\n",
      "\n",
      "[98 rows x 3 columns]\n",
      "has -> has : contextual\n",
      "**************************************************\n",
      "Token: a\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0           a  0.969185     9.691846\n",
      "1         its  0.005262     0.013155\n",
      "3         the  0.002729     0.006823\n",
      "5         one  0.002024     0.005060\n",
      "9         two  0.000601     0.001503\n",
      "..        ...       ...          ...\n",
      "213       wee  0.000005     0.000013\n",
      "216       old  0.000005     0.000013\n",
      "227       fun  0.000005     0.000012\n",
      "233      oval  0.000005     0.000012\n",
      "238       too  0.000005     0.000011\n",
      "\n",
      "[65 rows x 3 columns]\n",
      "a -> a : contextual\n",
      "**************************************************\n",
      "Token: tiny\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "14       tiny  0.019226     0.769023\n",
      "4        long  0.052487     0.524871\n",
      "5         big  0.048394     0.483936\n",
      "17       thin  0.015306     0.204079\n",
      "15     skinny  0.018744     0.187440\n",
      "21      thick  0.011258     0.112578\n",
      "28       nice  0.007552     0.075519\n",
      "30       wide  0.006175     0.061745\n",
      "38      shiny  0.004610     0.061470\n",
      "31      hairy  0.006111     0.061108\n",
      "36       tall  0.004738     0.047385\n",
      "45        tan  0.002452     0.032691\n",
      "52       pink  0.001738     0.023168\n",
      "62       cozy  0.001374     0.013735\n",
      "74       grey  0.000951     0.009513\n",
      "77       sexy  0.000865     0.008653\n",
      "93       fine  0.000636     0.008478\n",
      "95      giant  0.000633     0.006326\n",
      "111      gray  0.000496     0.004957\n",
      "113     tight  0.000471     0.004709\n",
      "114      high  0.000467     0.004672\n",
      "137      mini  0.000286     0.003817\n",
      "127       shy  0.000363     0.003632\n",
      "128     stiff  0.000359     0.003587\n",
      "138      firm  0.000286     0.002862\n",
      "144     fancy  0.000258     0.002577\n",
      "147     funny  0.000228     0.002280\n",
      "152      trim  0.000207     0.002070\n",
      "156      body  0.000199     0.001991\n",
      "163      limp  0.000180     0.001800\n",
      "169      baby  0.000173     0.001731\n",
      "174     dirty  0.000165     0.001651\n",
      "190       dry  0.000140     0.001403\n",
      "215     juicy  0.000102     0.001022\n",
      "234      lazy  0.000086     0.000864\n",
      "235      rich  0.000086     0.000856\n",
      "246     sandy  0.000077     0.000774\n",
      "248      main  0.000077     0.000768\n",
      "tiny -> tiny : contextual\n",
      "**************************************************\n",
      "Token: body\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        body  0.538432    21.537261\n",
      "2        nose  0.052178     0.521780\n",
      "6       belly  0.023084     0.230842\n",
      "8        back  0.009382     0.093823\n",
      "11       butt  0.004569     0.045693\n",
      "16       coat  0.003384     0.033837\n",
      "19       foot  0.002141     0.021405\n",
      "28        bum  0.000898     0.008981\n",
      "33       hood  0.000448     0.004484\n",
      "48        bed  0.000307     0.004094\n",
      "46       horn  0.000326     0.003257\n",
      "50       dome  0.000285     0.002846\n",
      "56       ball  0.000230     0.002300\n",
      "59       hide  0.000223     0.002232\n",
      "68       boot  0.000163     0.002170\n",
      "69       bill  0.000155     0.001553\n",
      "74        bag  0.000127     0.001270\n",
      "88        bow  0.000092     0.001222\n",
      "76       comb  0.000117     0.001168\n",
      "77       cone  0.000116     0.001158\n",
      "80       base  0.000109     0.001092\n",
      "170      body  0.000026     0.001027\n",
      "89        top  0.000089     0.000892\n",
      "93       side  0.000084     0.000839\n",
      "96        dog  0.000077     0.000771\n",
      "100      bell  0.000074     0.000736\n",
      "114      bump  0.000061     0.000607\n",
      "135      baby  0.000042     0.000555\n",
      "120      roof  0.000055     0.000546\n",
      "121      home  0.000055     0.000545\n",
      "122       toe  0.000053     0.000531\n",
      "140       box  0.000039     0.000526\n",
      "131       lot  0.000047     0.000467\n",
      "133      toes  0.000044     0.000445\n",
      "155       toy  0.000031     0.000414\n",
      "137      cock  0.000041     0.000411\n",
      "216      Body  0.000015     0.000298\n",
      "186      pony  0.000022     0.000291\n",
      "165      mole  0.000027     0.000267\n",
      "166      down  0.000026     0.000262\n",
      "236       bod  0.000013     0.000261\n",
      "167      form  0.000026     0.000261\n",
      "173       bit  0.000025     0.000251\n",
      "181      hole  0.000023     0.000229\n",
      "182       end  0.000022     0.000224\n",
      "229      bowl  0.000014     0.000184\n",
      "197      room  0.000018     0.000177\n",
      "201       one  0.000017     0.000172\n",
      "205    border  0.000016     0.000164\n",
      "232       cow  0.000013     0.000133\n",
      "233      belt  0.000013     0.000132\n",
      "238       and  0.000013     0.000128\n",
      "239       pot  0.000013     0.000128\n",
      "245      tray  0.000012     0.000122\n",
      "246     bunny  0.000012     0.000122\n",
      "body -> body : contextual\n",
      "**************************************************\n",
      "Token: covered\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0     covered  0.910067    63.704725\n",
      "3      coated  0.009103     0.159308\n",
      "10      cover  0.001438     0.033543\n",
      "9    covering  0.001449     0.025359\n",
      "33    colored  0.000201     0.004699\n",
      "28    layered  0.000227     0.003978\n",
      "73    covered  0.000050     0.003476\n",
      "37   coloured  0.000135     0.002359\n",
      "71     covers  0.000053     0.001241\n",
      "89       over  0.000039     0.000679\n",
      "244   coupled  0.000008     0.000147\n",
      "covered -> covered : contextual\n",
      "**************************************************\n",
      "Token: with\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        with  0.497187    19.887471\n",
      "1          in  0.451862     4.518621\n",
      "21       high  0.000119     0.001581\n",
      "20        the  0.000129     0.001289\n",
      "22        big  0.000118     0.001177\n",
      "33         it  0.000081     0.001086\n",
      "25         is  0.000107     0.001068\n",
      "51        wit  0.000050     0.000992\n",
      "42      white  0.000065     0.000866\n",
      "44       whit  0.000060     0.000799\n",
      "39          w  0.000072     0.000721\n",
      "41       lots  0.000065     0.000654\n",
      "45       well  0.000059     0.000592\n",
      "50       into  0.000052     0.000515\n",
      "63       rich  0.000036     0.000483\n",
      "52       like  0.000047     0.000471\n",
      "128      with  0.000012     0.000466\n",
      "66     within  0.000035     0.000464\n",
      "55      tight  0.000046     0.000456\n",
      "58         to  0.000043     0.000432\n",
      "85        its  0.000024     0.000317\n",
      "72       warm  0.000029     0.000287\n",
      "142      With  0.000011     0.000212\n",
      "97         at  0.000021     0.000208\n",
      "98       much  0.000019     0.000190\n",
      "102       his  0.000018     0.000184\n",
      "117     which  0.000013     0.000175\n",
      "139      wide  0.000011     0.000145\n",
      "112   without  0.000014     0.000141\n",
      "119       out  0.000013     0.000128\n",
      "126        in  0.000012     0.000118\n",
      "127      pink  0.000012     0.000118\n",
      "130       ins  0.000011     0.000115\n",
      "133      fine  0.000011     0.000112\n",
      "134       was  0.000011     0.000112\n",
      "168      both  0.000008     0.000104\n",
      "146       wel  0.000010     0.000098\n",
      "180       wet  0.000007     0.000096\n",
      "152       but  0.000009     0.000089\n",
      "164     twice  0.000008     0.000080\n",
      "166     light  0.000008     0.000079\n",
      "172      inch  0.000008     0.000077\n",
      "204      will  0.000006     0.000077\n",
      "176         h  0.000007     0.000073\n",
      "201       dot  0.000006     0.000058\n",
      "208      tiny  0.000005     0.000054\n",
      "226     dirty  0.000005     0.000047\n",
      "231      each  0.000005     0.000046\n",
      "232     quite  0.000005     0.000045\n",
      "with -> with : contextual\n",
      "**************************************************\n",
      "Token: smoot\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        soft  0.287866     4.797762\n",
      "8       short  0.017014     0.283568\n",
      "19     smooth  0.005475     0.136874\n",
      "26      small  0.004111     0.051391\n",
      "53       some  0.001229     0.015368\n",
      "64      sweet  0.000723     0.009042\n",
      "77       cool  0.000488     0.006106\n",
      "87       more  0.000377     0.004708\n",
      "124     smart  0.000214     0.003567\n",
      "123     snowy  0.000214     0.002678\n",
      "194      wool  0.000091     0.001143\n",
      "206      coat  0.000084     0.001050\n",
      "221      good  0.000070     0.000880\n",
      "smoot -> smoot : contextual\n",
      "**************************************************\n",
      "Token: fur\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0         fur  0.873436    26.203072\n",
      "1           ,  0.051871     0.389030\n",
      "2        hair  0.023550     0.176624\n",
      "15        fur  0.000591     0.017744\n",
      "7        ears  0.001675     0.012562\n",
      "..        ...       ...          ...\n",
      "235        ch  0.000018     0.000136\n",
      "237       ine  0.000018     0.000136\n",
      "240       ots  0.000017     0.000131\n",
      "242       uzz  0.000017     0.000131\n",
      "243      over  0.000017     0.000129\n",
      "\n",
      "[91 rows x 3 columns]\n",
      "fur -> fur : contextual\n",
      "**************************************************\n",
      "Token: and\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0         and  9.142236e-01    27.426707\n",
      "1           ,  6.192294e-02     0.464422\n",
      "2           ;  7.927950e-03     0.059460\n",
      "4         but  2.620471e-03     0.019654\n",
      "7          as  7.859682e-04     0.007860\n",
      "..        ...           ...          ...\n",
      "234       why  5.018851e-07     0.000004\n",
      "238        *,  4.965794e-07     0.000004\n",
      "244        +,  4.643981e-07     0.000003\n",
      "245       off  4.637147e-07     0.000003\n",
      "247           4.625240e-07     0.000003\n",
      "\n",
      "[120 rows x 3 columns]\n",
      "and -> and : contextual\n",
      "**************************************************\n",
      "Token: it\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0          it  0.914659    18.293188\n",
      "5          It  0.002259     0.022591\n",
      "7         cat  0.001627     0.010848\n",
      "10         is  0.001034     0.010340\n",
      "8          he  0.001295     0.008632\n",
      "..        ...       ...          ...\n",
      "218      Mini  0.000005     0.000023\n",
      "227      chin  0.000004     0.000022\n",
      "228      part  0.000004     0.000021\n",
      "244      lion  0.000004     0.000019\n",
      "248       had  0.000004     0.000019\n",
      "\n",
      "[90 rows x 3 columns]\n",
      "it -> it : contextual\n",
      "**************************************************\n",
      "Token: has\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0         has  9.835148e-01    29.505444\n",
      "1        have  9.065843e-03     0.090658\n",
      "2         had  1.279308e-03     0.019190\n",
      "3          is  8.377369e-04     0.008377\n",
      "4       wears  5.459082e-04     0.004094\n",
      "..        ...           ...          ...\n",
      "235       HAS  9.848934e-07     0.000007\n",
      "237      toes  9.644153e-07     0.000007\n",
      "241        or  9.304317e-07     0.000007\n",
      "248       tow  8.602888e-07     0.000006\n",
      "249     boast  8.530600e-07     0.000006\n",
      "\n",
      "[107 rows x 3 columns]\n",
      "has -> has : contextual\n",
      "**************************************************\n",
      "Token: a\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0           a  0.841901     8.419014\n",
      "4         big  0.008511     0.021277\n",
      "6         two  0.007651     0.019127\n",
      "8         one  0.006466     0.016166\n",
      "24        red  0.000793     0.001982\n",
      "26        the  0.000725     0.001813\n",
      "41         an  0.000319     0.001596\n",
      "29        its  0.000589     0.001472\n",
      "33        six  0.000436     0.001089\n",
      "42       tall  0.000305     0.000763\n",
      "45       dark  0.000272     0.000679\n",
      "48       flat  0.000251     0.000627\n",
      "49       many  0.000242     0.000604\n",
      "93          2  0.000095     0.000473\n",
      "56       neat  0.000184     0.000461\n",
      "73         no  0.000137     0.000456\n",
      "63       warm  0.000157     0.000393\n",
      "64       also  0.000157     0.000393\n",
      "68       that  0.000152     0.000380\n",
      "70        pet  0.000148     0.000370\n",
      "109         3  0.000069     0.000345\n",
      "113        as  0.000066     0.000329\n",
      "85        low  0.000106     0.000266\n",
      "103       and  0.000073     0.000244\n",
      "96       tail  0.000089     0.000223\n",
      "99        his  0.000084     0.000211\n",
      "160         4  0.000037     0.000186\n",
      "104       fun  0.000072     0.000181\n",
      "108      gray  0.000070     0.000174\n",
      "171         5  0.000035     0.000173\n",
      "134       fat  0.000050     0.000166\n",
      "114      real  0.000065     0.000162\n",
      "178         6  0.000032     0.000161\n",
      "120       own  0.000057     0.000143\n",
      "148        it  0.000041     0.000138\n",
      "153        of  0.000039     0.000131\n",
      "163       has  0.000036     0.000122\n",
      "172       had  0.000035     0.000115\n",
      "209         1  0.000023     0.000114\n",
      "144      baby  0.000045     0.000112\n",
      "147       few  0.000043     0.000107\n",
      "149       old  0.000041     0.000102\n",
      "188       any  0.000029     0.000097\n",
      "157       fur  0.000039     0.000097\n",
      "241         ,  0.000018     0.000091\n",
      "165      half  0.000036     0.000091\n",
      "166      have  0.000036     0.000089\n",
      "197       tan  0.000027     0.000089\n",
      "200        is  0.000026     0.000086\n",
      "174       new  0.000034     0.000086\n",
      "183       got  0.000031     0.000076\n",
      "186       wet  0.000029     0.000074\n",
      "187       her  0.000029     0.000073\n",
      "189      hard  0.000029     0.000072\n",
      "234       cat  0.000019     0.000064\n",
      "236        so  0.000019     0.000062\n",
      "203       hot  0.000025     0.000062\n",
      "244        on  0.000018     0.000060\n",
      "242      same  0.000018     0.000046\n",
      "247       ten  0.000018     0.000045\n",
      "a -> a : contextual\n",
      "**************************************************\n",
      "Token: furry\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "5       furry  0.070124     3.506180\n",
      "0      fluffy  0.227623     2.845292\n",
      "10      fuzzy  0.011076     0.184607\n",
      "16      curly  0.004749     0.079147\n",
      "29        fur  0.001794     0.029902\n",
      "33      funny  0.001274     0.021230\n",
      "37      hairy  0.001119     0.013983\n",
      "53     sturdy  0.000698     0.008727\n",
      "69      fancy  0.000493     0.006162\n",
      "79        fun  0.000414     0.005172\n",
      "86       grey  0.000367     0.004589\n",
      "89       full  0.000353     0.004415\n",
      "122      very  0.000209     0.002618\n",
      "149     puppy  0.000150     0.001881\n",
      "164      fair  0.000128     0.001597\n",
      "166      gray  0.000125     0.001557\n",
      "190     dirty  0.000098     0.001231\n",
      "219     fiery  0.000070     0.001162\n",
      "217     stray  0.000072     0.000899\n",
      "242     bulky  0.000060     0.000754\n",
      "249      four  0.000059     0.000732\n",
      "furry -> furry : contextual\n",
      "**************************************************\n",
      "Token: tail\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        tail  0.853081    34.123232\n",
      "3        face  0.013237     0.132374\n",
      "6        back  0.007851     0.078513\n",
      "12       chin  0.002129     0.021293\n",
      "23       hair  0.000649     0.008649\n",
      "..        ...       ...          ...\n",
      "232      walk  0.000008     0.000078\n",
      "235      palm  0.000008     0.000077\n",
      "237      bowl  0.000008     0.000077\n",
      "240      pile  0.000008     0.000076\n",
      "242      rack  0.000007     0.000074\n",
      "\n",
      "[61 rows x 3 columns]\n",
      "tail -> tail : contextual\n",
      "**************************************************\n",
      "Token: as\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score   total_score\n",
      "0          as  9.994319e-01  1.998864e+01\n",
      "2          is  4.341456e-05  4.341456e-04\n",
      "3           a  4.053696e-05  4.053696e-04\n",
      "5         and  2.990616e-05  1.993744e-04\n",
      "16         as  8.717012e-06  1.743402e-04\n",
      "..        ...           ...           ...\n",
      "235       red  9.149256e-08  4.574628e-07\n",
      "240       set  9.055540e-08  4.527770e-07\n",
      "244       cut  9.009231e-08  4.504615e-07\n",
      "245      read  8.978490e-08  4.489245e-07\n",
      "249      have  8.647715e-08  4.323858e-07\n",
      "\n",
      "[128 rows x 3 columns]\n",
      "as -> as : contextual\n",
      "**************************************************\n",
      "Token: well\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0        well  9.981590e-01    39.926360\n",
      "3        well  1.143886e-04     0.004576\n",
      "6        tail  5.703839e-05     0.000570\n",
      "10       Well  2.550097e-05     0.000510\n",
      "8         pet  3.247768e-05     0.000325\n",
      "45        wel  4.062671e-06     0.000081\n",
      "37      small  5.647329e-06     0.000056\n",
      "38      whole  5.019601e-06     0.000050\n",
      "54       head  3.580566e-06     0.000036\n",
      "104      hell  1.683165e-06     0.000034\n",
      "59       with  3.216732e-06     0.000032\n",
      "67       pets  2.817535e-06     0.000028\n",
      "81        pen  2.146356e-06     0.000021\n",
      "85        yet  2.087323e-06     0.000021\n",
      "89      below  1.935435e-06     0.000019\n",
      "101      lead  1.747000e-06     0.000017\n",
      "143      tall  1.262399e-06     0.000017\n",
      "105      seen  1.660690e-06     0.000017\n",
      "112       way  1.610702e-06     0.000016\n",
      "121       old  1.514067e-06     0.000015\n",
      "127       end  1.444799e-06     0.000014\n",
      "138      very  1.353070e-06     0.000014\n",
      "141      legs  1.279054e-06     0.000013\n",
      "146        le  1.241003e-06     0.000012\n",
      "147       per  1.233367e-06     0.000012\n",
      "152      hers  1.178352e-06     0.000012\n",
      "165    yellow  1.096544e-06     0.000011\n",
      "172       red  1.053716e-06     0.000011\n",
      "179       her  1.001604e-06     0.000010\n",
      "184      half  9.558503e-07     0.000010\n",
      "186      warm  9.506974e-07     0.000010\n",
      "190      here  9.382317e-07     0.000009\n",
      "192      wide  9.169173e-07     0.000009\n",
      "193      cool  9.132380e-07     0.000009\n",
      "248       all  6.569231e-07     0.000009\n",
      "207      left  8.032288e-07     0.000008\n",
      "208        ly  7.964371e-07     0.000008\n",
      "209      gold  7.957022e-07     0.000008\n",
      "230        ie  7.001852e-07     0.000007\n",
      "234       was  6.929286e-07     0.000007\n",
      "239      best  6.830607e-07     0.000007\n",
      "well -> well : contextual\n",
      "**************************************************\n",
      "Token: .\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0        well  9.981590e-01    39.926360\n",
      "3        well  1.143886e-04     0.004576\n",
      "6        tail  5.703839e-05     0.000570\n",
      "10       Well  2.550097e-05     0.000510\n",
      "8         pet  3.247768e-05     0.000325\n",
      "45        wel  4.062671e-06     0.000081\n",
      "37      small  5.647329e-06     0.000056\n",
      "38      whole  5.019601e-06     0.000050\n",
      "54       head  3.580566e-06     0.000036\n",
      "104      hell  1.683165e-06     0.000034\n",
      "59       with  3.216732e-06     0.000032\n",
      "67       pets  2.817535e-06     0.000028\n",
      "81        pen  2.146356e-06     0.000021\n",
      "85        yet  2.087323e-06     0.000021\n",
      "89      below  1.935435e-06     0.000019\n",
      "101      lead  1.747000e-06     0.000017\n",
      "143      tall  1.262399e-06     0.000017\n",
      "105      seen  1.660690e-06     0.000017\n",
      "112       way  1.610702e-06     0.000016\n",
      "121       old  1.514067e-06     0.000015\n",
      "127       end  1.444799e-06     0.000014\n",
      "138      very  1.353070e-06     0.000014\n",
      "141      legs  1.279054e-06     0.000013\n",
      "146        le  1.241003e-06     0.000012\n",
      "147       per  1.233367e-06     0.000012\n",
      "152      hers  1.178352e-06     0.000012\n",
      "165    yellow  1.096544e-06     0.000011\n",
      "172       red  1.053716e-06     0.000011\n",
      "179       her  1.001604e-06     0.000010\n",
      "184      half  9.558503e-07     0.000010\n",
      "186      warm  9.506974e-07     0.000010\n",
      "190      here  9.382317e-07     0.000009\n",
      "192      wide  9.169173e-07     0.000009\n",
      "193      cool  9.132380e-07     0.000009\n",
      "248       all  6.569231e-07     0.000009\n",
      "207      left  8.032288e-07     0.000008\n",
      "208        ly  7.964371e-07     0.000008\n",
      "209      gold  7.957022e-07     0.000008\n",
      "230        ie  7.001852e-07     0.000007\n",
      "234       was  6.929286e-07     0.000007\n",
      "239      best  6.830607e-07     0.000007\n",
      ". -> . : contextual\n",
      "**************************************************\n",
      "Token: Cats\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        Cars  0.578206    11.564128\n",
      "2          It  0.134531     1.345308\n",
      "4        Cats  0.010806     0.432249\n",
      "3         Car  0.012875     0.171666\n",
      "6         Can  0.001651     0.022010\n",
      "..        ...       ...          ...\n",
      "210     Tears  0.000009     0.000085\n",
      "220       can  0.000008     0.000079\n",
      "223     Races  0.000008     0.000079\n",
      "232    Plants  0.000007     0.000074\n",
      "247      Plus  0.000007     0.000070\n",
      "\n",
      "[84 rows x 3 columns]\n",
      "Cats -> Cats : contextual\n",
      "**************************************************\n",
      "Token: have\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0        have  8.079854e-01    32.319417\n",
      "1         has  1.856388e-01     2.475184\n",
      "2         had  1.327145e-03     0.017695\n",
      "5         are  3.045742e-04     0.004061\n",
      "9        Have  1.821431e-04     0.003643\n",
      "..        ...           ...          ...\n",
      "239      Love  9.792832e-07     0.000013\n",
      "211        ad  1.211848e-06     0.000012\n",
      "218      gain  1.124867e-06     0.000011\n",
      "234     grace  1.019316e-06     0.000010\n",
      "241      nose  9.760869e-07     0.000010\n",
      "\n",
      "[64 rows x 3 columns]\n",
      "have -> have : contextual\n",
      "**************************************************\n",
      "Token: an\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0          an  0.917967    18.359350\n",
      "2           a  0.020899     0.208990\n",
      "3         the  0.002692     0.013462\n",
      "6         big  0.001173     0.005863\n",
      "13        one  0.000626     0.004176\n",
      "..        ...       ...          ...\n",
      "210       our  0.000007     0.000037\n",
      "223      oval  0.000007     0.000034\n",
      "228      kind  0.000007     0.000033\n",
      "230     plain  0.000006     0.000032\n",
      "237       six  0.000006     0.000030\n",
      "\n",
      "[64 rows x 3 columns]\n",
      "an -> an : contextual\n",
      "**************************************************\n",
      "Token: adorable\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score  total_score\n",
      "0     adorable  0.724394    57.951522\n",
      "228  admirable  0.000014     0.000383\n",
      "adorable -> adorable : contextual\n",
      "**************************************************\n",
      "Token: face\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        face  0.863812    34.552498\n",
      "4       faces  0.012804     0.256083\n",
      "8        nose  0.003898     0.038984\n",
      "14      shape  0.001067     0.010673\n",
      "21     facial  0.000539     0.005394\n",
      "22       form  0.000503     0.005030\n",
      "26      image  0.000266     0.002658\n",
      "29       ears  0.000210     0.002103\n",
      "30       size  0.000199     0.001988\n",
      "43       Face  0.000083     0.001666\n",
      "71       face  0.000041     0.001655\n",
      "38      frame  0.000117     0.001561\n",
      "32        cat  0.000135     0.001347\n",
      "33      stare  0.000133     0.001326\n",
      "37       cute  0.000123     0.001234\n",
      "49       hair  0.000075     0.000755\n",
      "81      faced  0.000036     0.000712\n",
      "56        eye  0.000067     0.000666\n",
      "57     facing  0.000065     0.000654\n",
      "72       gaze  0.000041     0.000551\n",
      "62        fur  0.000055     0.000548\n",
      "75       name  0.000038     0.000508\n",
      "69       neck  0.000042     0.000424\n",
      "70       mask  0.000042     0.000421\n",
      "79       baby  0.000036     0.000360\n",
      "89      voice  0.000029     0.000286\n",
      "95       type  0.000026     0.000258\n",
      "114       age  0.000019     0.000253\n",
      "101    glance  0.000023     0.000231\n",
      "107      side  0.000020     0.000201\n",
      "108    stance  0.000020     0.000200\n",
      "113       cap  0.000019     0.000192\n",
      "115      pose  0.000019     0.000189\n",
      "118       jaw  0.000018     0.000184\n",
      "119     taste  0.000018     0.000184\n",
      "122      tail  0.000018     0.000179\n",
      "132       one  0.000016     0.000162\n",
      "153    facade  0.000011     0.000150\n",
      "135       way  0.000014     0.000139\n",
      "186      back  0.000008     0.000112\n",
      "161      life  0.000011     0.000105\n",
      "171       and  0.000009     0.000094\n",
      "179      home  0.000009     0.000088\n",
      "187       hat  0.000008     0.000084\n",
      "207      walk  0.000007     0.000067\n",
      "210      cats  0.000007     0.000066\n",
      "219       ear  0.000006     0.000062\n",
      "236      paws  0.000005     0.000052\n",
      "243      line  0.000005     0.000050\n",
      "246    factor  0.000005     0.000049\n",
      "face -> face : contextual\n",
      "**************************************************\n",
      "Token: with\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0        with  8.571264e-01    34.285057\n",
      "7        like  1.784175e-03     0.017842\n",
      "34       with  4.536726e-05     0.001815\n",
      "22      which  9.329519e-05     0.001244\n",
      "32       With  4.970924e-05     0.000994\n",
      "31         in  5.147417e-05     0.000515\n",
      "35        but  4.484385e-05     0.000448\n",
      "39    without  4.084964e-05     0.000408\n",
      "60        wit  1.861144e-05     0.000372\n",
      "41       such  3.395156e-05     0.000340\n",
      "54         to  2.161355e-05     0.000216\n",
      "55         is  1.945766e-05     0.000195\n",
      "90         at  6.967227e-06     0.000070\n",
      "111       got  4.536239e-06     0.000045\n",
      "130       its  2.984259e-06     0.000040\n",
      "144        it  2.536478e-06     0.000034\n",
      "124      like  3.240791e-06     0.000032\n",
      "187      With  1.476380e-06     0.000030\n",
      "153      both  2.060829e-06     0.000027\n",
      "142      into  2.569757e-06     0.000026\n",
      "170    within  1.729134e-06     0.000023\n",
      "152       the  2.185522e-06     0.000022\n",
      "197       wid  1.372578e-06     0.000018\n",
      "192       not  1.401209e-06     0.000014\n",
      "198      when  1.368917e-06     0.000014\n",
      "203         w  1.335483e-06     0.000013\n",
      "204       big  1.295734e-06     0.000013\n",
      "214       dot  1.232992e-06     0.000012\n",
      "229        if  1.042610e-06     0.000010\n",
      "234       was  9.910887e-07     0.000010\n",
      "242     while  8.826669e-07     0.000009\n",
      "245       via  8.592287e-07     0.000009\n",
      "248      size  8.461175e-07     0.000008\n",
      "with -> with : contextual\n",
      "**************************************************\n",
      "Token: a\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0           a  0.975436     9.754360\n",
      "1         its  0.008541     0.021352\n",
      "4         the  0.001714     0.004284\n",
      "6         one  0.000916     0.002291\n",
      "8         two  0.000675     0.001687\n",
      "..        ...       ...          ...\n",
      "225       hot  0.000002     0.000006\n",
      "232       pet  0.000002     0.000005\n",
      "234       wet  0.000002     0.000005\n",
      "240       cut  0.000002     0.000005\n",
      "242       fun  0.000002     0.000005\n",
      "\n",
      "[73 rows x 3 columns]\n",
      "a -> a : contextual\n",
      "**************************************************\n",
      "Token: tiny\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "4        tiny  0.055113     2.204502\n",
      "0        long  0.184493     1.844928\n",
      "1         big  0.150718     1.507182\n",
      "9        wide  0.016614     0.166142\n",
      "18       pink  0.008546     0.113941\n",
      "34       thin  0.002278     0.030374\n",
      "32       nice  0.002366     0.023659\n",
      "37      funny  0.002099     0.020991\n",
      "40       high  0.001637     0.016374\n",
      "42     skinny  0.001329     0.013292\n",
      "43       tall  0.001212     0.012121\n",
      "47      thick  0.001027     0.010267\n",
      "56       baby  0.000775     0.007754\n",
      "69      shiny  0.000535     0.007132\n",
      "62      fancy  0.000612     0.006120\n",
      "78       fine  0.000457     0.006098\n",
      "67      point  0.000547     0.005466\n",
      "73      giant  0.000478     0.004776\n",
      "95      silly  0.000296     0.002958\n",
      "101      cozy  0.000281     0.002809\n",
      "102       pig  0.000274     0.002738\n",
      "104      very  0.000271     0.002707\n",
      "107        no  0.000253     0.002530\n",
      "111       fun  0.000239     0.002387\n",
      "116      sexy  0.000216     0.002155\n",
      "121     dirty  0.000210     0.002098\n",
      "125      firm  0.000195     0.001947\n",
      "136      ruby  0.000172     0.001718\n",
      "138     stiff  0.000170     0.001704\n",
      "139     bunny  0.000170     0.001701\n",
      "147      mild  0.000158     0.001575\n",
      "150     tight  0.000142     0.001423\n",
      "172      kind  0.000102     0.001362\n",
      "163      bent  0.000112     0.001119\n",
      "166       dim  0.000108     0.001077\n",
      "177     hairy  0.000098     0.000978\n",
      "185     faint  0.000093     0.000925\n",
      "201       shy  0.000076     0.000763\n",
      "203      grey  0.000076     0.000755\n",
      "224      cone  0.000064     0.000640\n",
      "tiny -> tiny : contextual\n",
      "**************************************************\n",
      "Token: nose\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        nose  0.605904    24.236143\n",
      "2         eye  0.068641     0.686406\n",
      "5        face  0.018051     0.180509\n",
      "18       body  0.000641     0.006409\n",
      "20       neck  0.000419     0.004189\n",
      "28       mole  0.000179     0.002387\n",
      "25       size  0.000214     0.002139\n",
      "32       horn  0.000141     0.001415\n",
      "51      noses  0.000057     0.001131\n",
      "45       mask  0.000069     0.000693\n",
      "54       gaze  0.000044     0.000439\n",
      "58      voice  0.000038     0.000377\n",
      "62        bow  0.000031     0.000313\n",
      "81       Nose  0.000015     0.000307\n",
      "75       dome  0.000018     0.000244\n",
      "67        dot  0.000024     0.000242\n",
      "68       look  0.000024     0.000239\n",
      "74       hood  0.000021     0.000206\n",
      "84       hole  0.000015     0.000200\n",
      "107       one  0.000010     0.000129\n",
      "92       comb  0.000012     0.000121\n",
      "98        dog  0.000011     0.000114\n",
      "135      bone  0.000006     0.000079\n",
      "122      foot  0.000007     0.000070\n",
      "126      hook  0.000007     0.000068\n",
      "127     torso  0.000007     0.000066\n",
      "154      name  0.000005     0.000061\n",
      "133      toes  0.000006     0.000060\n",
      "138       bob  0.000005     0.000052\n",
      "141      cute  0.000005     0.000051\n",
      "171      cone  0.000004     0.000050\n",
      "146      bite  0.000005     0.000049\n",
      "151      anus  0.000005     0.000048\n",
      "162      moon  0.000004     0.000040\n",
      "163      roof  0.000004     0.000040\n",
      "165     notch  0.000004     0.000039\n",
      "168      fist  0.000004     0.000038\n",
      "209       nod  0.000003     0.000037\n",
      "180       top  0.000003     0.000034\n",
      "183     globe  0.000003     0.000033\n",
      "187      cage  0.000003     0.000031\n",
      "189      wide  0.000003     0.000031\n",
      "190       eye  0.000003     0.000031\n",
      "235       toe  0.000002     0.000029\n",
      "237     mouse  0.000002     0.000027\n",
      "213       set  0.000003     0.000027\n",
      "219       orb  0.000003     0.000025\n",
      "225      line  0.000002     0.000023\n",
      "243      cock  0.000002     0.000020\n",
      "245      logo  0.000002     0.000019\n",
      "247      form  0.000002     0.000019\n",
      "nose -> nose : contextual\n",
      "**************************************************\n",
      "Token: ,\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0        nose  0.605904    24.236143\n",
      "2         eye  0.068641     0.686406\n",
      "5        face  0.018051     0.180509\n",
      "18       body  0.000641     0.006409\n",
      "20       neck  0.000419     0.004189\n",
      "28       mole  0.000179     0.002387\n",
      "25       size  0.000214     0.002139\n",
      "32       horn  0.000141     0.001415\n",
      "51      noses  0.000057     0.001131\n",
      "45       mask  0.000069     0.000693\n",
      "54       gaze  0.000044     0.000439\n",
      "58      voice  0.000038     0.000377\n",
      "62        bow  0.000031     0.000313\n",
      "81       Nose  0.000015     0.000307\n",
      "75       dome  0.000018     0.000244\n",
      "67        dot  0.000024     0.000242\n",
      "68       look  0.000024     0.000239\n",
      "74       hood  0.000021     0.000206\n",
      "84       hole  0.000015     0.000200\n",
      "107       one  0.000010     0.000129\n",
      "92       comb  0.000012     0.000121\n",
      "98        dog  0.000011     0.000114\n",
      "135      bone  0.000006     0.000079\n",
      "122      foot  0.000007     0.000070\n",
      "126      hook  0.000007     0.000068\n",
      "127     torso  0.000007     0.000066\n",
      "154      name  0.000005     0.000061\n",
      "133      toes  0.000006     0.000060\n",
      "138       bob  0.000005     0.000052\n",
      "141      cute  0.000005     0.000051\n",
      "171      cone  0.000004     0.000050\n",
      "146      bite  0.000005     0.000049\n",
      "151      anus  0.000005     0.000048\n",
      "162      moon  0.000004     0.000040\n",
      "163      roof  0.000004     0.000040\n",
      "165     notch  0.000004     0.000039\n",
      "168      fist  0.000004     0.000038\n",
      "209       nod  0.000003     0.000037\n",
      "180       top  0.000003     0.000034\n",
      "183     globe  0.000003     0.000033\n",
      "187      cage  0.000003     0.000031\n",
      "189      wide  0.000003     0.000031\n",
      "190       eye  0.000003     0.000031\n",
      "235       toe  0.000002     0.000029\n",
      "237     mouse  0.000002     0.000027\n",
      "213       set  0.000003     0.000027\n",
      "219       orb  0.000003     0.000025\n",
      "225      line  0.000002     0.000023\n",
      "243      cock  0.000002     0.000020\n",
      "245      logo  0.000002     0.000019\n",
      "247      form  0.000002     0.000019\n",
      ", -> , : contextual\n",
      "**************************************************\n",
      "Token: a\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0           a  0.952728     9.527278\n",
      "2         and  0.002503     0.008345\n",
      "5         its  0.001441     0.003602\n",
      "6         the  0.001402     0.003505\n",
      "7         big  0.001204     0.003010\n",
      "8         one  0.001128     0.002821\n",
      "27          a  0.000230     0.002296\n",
      "20        two  0.000438     0.001096\n",
      "38          A  0.000145     0.000726\n",
      "39         no  0.000133     0.000442\n",
      "34       neat  0.000173     0.000434\n",
      "37        but  0.000153     0.000382\n",
      "47         an  0.000075     0.000375\n",
      "41        too  0.000111     0.000276\n",
      "51        red  0.000064     0.000160\n",
      "67        fat  0.000047     0.000156\n",
      "52        his  0.000062     0.000156\n",
      "54       tall  0.000059     0.000148\n",
      "57       flat  0.000058     0.000145\n",
      "62        not  0.000053     0.000132\n",
      "74         it  0.000039     0.000130\n",
      "79         to  0.000037     0.000123\n",
      "116        as  0.000022     0.000109\n",
      "101        so  0.000028     0.000093\n",
      "134         1  0.000018     0.000088\n",
      "89       warm  0.000031     0.000078\n",
      "155         ,  0.000015     0.000075\n",
      "161         2  0.000014     0.000071\n",
      "98        yet  0.000028     0.000070\n",
      "99       real  0.000028     0.000070\n",
      "165         A  0.000014     0.000068\n",
      "170         s  0.000013     0.000066\n",
      "138        of  0.000017     0.000056\n",
      "114      dark  0.000023     0.000056\n",
      "168        or  0.000014     0.000045\n",
      "136      yawn  0.000017     0.000043\n",
      "137      also  0.000017     0.000043\n",
      "177        in  0.000013     0.000043\n",
      "141      same  0.000016     0.000041\n",
      "233         3  0.000008     0.000041\n",
      "143      that  0.000016     0.000041\n",
      "147       old  0.000016     0.000039\n",
      "151       new  0.000015     0.000038\n",
      "193        my  0.000011     0.000036\n",
      "163      many  0.000014     0.000035\n",
      "207        is  0.000010     0.000033\n",
      "172       her  0.000013     0.000033\n",
      "197      oval  0.000010     0.000026\n",
      "199      ears  0.000010     0.000026\n",
      "249       has  0.000007     0.000025\n",
      "208       fun  0.000010     0.000025\n",
      "209      half  0.000010     0.000025\n",
      "234       now  0.000008     0.000020\n",
      "239      baby  0.000008     0.000020\n",
      "240       wee  0.000008     0.000020\n",
      "a -> a : contextual\n",
      "**************************************************\n",
      "Token: big\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "3         big  0.067433     2.022979\n",
      "2        tiny  0.172964     1.297231\n",
      "4        wide  0.047373     0.355296\n",
      "8        huge  0.011024     0.082682\n",
      "14       long  0.004462     0.033465\n",
      "17       nice  0.003041     0.022810\n",
      "19       pink  0.002617     0.019630\n",
      "20      tight  0.002372     0.017790\n",
      "22        red  0.002097     0.015725\n",
      "23       thin  0.001872     0.014038\n",
      "51     bright  0.000675     0.005066\n",
      "57        fat  0.000619     0.004642\n",
      "61     bigger  0.000547     0.004104\n",
      "69        sad  0.000498     0.003736\n",
      "70       fine  0.000489     0.003669\n",
      "72       slit  0.000482     0.003611\n",
      "77        shy  0.000454     0.003405\n",
      "89       baby  0.000332     0.002488\n",
      "96       slim  0.000305     0.002284\n",
      "106       wet  0.000269     0.002019\n",
      "116     light  0.000232     0.001742\n",
      "122      kind  0.000215     0.001613\n",
      "123      blue  0.000215     0.001609\n",
      "124       cat  0.000204     0.001529\n",
      "182       bit  0.000098     0.001476\n",
      "131      tidy  0.000176     0.001322\n",
      "135      mild  0.000173     0.001297\n",
      "201       pig  0.000086     0.001290\n",
      "145        no  0.000157     0.001176\n",
      "147       wee  0.000146     0.001093\n",
      "157      busy  0.000134     0.001002\n",
      "160      wise  0.000125     0.000938\n",
      "198       dim  0.000088     0.000885\n",
      "170       fun  0.000110     0.000826\n",
      "171      mini  0.000110     0.000825\n",
      "209      high  0.000080     0.000796\n",
      "214       dog  0.000077     0.000768\n",
      "185      bold  0.000096     0.000719\n",
      "204    biting  0.000082     0.000615\n",
      "210       dry  0.000079     0.000595\n",
      "212       low  0.000079     0.000591\n",
      "225      fair  0.000069     0.000516\n",
      "238      firm  0.000064     0.000478\n",
      "247       coy  0.000058     0.000432\n",
      "big -> big : contextual\n",
      "**************************************************\n",
      "Token: mouth\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0       mouth  0.383506    19.175299\n",
      "29      tooth  0.000619     0.010314\n",
      "28      teeth  0.000627     0.007836\n",
      "31       mole  0.000576     0.007204\n",
      "37       butt  0.000270     0.003372\n",
      "48        mug  0.000164     0.002055\n",
      "60       moon  0.000112     0.001404\n",
      "74      brush  0.000071     0.000893\n",
      "81      laugh  0.000055     0.000682\n",
      "227     mouth  0.000008     0.000424\n",
      "100       cut  0.000034     0.000420\n",
      "163    mouths  0.000016     0.000405\n",
      "103       dot  0.000032     0.000395\n",
      "112       gut  0.000029     0.000357\n",
      "124     blush  0.000026     0.000322\n",
      "126     round  0.000025     0.000316\n",
      "145      soul  0.000020     0.000246\n",
      "169      coat  0.000014     0.000180\n",
      "191     notch  0.000011     0.000144\n",
      "236     pouch  0.000008     0.000133\n",
      "209      foot  0.000010     0.000123\n",
      "mouth -> mouth : contextual\n",
      "**************************************************\n",
      "Token: and\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "0         and  9.487647e-01    28.462942\n",
      "2           ,  2.212701e-02     0.165953\n",
      "8         and  5.791491e-05     0.001737\n",
      "7          an  6.219149e-05     0.000933\n",
      "5           &  8.925254e-05     0.000669\n",
      "..        ...           ...          ...\n",
      "222       two  4.435305e-07     0.000003\n",
      "223       ...  4.434544e-07     0.000003\n",
      "229        's  4.285125e-07     0.000003\n",
      "235       fur  4.203237e-07     0.000003\n",
      "246         )  4.005665e-07     0.000003\n",
      "\n",
      "[85 rows x 3 columns]\n",
      "and -> and : contextual\n",
      "**************************************************\n",
      "Token: a\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "0           a  0.909742     9.097423\n",
      "2         has  0.005747     0.019155\n",
      "7         the  0.001373     0.003433\n",
      "10       also  0.000744     0.001860\n",
      "13       have  0.000522     0.001306\n",
      "..        ...       ...          ...\n",
      "186       get  0.000005     0.000012\n",
      "197       her  0.000004     0.000011\n",
      "235        do  0.000003     0.000010\n",
      "217       how  0.000004     0.000009\n",
      "249       old  0.000003     0.000007\n",
      "\n",
      "[67 rows x 3 columns]\n",
      "a -> a : contextual\n",
      "**************************************************\n",
      "Token: few\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "4         few  0.079842     2.395250\n",
      "0         big  0.162158     1.216182\n",
      "20        red  0.005100     0.050995\n",
      "21       deep  0.004957     0.037180\n",
      "32        fur  0.002861     0.028605\n",
      "31        cat  0.002861     0.021458\n",
      "33       flat  0.002787     0.020900\n",
      "40        two  0.002425     0.018190\n",
      "44         tu  0.002037     0.015275\n",
      "52        fat  0.001394     0.013938\n",
      "62        paw  0.001205     0.012046\n",
      "51        dim  0.001605     0.012039\n",
      "65        wet  0.001111     0.011110\n",
      "55        bit  0.001313     0.009849\n",
      "78        bow  0.000829     0.008285\n",
      "70          p  0.000907     0.006806\n",
      "72       full  0.000897     0.006731\n",
      "79       very  0.000799     0.005992\n",
      "113       fun  0.000581     0.005814\n",
      "114         w  0.000528     0.005279\n",
      "86          l  0.000695     0.005213\n",
      "94        dog  0.000657     0.004928\n",
      "95         sc  0.000654     0.004907\n",
      "96         sm  0.000648     0.004862\n",
      "101       pig  0.000630     0.004723\n",
      "102       tum  0.000628     0.004707\n",
      "107        sn  0.000609     0.004568\n",
      "108       lot  0.000609     0.004565\n",
      "111       dro  0.000587     0.004402\n",
      "130        be  0.000439     0.004393\n",
      "152         f  0.000345     0.003446\n",
      "126       sun  0.000458     0.003433\n",
      "131      neat  0.000435     0.003265\n",
      "134       eye  0.000426     0.003195\n",
      "138        no  0.000402     0.003012\n",
      "139      head  0.000401     0.003006\n",
      "173       fox  0.000297     0.002969\n",
      "181       wee  0.000284     0.002844\n",
      "182       low  0.000282     0.002821\n",
      "184       pet  0.000270     0.002702\n",
      "157      face  0.000327     0.002454\n",
      "204        pe  0.000244     0.002439\n",
      "168      grey  0.000312     0.002337\n",
      "175      sexy  0.000294     0.002203\n",
      "180      four  0.000288     0.002158\n",
      "183       cut  0.000273     0.002045\n",
      "186         c  0.000268     0.002009\n",
      "189       sad  0.000263     0.001973\n",
      "193       pot  0.000256     0.001917\n",
      "194       bul  0.000255     0.001915\n",
      "201      fine  0.000247     0.001851\n",
      "206       shy  0.000240     0.001800\n",
      "211         m  0.000233     0.001744\n",
      "224         t  0.000214     0.001607\n",
      "232         g  0.000204     0.001528\n",
      "235       sly  0.000200     0.001501\n",
      "242        bl  0.000189     0.001418\n",
      "243       one  0.000187     0.001406\n",
      "246       che  0.000186     0.001398\n",
      "few -> few : contextual\n",
      "**************************************************\n",
      "Token: whites\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "7       holes  0.016157     0.242358\n",
      "16      lines  0.005131     0.076968\n",
      "45     spikes  0.000867     0.013000\n",
      "53      bites  0.000608     0.012155\n",
      "56      wings  0.000536     0.008034\n",
      "68       pits  0.000372     0.005585\n",
      "71     things  0.000348     0.005220\n",
      "150    whites  0.000083     0.004995\n",
      "92     plates  0.000214     0.003209\n",
      "106      bits  0.000167     0.002503\n",
      "112    smiles  0.000143     0.002152\n",
      "137     wires  0.000097     0.001939\n",
      "133    shades  0.000104     0.001564\n",
      "138    wheels  0.000096     0.001446\n",
      "149     shoes  0.000084     0.001263\n",
      "154     whisk  0.000077     0.001152\n",
      "165     pipes  0.000067     0.000999\n",
      "172     piles  0.000063     0.000944\n",
      "176     items  0.000060     0.000906\n",
      "185     miles  0.000055     0.000826\n",
      "207     hills  0.000045     0.000670\n",
      "245     chips  0.000034     0.000509\n",
      "whites -> whites : contextual\n",
      "**************************************************\n",
      "Token: under\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score  total_score\n",
      "3       under  0.017825     0.891268\n",
      "9        near  0.003643     0.045534\n",
      "13       over  0.002313     0.028919\n",
      "35        and  0.000168     0.002100\n",
      "41      after  0.000117     0.001468\n",
      "77      cover  0.000017     0.000214\n",
      "93      upper  0.000011     0.000180\n",
      "129     Under  0.000006     0.000159\n",
      "127       per  0.000007     0.000082\n",
      "128     lower  0.000006     0.000081\n",
      "147       one  0.000005     0.000064\n",
      "187     until  0.000003     0.000044\n",
      "234     enter  0.000002     0.000038\n",
      "under -> under : contextual\n",
      "**************************************************\n",
      "Token: its\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "1         its  3.041792e-01     9.125375\n",
      "0         the  6.225514e-01     4.669136\n",
      "4          it  1.332443e-03     0.019987\n",
      "3         his  1.965185e-03     0.019652\n",
      "6           a  1.041141e-03     0.007809\n",
      "..        ...           ...          ...\n",
      "228       ted  9.160901e-07     0.000007\n",
      "230       ear  9.126892e-07     0.000007\n",
      "231        hy  9.081258e-07     0.000007\n",
      "234       fox  9.042713e-07     0.000007\n",
      "236      most  9.023245e-07     0.000007\n",
      "\n",
      "[102 rows x 3 columns]\n",
      "its -> its : contextual\n",
      "**************************************************\n",
      "Token: nose\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "1         eye  8.102453e-02     0.810245\n",
      "4        nose  1.909014e-03     0.076361\n",
      "15       face  2.034523e-04     0.002035\n",
      "23       gaze  9.147952e-05     0.000915\n",
      "28       mask  4.966659e-05     0.000497\n",
      "45      noses  1.310468e-05     0.000262\n",
      "37       coat  2.153199e-05     0.000215\n",
      "40       comb  1.940836e-05     0.000194\n",
      "44       hood  1.328872e-05     0.000133\n",
      "48       body  1.285229e-05     0.000129\n",
      "59       name  8.954529e-06     0.000119\n",
      "53       neck  1.172149e-05     0.000117\n",
      "62       toes  8.356288e-06     0.000084\n",
      "74      voice  5.001449e-06     0.000050\n",
      "85        Eye  3.295985e-06     0.000033\n",
      "90        own  3.019056e-06     0.000030\n",
      "91      cover  2.952936e-06     0.000030\n",
      "98       bowl  2.746460e-06     0.000027\n",
      "101      blue  2.566377e-06     0.000026\n",
      "104      foot  2.490152e-06     0.000025\n",
      "109       eye  2.355130e-06     0.000024\n",
      "119      horn  1.927443e-06     0.000019\n",
      "138      cage  1.501083e-06     0.000015\n",
      "143       dog  1.430291e-06     0.000014\n",
      "170      mole  9.571577e-07     0.000013\n",
      "151       pie  1.259264e-06     0.000013\n",
      "154       too  1.162437e-06     0.000012\n",
      "183     mouse  8.095018e-07     0.000011\n",
      "161      lash  1.077426e-06     0.000011\n",
      "162      door  1.075931e-06     0.000011\n",
      "163      fold  1.064476e-06     0.000011\n",
      "168       dot  9.609552e-07     0.000010\n",
      "203      robe  7.185919e-07     0.000010\n",
      "212       one  6.590725e-07     0.000009\n",
      "179       age  8.692774e-07     0.000009\n",
      "181       bow  8.237377e-07     0.000008\n",
      "184       top  8.023325e-07     0.000008\n",
      "195      roof  7.529658e-07     0.000008\n",
      "200      dish  7.233334e-07     0.000007\n",
      "211       see  6.628192e-07     0.000007\n",
      "221      side  5.997491e-07     0.000006\n",
      "239      dash  5.158143e-07     0.000005\n",
      "242      vest  5.060924e-07     0.000005\n",
      "nose -> nose : contextual\n",
      "**************************************************\n",
      "Token: .\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score  total_score\n",
      "1         eye  8.102453e-02     0.810245\n",
      "4        nose  1.909014e-03     0.076361\n",
      "15       face  2.034523e-04     0.002035\n",
      "23       gaze  9.147952e-05     0.000915\n",
      "28       mask  4.966659e-05     0.000497\n",
      "45      noses  1.310468e-05     0.000262\n",
      "37       coat  2.153199e-05     0.000215\n",
      "40       comb  1.940836e-05     0.000194\n",
      "44       hood  1.328872e-05     0.000133\n",
      "48       body  1.285229e-05     0.000129\n",
      "59       name  8.954529e-06     0.000119\n",
      "53       neck  1.172149e-05     0.000117\n",
      "62       toes  8.356288e-06     0.000084\n",
      "74      voice  5.001449e-06     0.000050\n",
      "85        Eye  3.295985e-06     0.000033\n",
      "90        own  3.019056e-06     0.000030\n",
      "91      cover  2.952936e-06     0.000030\n",
      "98       bowl  2.746460e-06     0.000027\n",
      "101      blue  2.566377e-06     0.000026\n",
      "104      foot  2.490152e-06     0.000025\n",
      "109       eye  2.355130e-06     0.000024\n",
      "119      horn  1.927443e-06     0.000019\n",
      "138      cage  1.501083e-06     0.000015\n",
      "143       dog  1.430291e-06     0.000014\n",
      "170      mole  9.571577e-07     0.000013\n",
      "151       pie  1.259264e-06     0.000013\n",
      "154       too  1.162437e-06     0.000012\n",
      "183     mouse  8.095018e-07     0.000011\n",
      "161      lash  1.077426e-06     0.000011\n",
      "162      door  1.075931e-06     0.000011\n",
      "163      fold  1.064476e-06     0.000011\n",
      "168       dot  9.609552e-07     0.000010\n",
      "203      robe  7.185919e-07     0.000010\n",
      "212       one  6.590725e-07     0.000009\n",
      "179       age  8.692774e-07     0.000009\n",
      "181       bow  8.237377e-07     0.000008\n",
      "184       top  8.023325e-07     0.000008\n",
      "195      roof  7.529658e-07     0.000008\n",
      "200      dish  7.233334e-07     0.000007\n",
      "211       see  6.628192e-07     0.000007\n",
      "221      side  5.997491e-07     0.000006\n",
      "239      dash  5.158143e-07     0.000005\n",
      "242      vest  5.060924e-07     0.000005\n",
      ". -> . : contextual\n",
      "corrected : Cars have very sweet features. It has two beautiful eye, adorable tiny paws, sharp claws, and two furry ear which are very sensitive to sounds. It has a tiny body covered with smoot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose, a big mouth and a few whites under its nose.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Cars have very sweet features. It has two beautiful eye, adorable tiny paws, sharp claws, and two furry ear which are very sensitive to sounds. It has a tiny body covered with smoot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose, a big mouth and a few whites under its nose.'"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_EDIT_DISTANCE_TO_LEN_RATIO = 0.4\n",
    "MAX_EDIT_DISTANCE = 3\n",
    "MIN_SCORE = 0.0\n",
    "TOP_K = 250\n",
    "VERBOSE = True\n",
    "\n",
    "if language == 'en':\n",
    "    # input_text = \"The capitan of Iran is tehran.\"\n",
    "    # input_text = \"i am speeking english very wall.\"\n",
    "    # input_text = \"He was stadying english for the finall exam.\"\n",
    "    # text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "    # text = \"I'm a very [MASK] player in football.\"\n",
    "    # input_text = \"He drove a cat.\"\n",
    "    # text = \"do you want to watch tv.\"\n",
    "    # text = \"I love playing [MASK].\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        The quantity thoery of money also assume that the quantity of money in an economy has a large influense on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the sopply of gods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        Does it privent Iran from getting nuclear weapens. Many exports say that if all parties adhered to their pledges, the deal almost certainly could have achieved that goal for longer than a dekade!\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        The Federal Reserve monitor risks to the financal system and works to help ensure the system supports a haelthy economy for U.S. households, communities, and busineses.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        Bitcoin is a decentrallized digital curency that can be transfered on the peer-to-peer bitcoin network. Bitcoin transactions are veryfied by network nodes throgh cryptography and recorded in a public distributed ledger called a blockchain. The criptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The curency began use in 2009 when its implemntation was released as open-source software.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        The 2022 FILA World Cup is scheduled to be the 22nd running of the FILA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 Novamber to 18 Decamber 2022.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        President Daneld Trump annonced on Tuesday he will withdraw the United States from the Iran nuclear deal and restore far-reaching sanktions aimed at withdrawal Iran from the global finansial system.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        Cars have very sweet features. It has two beautifull eye, adorable tiny paws, sharp claws, and two fury ear which are very sensitive to sounds. It has a tiny body covered with smoot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose, a big mouth and a few whiskers under its nose.\n",
    "    \"\"\"\n",
    "\n",
    "    # input_text = \"\"\"\n",
    "    #     I am going to stadiom.\n",
    "    # \"\"\"\n",
    "\n",
    "    if model_type != 'roberta':\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "if language == 'fa':\n",
    "    input_text = \"          .\"\n",
    "    input_text = \"         .         .\"\n",
    "    input_text = \"      .\"\n",
    "    input_text = \"   .\"\n",
    "    input_text = \"   .\"\n",
    "    input_text = \"                  .\"\n",
    "    input_text = \"            1850       . \"\n",
    "    input_text = \"   !\"\n",
    "    input_text = \"   3    .\"\n",
    "\n",
    "    input_text = \"                             .          \"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "                                        .\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "                                            .\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "              .\n",
    "    \"\"\"\n",
    "\n",
    "input_text = input_text.strip()\n",
    "\n",
    "spell_corrector = SpellCorrector(MAX_EDIT_DISTANCE_TO_LEN_RATIO, MAX_EDIT_DISTANCE, MIN_SCORE, VERBOSE, TOP_K)\n",
    "from spacy import displacy\n",
    "# displacy.render(doc, style=\"dep\")\n",
    "\n",
    "print(\"Spell Correction for text sentences:\")\n",
    "result = spell_corrector(input_text)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"whisker\" in vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}