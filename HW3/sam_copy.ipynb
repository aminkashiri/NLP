{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Required Libraries\n",
    "In this project, we use **transformers** library (from **huggingface.co**) to use the pre-trained **BERT** base model. We use BERT and RoBERTa models for English and BERT and ALBERT models for Persian."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM, AlbertTokenizer, AlbertForMaskedLM, RobertaTokenizer, RobertaModel\n",
    "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import stanza\n",
    "import spacy\n",
    "import spacy_stanza"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models\n",
    "\n",
    "The following is a brief description of these Transformer models and their differences and similarities with the base bert model:\n",
    "\n",
    "1. ALBERT: As stated earlier, BERT base consists of 110 million parameters which makes it computationally intensive and therefore a light version was required with reduced parameters. ALBERT model has 12 million parameters with 768 hidden layers and 128 embedding layers. As expected, the lighter model reduced the training time and inference time. To achieve lesser set of parameters, the **Cross-layer parameter sharing** & **Factorized embedding layer parameterization** techniques are used.\n",
    "\n",
    "2. RoBERTa stands for “Robustly Optimized BERT pre-training Approach”. In many ways this is a better version of the BERT model. The key points of difference are as follows:\n",
    "\n",
    "    - **Dynamic Masking**: BERT uses static masking i.e. the same part of the sentence is masked in each Epoch. In contrast, RoBERTa uses dynamic masking, wherein for different Epochs different part of the sentences are masked. This makes the model more robust.\n",
    "\n",
    "    - **Remove NSP Task**: It was observed that the NSP task is not very useful for pre-training the BERT model. Therefore, the RoBERTa only with the MLM task.\n",
    "\n",
    "    - **More data Points**: BERT is pre-trained on “Toronto BookCorpus” and “English Wikipedia datasets” i.e. as a total of 16 GB of data. In contrast, in addition to these two datasets, RoBERTa was also trained on other datasets like CC-News (Common Crawl-News), Open WebText etc. The total size of these datasets is around 160 GB.\n",
    "\n",
    "    - **Large Batch size**: To improve on the speed and performance of the model, RoBERTa used a batch size of 8,000 with 300,000 steps. In comparison, BERT uses a batch size of 256 with 1 million steps.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cpu\n",
      "fa bert Model Loaded ...\n"
     ]
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vocabulary\n",
    "\n",
    "We use transformer model vocabulary to identify possible typos (misspelling). In this way, if a word is not in the vocabulary, it **probably** has a misspelling. In the next step, this typo is corrected with the help of a pre-trained model predictions and lexical distance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stanza\n",
    "\n",
    "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stanza\n",
    "\n",
    "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if language == 'fa':\n",
    "    stanza.install_corenlp()\n",
    "    stanza.download('fa')\n",
    "    nlp = spacy_stanza.load_pipeline(\"fa\")\n",
    "\n",
    "elif language == 'en':\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Stanza: {language} not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Persian texts, a semicolon plays a key role. Unfortunately, pre-trained models in Persian do not support half space and their predicted words do not have half space.\n",
    "With this function, if the difference between the predicted words and the main word in the given input is only contains half-space, we do not change the main word in the given input."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Correct Lexico Typo"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spell Correction\n",
    "\n",
    "## Correct Lexico Typo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Correct Contextual Typo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def contextual_typo_correction(\n",
    "        text,\n",
    "        alpha=10,\n",
    "        max_edit_distance=2,\n",
    "        top_k=10,\n",
    "        verbose=False,\n",
    "):\n",
    "    doc = nlp(text)\n",
    "    for index in range(len(doc)):\n",
    "\n",
    "        current_token: Token = doc[index]\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(f\"Token: {current_token.text}\")\n",
    "\n",
    "        start_char_index = current_token.idx\n",
    "        end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "        masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "        predicts = unmasker(masked_text, top_k=top_k)\n",
    "        ### Select Token From Predicts\n",
    "        predicts = pd.DataFrame(predicts)\n",
    "\n",
    "        try:\n",
    "            if current_token.text in string.punctuation:\n",
    "                filtered_predicts = predicts.loc[predicts['token_str'].apply(lambda tk: tk in string.punctuation), :].copy()\n",
    "                selected_predict = filtered_predicts['token_str'].iloc[0]\n",
    "\n",
    "            elif any(c.isdigit() for c in current_token.text):\n",
    "                selected_predict = current_token.text\n",
    "\n",
    "            else:\n",
    "                predicts.loc[:, 'token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "                predicts.loc[:, 'edit_distance'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk))\n",
    "\n",
    "                # Filter tokens with at most 3 edit distance\n",
    "                filtered_predicts = predicts.loc[predicts['edit_distance'] <= max_edit_distance, :].copy()\n",
    "\n",
    "                # Apply total score function\n",
    "                # e: edit distance + 1\n",
    "                # l: token length\n",
    "                filtered_predicts.loc[:, 'e_to_l'] = (filtered_predicts.loc[:, 'edit_distance'] + 1) / len(current_token.text)\n",
    "\n",
    "                filtered_predicts.loc[:, 'total_score'] = filtered_predicts.loc[:, 'score'] / filtered_predicts.loc[:, 'e_to_l'] ** alpha\n",
    "\n",
    "                filtered_predicts = filtered_predicts.sort_values('total_score', ascending=False)\n",
    "                selected_predict_row = filtered_predicts.iloc[0, :]\n",
    "\n",
    "                selected_predict = selected_predict_row['token_str']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(filtered_predicts)}\")\n",
    "            selected_predict = current_token.text\n",
    "\n",
    "        if selected_predict != current_token.text:\n",
    "            if not half_space_case(selected_predict, current_token.text):\n",
    "                text = masked_text.replace(MASK, selected_predict, 1)\n",
    "                doc = nlp(text)\n",
    "\n",
    "            else:\n",
    "                vocab.add(current_token.text)\n",
    "                selected_predict = current_token.text\n",
    "\n",
    "        if verbose:\n",
    "            if current_token.text != selected_predict:\n",
    "                print(\"Filtered Predicts: \\n\")\n",
    "                print(filtered_predicts[['token_str', 'score', 'total_score']])\n",
    "\n",
    "                print(f\"{current_token.text} -> {selected_predict} : contextual\")\n",
    "\n",
    "                typo_correction_details = {\n",
    "                    \"raw\": current_token.text,\n",
    "                    \"corrected\": selected_predict,\n",
    "                    \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
    "                    \"around\": text[start_char_index - 10: end_char_index + 10],\n",
    "                    \"type\": \"contextual\"\n",
    "                }\n",
    "\n",
    "                print(typo_correction_details)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Correction Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=5,\n",
    "            max_edit_distance=2,\n",
    "            verbose=False,\n",
    "            top_k=50\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def _lexico_typo_correction(self, text):\n",
    "        return lexico_typo_correction(text, self.alpha, self.max_edit_distance, self.top_k, self.verbose, )\n",
    "\n",
    "    def _contextual_typo_correction(self, text):\n",
    "        return contextual_typo_correction(text, self.alpha, self.max_edit_distance, self.top_k, self.verbose, )\n",
    "\n",
    "    def correction_pipeline(self, text):\n",
    "        # print(\"Lexico Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._lexico_typo_correction(text)\n",
    "\n",
    "        # print(\"Contextual Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._contextual_typo_correction(corrected_text)\n",
    "\n",
    "        return corrected_text\n",
    "\n",
    "    def __call__(self, text, *args, **kwargs):\n",
    "        return self.correction_pipeline(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Token: کسف\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score   total_score\n",
      "2          کشف  0.065814  12619.911998\n",
      "160        کسب  0.000415     79.552601\n",
      "1898       کسر  0.000008      1.567860\n",
      "3022       کسی  0.000004      0.753784\n",
      "3215        کف  0.000004      0.685134\n",
      "...        ...       ...           ...\n",
      "4920        دس  0.000002      0.000002\n",
      "4939        نف  0.000002      0.000002\n",
      "4940       کلک  0.000002      0.000002\n",
      "4952        سگ  0.000002      0.000002\n",
      "4992        جس  0.000002      0.000002\n",
      "\n",
      "[120 rows x 3 columns]\n",
      "کسف -> کشف : lexical\n",
      "{'raw': 'کسف', 'corrected': 'کشف', 'span': '[32, 35]', 'type': 'lexical'}\n",
      "**************************************************\n",
      "Token: تیرانی\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score   total_score\n",
      "24      ایرانی  0.006808  1.401631e+12\n",
      "1216    تهرانی  0.000036  7.443975e+09\n",
      "69     تایوانی  0.001803  1.936309e+06\n",
      "344     نورانی  0.000211  2.263658e+05\n",
      "406     شیرازی  0.000172  1.843384e+05\n",
      "633     کیهانی  0.000091  9.744183e+04\n",
      "665      ایران  0.000084  9.063041e+04\n",
      "723     شیطانی  0.000076  8.150461e+04\n",
      "991     حیوانی  0.000048  5.149391e+04\n",
      "1084    بیرونی  0.000042  4.524209e+04\n",
      "1348    میدانی  0.000031  3.366329e+04\n",
      "1506     میانی  0.000027  2.860571e+04\n",
      "2437    میراثی  0.000013  1.403591e+04\n",
      "2831     کیانی  0.000010  1.101377e+04\n",
      "2985    ایمانی  0.000009  1.020002e+04\n",
      "3084     قرانی  0.000009  9.704528e+03\n",
      "3803    تمرینی  0.000007  6.990530e+03\n",
      "3816    هیجانی  0.000006  6.956379e+03\n",
      "3985     تراپی  0.000006  6.501799e+03\n",
      "4087    گیلانی  0.000006  6.240923e+03\n",
      "4156     ترانه  0.000006  6.098743e+03\n",
      "4919    شیرینی  0.000004  4.668734e+03\n",
      "4942    شیبانی  0.000004  4.636539e+03\n",
      "4985     بیانی  0.000004  4.563315e+03\n",
      "تیرانی -> ایرانی : lexical\n",
      "{'raw': 'تیرانی', 'corrected': 'ایرانی', 'span': '[57, 63]', 'type': 'lexical'}\n",
      "**************************************************\n",
      "Token: پس\n",
      "**************************************************\n",
      "Token: از\n",
      "**************************************************\n",
      "Token: سال‌ها\n",
      "**************************************************\n",
      "Token: تلاش\n",
      "**************************************************\n",
      "Token: ،\n",
      "**************************************************\n",
      "Token: رازی\n",
      "**************************************************\n",
      "Token: موفق\n",
      "**************************************************\n",
      "Token: به\n",
      "**************************************************\n",
      "Token: کشف\n",
      "**************************************************\n",
      "Token: الکل\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score  total_score\n",
      "4957      الکر  0.000004  4418.687012\n",
      "309       مشکل  0.000114     0.638520\n",
      "338       سلول  0.000101     0.564395\n",
      "382        اشک  0.000087     0.485542\n",
      "428      اشکال  0.000077     0.431408\n",
      "462       حلال  0.000072     0.401030\n",
      "465       دلیل  0.000071     0.396473\n",
      "490        اول  0.000066     0.369756\n",
      "576       الهه  0.000055     0.310379\n",
      "619       الیس  0.000052     0.292099\n",
      "717        ملک  0.000044     0.248922\n",
      "791       الاغ  0.000040     0.223200\n",
      "895        شکل  0.000035     0.194647\n",
      "914       ملکه  0.000034     0.189907\n",
      "1017      الهی  0.000030     0.170131\n",
      "1031     مایکل  0.000030     0.168873\n",
      "1225      الگو  0.000025     0.140755\n",
      "1336       الم  0.000023     0.128296\n",
      "1364      ایکو  0.000022     0.125766\n",
      "1376      الشی  0.000022     0.123204\n",
      "1392       علل  0.000022     0.121653\n",
      "1439      سالک  0.000021     0.117539\n",
      "1482      اشکی  0.000020     0.113337\n",
      "1511      هالک  0.000020     0.111447\n",
      "1621       الن  0.000018     0.103381\n",
      "1689       اصل  0.000018     0.098078\n",
      "1742      نیکل  0.000017     0.095051\n",
      "1744      التر  0.000017     0.095039\n",
      "1881      الاس  0.000015     0.086201\n",
      "1912        اک  0.000015     0.084287\n",
      "2018      الفر  0.000014     0.079083\n",
      "2166      الفی  0.000013     0.072338\n",
      "2360      النا  0.000011     0.063639\n",
      "2469      الوا  0.000011     0.060707\n",
      "2570       الر  0.000010     0.057468\n",
      "2635       لکه  0.000010     0.055566\n",
      "2849       الف  0.000009     0.049500\n",
      "2861       اپل  0.000009     0.049311\n",
      "2879       الص  0.000009     0.048861\n",
      "3007      ایکس  0.000008     0.046332\n",
      "3073     الکسی  0.000008     0.044788\n",
      "3267      المپ  0.000007     0.041170\n",
      "3314      هلال  0.000007     0.040212\n",
      "3552       اکر  0.000007     0.036742\n",
      "3611       ارک  0.000006     0.035893\n",
      "3719       الغ  0.000006     0.034763\n",
      "3859      الیز  0.000006     0.033119\n",
      "3876      ملکی  0.000006     0.032927\n",
      "3999      افضل  0.000006     0.031562\n",
      "4139       الع  0.000005     0.030017\n",
      "4200      جلال  0.000005     0.029380\n",
      "4282     اسکول  0.000005     0.028579\n",
      "4628     ادکلن  0.000005     0.025573\n",
      "4671       الب  0.000004     0.025168\n",
      "4830      مالک  0.000004     0.024047\n",
      "4848      موکل  0.000004     0.023906\n",
      "4995     البلا  0.000004     0.022796\n",
      "الکل -> الکر : contextual\n",
      "{'raw': 'الکل', 'corrected': 'الکر', 'span': '[36, 40]', 'around': 'فق به کشف الکر شد. این د', 'type': 'contextual'}\n",
      "**************************************************\n",
      "Token: شد\n",
      "**************************************************\n",
      "Token: .\n",
      "**************************************************\n",
      "Token: این\n",
      "**************************************************\n",
      "Token: دانشمند\n",
      "**************************************************\n",
      "Token: ایرانی\n",
      "**************************************************\n",
      "Token: باعث\n",
      "**************************************************\n",
      "Token: افتخار\n",
      "**************************************************\n",
      "Token: در\n",
      "**************************************************\n",
      "Token: تاریخ\n",
      "**************************************************\n",
      "Token: کور\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str         score   total_score\n",
      "16        کشور  2.101800e-03  4.030225e+02\n",
      "365        نور  2.548362e-05  4.886511e+00\n",
      "590        کار  1.460557e-05  2.800633e+00\n",
      "1267      کوری  5.694038e-06  1.091838e+00\n",
      "1297       کفر  5.556643e-06  1.065492e+00\n",
      "...        ...           ...           ...\n",
      "4892       بکر  8.531048e-07  8.531048e-07\n",
      "4907      بلور  8.479088e-07  8.479088e-07\n",
      "4912      کوهن  8.451149e-07  8.451149e-07\n",
      "4931      شکوه  8.416659e-07  8.416659e-07\n",
      "4936      کافر  8.412165e-07  8.412165e-07\n",
      "\n",
      "[214 rows x 3 columns]\n",
      "کور -> کشور : contextual\n",
      "{'raw': 'کور', 'corrected': 'کشور', 'span': '[85, 88]', 'around': ' در تاریخ کشور است.', 'type': 'contextual'}\n",
      "**************************************************\n",
      "Token: است\n",
      "**************************************************\n",
      "Token: .\n",
      "False\n",
      "پس از سال‌ها تلاش، رازی موفق به کشف الکر شد. این دانشمند ایرانی باعث افتخار در تاریخ کشور است.\n",
      "\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      " * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Token: صفید\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score   total_score\n",
      "3         سفید  0.018338  1.968996e+07\n",
      "3307       صید  0.000029  3.096768e+04\n",
      "75         فیل  0.001177  6.588873e+00\n",
      "225        دید  0.000452  2.532480e+00\n",
      "245       شاید  0.000417  2.334470e+00\n",
      "340       سپید  0.000303  1.699328e+00\n",
      "344       جدید  0.000300  1.678339e+00\n",
      "568      سفیدی  0.000187  1.047383e+00\n",
      "637        عید  0.000168  9.410668e-01\n",
      "729       خرید  0.000149  8.333974e-01\n",
      "1046      سعید  0.000101  5.654125e-01\n",
      "1089       شید  0.000097  5.448653e-01\n",
      "1425       صفر  0.000072  4.051305e-01\n",
      "1484      شدید  0.000069  3.865048e-01\n",
      "2241       کید  0.000045  2.525368e-01\n",
      "2350       صمد  0.000043  2.398873e-01\n",
      "2686       فین  0.000037  2.051143e-01\n",
      "2818      واید  0.000035  1.945058e-01\n",
      "3755      صدفی  0.000025  1.381866e-01\n",
      "3839       فیک  0.000024  1.342775e-01\n",
      "3918      نوید  0.000023  1.313312e-01\n",
      "4593     وسفید  0.000019  1.055879e-01\n",
      "4691      ساید  0.000018  1.027372e-01\n",
      "4710       فیت  0.000018  1.021590e-01\n",
      "4814       فهد  0.000018  9.918283e-02\n",
      "4870      صلیب  0.000017  9.761945e-02\n",
      "صفید -> سفید : lexical\n",
      "{'raw': 'صفید', 'corrected': 'سفید', 'span': '[23, 27]', 'type': 'lexical'}\n",
      "**************************************************\n",
      "Token: جیران\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score   total_score\n",
      "1        ایران  0.062834  5.449974e+10\n",
      "2978     گیران  0.000009  7.663108e+06\n",
      "11        جهان  0.012803  5.791403e+04\n",
      "51       تهران  0.002499  1.130176e+04\n",
      "119     دیگران  0.000922  4.169385e+03\n",
      "305      بحران  0.000267  1.206458e+03\n",
      "311       میان  0.000262  1.183398e+03\n",
      "445      نیسان  0.000160  7.217339e+02\n",
      "482      بیرون  0.000145  6.560592e+02\n",
      "733       یوان  0.000078  3.534634e+02\n",
      "987       گران  0.000049  2.204637e+02\n",
      "1047     میدان  0.000045  2.049637e+02\n",
      "1108     کیهان  0.000041  1.866335e+02\n",
      "1189     میزان  0.000037  1.677383e+02\n",
      "1360     شیراز  0.000030  1.338128e+02\n",
      "1371    ایرانی  0.000029  1.318513e+02\n",
      "1460     جریان  0.000026  1.194746e+02\n",
      "1512     میلان  0.000025  1.130646e+02\n",
      "1564      قران  0.000024  1.082221e+02\n",
      "1772      جیرو  0.000020  8.900312e+01\n",
      "1798     اکران  0.000019  8.695347e+01\n",
      "1864    مدیران  0.000018  8.339782e+01\n",
      "2072     تیراژ  0.000016  7.145847e+01\n",
      "2139     بیوان  0.000015  6.806531e+01\n",
      "2247     گیلان  0.000014  6.261578e+01\n",
      "2871      بیان  0.000009  4.251667e+01\n",
      "3111     پیمان  0.000008  3.714883e+01\n",
      "3115     عمران  0.000008  3.699655e+01\n",
      "3136     پیکان  0.000008  3.646388e+01\n",
      "3155     باران  0.000008  3.611441e+01\n",
      "3528     کاران  0.000007  2.984995e+01\n",
      "3720     سیلان  0.000006  2.742126e+01\n",
      "4214      گیرا  0.000005  2.214508e+01\n",
      "4302     دوران  0.000005  2.145724e+01\n",
      "4313    جایشان  0.000005  2.137703e+01\n",
      "4536    ایروان  0.000004  1.969551e+01\n",
      "4924     هیجان  0.000004  1.718772e+01\n",
      "4951    شمیران  0.000004  1.704467e+01\n",
      "جیران -> ایران : lexical\n",
      "{'raw': 'جیران', 'corrected': 'ایران', 'span': '[71, 76]', 'type': 'lexical'}\n",
      "**************************************************\n",
      "Token: وقتی\n",
      "**************************************************\n",
      "Token: قیمت\n",
      "**************************************************\n",
      "Token: گوست\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str     score   total_score\n",
      "3         گوشت  0.060875  6.536354e+07\n",
      "191       پوست  0.000381  4.088512e+05\n",
      "1836       وست  0.000014  1.464004e+04\n",
      "2618       گست  0.000008  8.369703e+03\n",
      "53        گوجه  0.002293  1.284129e+01\n",
      "...        ...       ...           ...\n",
      "4882      گوگل  0.000003  1.516884e-02\n",
      "4883      زیست  0.000003  1.516687e-02\n",
      "4888     پوستی  0.000003  1.515665e-02\n",
      "4894      طوسی  0.000003  1.512905e-02\n",
      "4925     دویست  0.000003  1.494572e-02\n",
      "\n",
      "[65 rows x 3 columns]\n",
      "گوست -> گوشت : contextual\n",
      "{'raw': 'گوست', 'corrected': 'گوشت', 'span': '[10, 14]', 'around': 'وقتی قیمت گوشت قرمز یا س', 'type': 'contextual'}\n",
      "**************************************************\n",
      "Token: قرمز\n",
      "**************************************************\n",
      "Token: یا\n",
      "**************************************************\n",
      "Token: سفید\n",
      "**************************************************\n",
      "Token: در\n",
      "**************************************************\n",
      "Token: کشورهای\n",
      "**************************************************\n",
      "Token: دیگر\n",
      "**************************************************\n",
      "Token: بیشتر\n",
      "**************************************************\n",
      "Token: شده\n",
      "**************************************************\n",
      "Token: است\n",
      "**************************************************\n",
      "Token: ،\n",
      "**************************************************\n",
      "Token: ممکن\n",
      "**************************************************\n",
      "Token: است\n",
      "**************************************************\n",
      "Token: در\n",
      "**************************************************\n",
      "Token: ایران\n",
      "**************************************************\n",
      "Token: هم\n",
      "**************************************************\n",
      "Token: گرا\n",
      "Filtered Predicts: \n",
      "\n",
      "     token_str         score   total_score\n",
      "17        گران  3.867468e-03  7.415911e+02\n",
      "280        گرم  8.581560e-05  1.645523e+01\n",
      "2099       گرد  2.973506e-06  5.701729e-01\n",
      "2604       گره  2.067266e-06  3.964004e-01\n",
      "2875       قرا  1.734685e-06  3.326278e-01\n",
      "...        ...           ...           ...\n",
      "4808       مرت  7.140935e-07  7.140935e-07\n",
      "4839       پرک  7.074425e-07  7.074425e-07\n",
      "4894       سلا  6.966825e-07  6.966825e-07\n",
      "4932       گنگ  6.871654e-07  6.871654e-07\n",
      "4972      شگرد  6.799458e-07  6.799458e-07\n",
      "\n",
      "[200 rows x 3 columns]\n",
      "گرا -> گران : contextual\n",
      "{'raw': 'گرا', 'corrected': 'گران', 'span': '[80, 83]', 'around': ' ایران هم گران شود.', 'type': 'contextual'}\n",
      "**************************************************\n",
      "Token: شود\n",
      "**************************************************\n",
      "Token: .\n",
      "False\n",
      "وقتی قیمت گوشت قرمز یا سفید در کشورهای دیگر بیشتر شده است، ممکن است در ایران هم گران شود.\n",
      "\n",
      "\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      " * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\n",
      "\n",
      "\n",
      "DIGIT\n",
      "**************************************************\n",
      "Token: 1850\n",
      "Filtered Predicts: \n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filtered_predicts' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [108]\u001B[0m, in \u001B[0;36m<cell line: 116>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    117\u001B[0m test_case \u001B[38;5;241m=\u001B[39m test_cases[idx]\n\u001B[1;32m    119\u001B[0m input_text \u001B[38;5;241m=\u001B[39m test_case[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m--> 121\u001B[0m output_text \u001B[38;5;241m=\u001B[39m \u001B[43mspell_corrector\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28mprint\u001B[39m(output_text \u001B[38;5;241m==\u001B[39m test_case[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrue_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28mprint\u001B[39m(output_text)\n",
      "Input \u001B[0;32mIn [104]\u001B[0m, in \u001B[0;36mSpellCorrector.__call__\u001B[0;34m(self, text, *args, **kwargs)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, text, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorrection_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [104]\u001B[0m, in \u001B[0;36mSpellCorrector.correction_pipeline\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcorrection_pipeline\u001B[39m(\u001B[38;5;28mself\u001B[39m, text):\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# print(\"Lexico Correction ...\") if self.verbose else print()\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m     corrected_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lexico_typo_correction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# print(\"Contextual Correction ...\") if self.verbose else print()\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     corrected_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_contextual_typo_correction(corrected_text)\n",
      "Input \u001B[0;32mIn [104]\u001B[0m, in \u001B[0;36mSpellCorrector._lexico_typo_correction\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_lexico_typo_correction\u001B[39m(\u001B[38;5;28mself\u001B[39m, text):\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlexico_typo_correction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_edit_distance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtop_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [102]\u001B[0m, in \u001B[0;36mlexico_typo_correction\u001B[0;34m(text, alpha, max_edit_distance, top_k, verbose)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToken: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_token\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFiltered Predicts: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mfiltered_predicts\u001B[49m[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_str\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtotal_score\u001B[39m\u001B[38;5;124m'\u001B[39m]])\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_token\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m -> \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mselected_predict\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m : lexical\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m some_token_corrected:\n",
      "\u001B[0;31mUnboundLocalError\u001B[0m: local variable 'filtered_predicts' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if language == 'en':\n",
    "    test_cases = [\n",
    "        {\"input_text\": \"\"\"\n",
    "            The quantity thoery of money also assume that the quantity of money in an economy has a large influense on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the sopply of gods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\n",
    "            The quantity theory of money also assumes that the quantity of money in an economy has a large influence on its level of economic activity. So, a change in the money supply results in either a change in the price levels or a change in the supply of goods and services, or both. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.\n",
    "        \"\"\"},\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "            Does it privent Iran from getting nuclear weapens. Many exports say that if all parties adhered to their pledges, the deal almost certainly could have achieved that goal for longer than a dekade!\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\n",
    "            Does it prevent Iran from getting nuclear weapons? Many experts say that if all parties adhere to their pledges, the deal almost certainly could have achieved that goal for longer than a decade.\n",
    "        \"\"\"},\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "            The Federal Reserve monitor risks to the financal system and works to help insure the system supports a haelthy economy for US households, communities, and busineses.\n",
    "        \"\"\",\n",
    "\n",
    "         \"true_text\": \"\"\"\n",
    "            The Federal Reserve monitors risks to the financial system and works to help ensure the system supports a healthy economy for US households, communities, and businesses.\n",
    "        \"\"\"},\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "            Bitcoin is a decentrallized digital curency that can be transfered on the peer-to-peer bitcoin network. Bitcoin transactions are veryfied by network nodes throgh cryptography and recorded in a public distributed ledger called a blockchain. The criptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The curency began use in 2009 when its implemntation was released as open-source software.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\n",
    "            Bitcoin is a decentralized digital currency that can be transferred on the peer-to-peer bitcoin network. Bitcoin transactions are verified by network nodes through cryptography and recorded in a public distributed ledger called a blockchain. The cryptocurrency was invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.\n",
    "        \"\"\"},\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "            The 2022 FILA World Cup is scheduled to be the 22nd running of the FILA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 Novamber to 18 Decamber 2022.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\n",
    "            The 2022 FIFA World Cup is scheduled to be the 22nd running of the FIFA World Cup competition, the quadrennial international men's football championship contested by the national teams of the member associations of FIFA. It is scheduled to take place in Qatar from 21 November to 18 December 2022.\n",
    "        \"\"\"},\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "            President Daneld Trump annonced on Tuesday he well withdraw the United States from the Iran nuclear deal and restore far-reaching sanktions aimed at withdrawal Iran from the global finansial system.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\n",
    "            President Donald Trump announced on Tuesday he will withdraw the United States from the Iran nuclear deal and restore far-reaching sanctions aimed at withdrawal Iran from the global financial system.\n",
    "        \"\"\"},\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "            Cars has very sweet features. It has two beautifull eye, adorable tiny paws, sharp claws, and two fury ear which are very sensitive to sounds. It has a tiny body covered with sot fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\n",
    "            Cat has very sweet features. It has two beautiful eyes, adorable tiny paws with sharp claws, and two furry ears which are very sensitive to sounds. It has a tiny body covered with soft fur and it has a furry tail as well. Cats have an adorable face with a tiny nose.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    if model_type != 'roberta':\n",
    "        for test_case in test_cases:\n",
    "            test_case['input_text'] = test_case['input_text'].lower()\n",
    "            test_case['true_text'] = test_case['true_text'].lower()\n",
    "\n",
    "elif language == 'fa':\n",
    "    test_cases = [\n",
    "        # {\n",
    "        #     \"input_text\": \"\"\"\n",
    "        #\n",
    "        # \"\"\",\n",
    "        #     \"true_text\": \"\"\"\n",
    "        #\n",
    "        #  \"\"\"\n",
    "        # },\n",
    "        {\"input_text\": \"پس از سال‌ها تلاش، رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\",\n",
    "         \"true_text\": \"\"\"\"\"\"\n",
    "         },\n",
    "        {\"input_text\": \"وقتی قیمت گوست قرمز یا صفید در کشورهای دیگر بیشتر شده است، ممکن است در جیران هم گرا شود.\",\n",
    "         \"true_text\": \"\"\"\"\"\"\n",
    "         },\n",
    "        {\"input_text\": \"در هفته گذشته قیمت تلا تغییر چندانی نداشت، و در همان محدوده 1850 دلاری کار خود را به پایان رساند. \",\n",
    "         \"true_text\": \"\"\"\"\"\"\n",
    "         },\n",
    "        {\n",
    "            \"input_text\": \"بر اساس مسوبه سران قوا، معاملات فردایی طلا همانند معاملات فردایی ارض، ممنوع و غیرقانونی شناخته شد و فعالان این بازار به جرم اخلال اقتصادی، تحت پیگرد قرار خواهند گرفت. در نتیجه تانک مرکزی در بازار فردایی مداخله نخواهد کرد\",\n",
    "            \"true_text\": \"\"\"\"\"\"\n",
    "        },\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "        با نزدیک شدن قیمت دار غیر رسمی به سفف خود در روز قبل، تحلیلگران در بازار برای هفته بعد هشدار میدادند که باید احطیاط کرد و اقدامات امنیتی در بازار افزایش خواهد یافت.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\"\"\"\n",
    "         },\n",
    "\n",
    "        {\"input_text\": \"\"\"\n",
    "        با تولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تأمین کندم جهان داشتند، بازار کندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان کندم بود، در همه جای جهان افزایش یافت.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\"\"\"\n",
    "         },\n",
    "        {\"input_text\": \"\"\"\n",
    "        علت واقعی تعویق در مزاکرات وین چیست.\n",
    "        \"\"\",\n",
    "         \"true_text\": \"\"\"\"\"\"\n",
    "         },\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    raise f\"{language} language not found.\"\n",
    "\n",
    "ALPHA = 8 if language == 'en' else 30\n",
    "MAX_EDIT_DISTANCE = 2 if language == 'en' else 2\n",
    "TOP_K = 250 if language == 'en' else 5000\n",
    "VERBOSE = True\n",
    "\n",
    "for test_case in test_cases:\n",
    "    test_case['input_text'] = test_case['input_text'].strip()\n",
    "    test_case['true_text'] = test_case['true_text'].strip()\n",
    "\n",
    "spell_corrector = SpellCorrector(ALPHA, MAX_EDIT_DISTANCE, VERBOSE, TOP_K)\n",
    "from spacy import displacy\n",
    "\n",
    "for idx in range(len(test_cases)):\n",
    "    test_case = test_cases[idx]\n",
    "\n",
    "    input_text = test_case['input_text']\n",
    "\n",
    "    output_text = spell_corrector(input_text)\n",
    "\n",
    "    print(output_text == test_case['true_text'])\n",
    "\n",
    "    print(output_text)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* \" * 50)\n",
    "    print(\" *\" * 50)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"الکل\" in vocab"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}