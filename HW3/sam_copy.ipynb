{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Required Libraries\n",
    "In this project, we use **transformers** library (from **huggingface.co**) to use the pre-trained **BERT** base model. We use BERT and RoBERTa models for English and BERT and ALBERT models for Persian."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM, AlbertTokenizer, AlbertForMaskedLM, RobertaTokenizer, RobertaModel\n",
    "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import stanza\n",
    "import spacy\n",
    "import spacy_stanza"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models\n",
    "\n",
    "The following is a brief description of these Transformer models and their differences and similarities with the base bert model:\n",
    "\n",
    "1. ALBERT: As stated earlier, BERT base consists of 110 million parameters which makes it computationally intensive and therefore a light version was required with reduced parameters. ALBERT model has 12 million parameters with 768 hidden layers and 128 embedding layers. As expected, the lighter model reduced the training time and inference time. To achieve lesser set of parameters, the **Cross-layer parameter sharing** & **Factorized embedding layer parameterization** techniques are used.\n",
    "\n",
    "2. RoBERTa stands for “Robustly Optimized BERT pre-training Approach”. In many ways this is a better version of the BERT model. The key points of difference are as follows:\n",
    "\n",
    "    - **Dynamic Masking**: BERT uses static masking i.e. the same part of the sentence is masked in each Epoch. In contrast, RoBERTa uses dynamic masking, wherein for different Epochs different part of the sentences are masked. This makes the model more robust.\n",
    "\n",
    "    - **Remove NSP Task**: It was observed that the NSP task is not very useful for pre-training the BERT model. Therefore, the RoBERTa only with the MLM task.\n",
    "\n",
    "    - **More data Points**: BERT is pre-trained on “Toronto BookCorpus” and “English Wikipedia datasets” i.e. as a total of 16 GB of data. In contrast, in addition to these two datasets, RoBERTa was also trained on other datasets like CC-News (Common Crawl-News), Open WebText etc. The total size of these datasets is around 160 GB.\n",
    "\n",
    "    - **Large Batch size**: To improve on the speed and performance of the model, RoBERTa used a batch size of 8,000 with 300,000 steps. In comparison, BERT uses a batch size of 256 with 1 million steps.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cpu\n",
      "fa bert Model Loaded ...\n"
     ]
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vocabulary\n",
    "\n",
    "We use transformer model vocabulary to identify possible typos (misspelling). In this way, if a word is not in the vocabulary, it **probably** has a misspelling. In the next step, this typo is corrected with the help of a pre-trained model predictions and lexical distance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stanza\n",
    "\n",
    "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stanza\n",
    "\n",
    "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
    "\n",
    "## Setup\n",
    "\n",
    "We use **Stanza** library for Persian and **Spacy** base model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if language == 'fa':\n",
    "    stanza.install_corenlp()\n",
    "    stanza.download('fa')\n",
    "    nlp = spacy_stanza.load_pipeline(\"fa\")\n",
    "\n",
    "elif language == 'en':\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Stanza: {language} not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Persian texts, a semicolon plays a key role. Unfortunately, pre-trained models in Persian do not support half space and their predicted words do not have half space.\n",
    "With this function, if the difference between the predicted words and the main word in the given input is only contains half-space, we do not change the main word in the given input."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Correct Lexico Typo"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spell Correction\n",
    "\n",
    "## Correct Lexico Typo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Correct Contextual Typo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def contextual_typo_correction(\n",
    "        text,\n",
    "        alpha=10,\n",
    "        max_edit_distance=2,\n",
    "        top_k=10,\n",
    "        verbose=False,\n",
    "):\n",
    "    doc = nlp(text)\n",
    "    for index in range(len(doc)):\n",
    "\n",
    "        current_token: Token = doc[index]\n",
    "\n",
    "        print(\"*\" * 50)\n",
    "        print(f\"Token: {current_token.text}\")\n",
    "\n",
    "        start_char_index = current_token.idx\n",
    "        end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "        masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "        predicts = unmasker(masked_text, top_k=top_k)\n",
    "        ### Select Token From Predicts\n",
    "        predicts = pd.DataFrame(predicts)\n",
    "\n",
    "        try:\n",
    "            if current_token.text in string.punctuation:\n",
    "                filtered_predicts = predicts.loc[predicts['token_str'].apply(lambda tk: tk in string.punctuation), :].copy()\n",
    "                selected_predict = filtered_predicts['token_str'].iloc[0]\n",
    "\n",
    "            elif any(c.isdigit() for c in current_token.text):\n",
    "                selected_predict = current_token.text\n",
    "\n",
    "            else:\n",
    "                predicts.loc[:, 'token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "                predicts.loc[:, 'edit_distance'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk))\n",
    "\n",
    "                # Filter tokens with at most 3 edit distance\n",
    "                filtered_predicts = predicts.loc[predicts['edit_distance'] <= max_edit_distance, :].copy()\n",
    "\n",
    "                # Apply total score function\n",
    "                # e: edit distance + 1\n",
    "                # l: token length\n",
    "                filtered_predicts.loc[:, 'e_to_l'] = (filtered_predicts.loc[:, 'edit_distance'] + 1) / len(current_token.text)\n",
    "\n",
    "                filtered_predicts.loc[:, 'total_score'] = filtered_predicts.loc[:, 'score'] / filtered_predicts.loc[:, 'e_to_l'] ** alpha\n",
    "\n",
    "                filtered_predicts = filtered_predicts.sort_values('total_score', ascending=False)\n",
    "                selected_predict_row = filtered_predicts.iloc[0, :]\n",
    "\n",
    "                selected_predict = selected_predict_row['token_str']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e} From {current_token.text} Filtered Predictions Length: {len(filtered_predicts)}\")\n",
    "            selected_predict = current_token.text\n",
    "\n",
    "        if selected_predict != current_token.text:\n",
    "            if not half_space_case(selected_predict, current_token.text):\n",
    "                text = masked_text.replace(MASK, selected_predict, 1)\n",
    "                doc = nlp(text)\n",
    "\n",
    "            else:\n",
    "                vocab.add(current_token.text)\n",
    "                selected_predict = current_token.text\n",
    "\n",
    "        if verbose:\n",
    "            if current_token.text != selected_predict:\n",
    "                print(\"Filtered Predicts: \\n\")\n",
    "                print(filtered_predicts[['token_str', 'score', 'total_score']])\n",
    "\n",
    "                print(f\"{current_token.text} -> {selected_predict} : contextual\")\n",
    "\n",
    "                typo_correction_details = {\n",
    "                    \"raw\": current_token.text,\n",
    "                    \"corrected\": selected_predict,\n",
    "                    \"span\": f\"[{start_char_index}, {end_char_index}]\",\n",
    "                    \"around\": text[start_char_index - 10: end_char_index + 10],\n",
    "                    \"type\": \"contextual\"\n",
    "                }\n",
    "\n",
    "                print(typo_correction_details)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Correction Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=5,\n",
    "            max_edit_distance=2,\n",
    "            verbose=False,\n",
    "            top_k=50\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def _lexico_typo_correction(self, text):\n",
    "        return lexico_typo_correction(text, self.alpha, self.max_edit_distance, self.top_k, self.verbose, )\n",
    "\n",
    "    def _contextual_typo_correction(self, text):\n",
    "        return contextual_typo_correction(text, self.alpha, self.max_edit_distance, self.top_k, self.verbose, )\n",
    "\n",
    "    def correction_pipeline(self, text):\n",
    "        # print(\"Lexico Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._lexico_typo_correction(text)\n",
    "\n",
    "        # print(\"Contextual Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._contextual_typo_correction(corrected_text)\n",
    "\n",
    "        return corrected_text\n",
    "\n",
    "    def __call__(self, text, *args, **kwargs):\n",
    "        return self.correction_pipeline(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sample Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sample Texts\n",
    "\n",
    "In this section, X is executed on the given sample texts and the output is compared with the corrected input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"الکل\" in vocab"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}