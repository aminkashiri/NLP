{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Required Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from transformers import pipeline, BertTokenizer, BertForMaskedLM, AlbertTokenizer, AlbertForMaskedLM\n",
    "from transformers.pipelines.fill_mask import FillMaskPipeline\n",
    "import torch\n",
    "\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cpu\n",
      "fa Model Loaded ...\n"
     ]
    }
   ],
   "source": [
    "# torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_device = 'cpu'\n",
    "print(f\"Torch Device: {torch_device}\")\n",
    "\n",
    "language = 'fa'\n",
    "\n",
    "# EN\n",
    "# model_name = \"bert-large-uncased\"\n",
    "# model_name = \"bert-base-uncased\"\n",
    "\n",
    "# FA\n",
    "model_type = 'bert'\n",
    "\n",
    "# model_name = \"HooshvareLab/albert-fa-zwnj-base-v2\" # Albert\n",
    "# model_name = \"HooshvareLab/bert-fa-base-uncased\" # BERT V2\n",
    "model_name = \"HooshvareLab/bert-fa-zwnj-base\" # BERT V3\n",
    "\n",
    "if model_type == 'bert':\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForMaskedLM.from_pretrained(model_name).to(torch_device) if language == 'en' else BertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "elif model_type == 'albert':\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "    model = AlbertForMaskedLM.from_pretrained(model_name).to(torch_device) if language == 'en' else AlbertForMaskedLM.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "\n",
    "print(f\"{language} Model Loaded ...\")\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.09131668508052826,\n  'token': 3111,\n  'token_str': 'پ ذ ی ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] پذیر نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.03739292919635773,\n  'token': 4219,\n  'token_str': 'د س ت ر س',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] دسترس نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.03577372059226036,\n  'token': 35045,\n  'token_str': '# # ش ن ا',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK]شنا نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.023947319015860558,\n  'token': 3320,\n  'token_str': 'ط ب ی ع ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] طبیعی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.020062608644366264,\n  'token': 3691,\n  'token_str': 'ع ل م ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] علمی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.01819879747927189,\n  'token': 3109,\n  'token_str': 'س ا د ه',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] ساده نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.015504238195717335,\n  'token': 4732,\n  'token_str': 'پ ی چ ی د ه',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] پیچیده نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.01519469078630209,\n  'token': 2534,\n  'token_str': 'م م ک ن',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] ممکن نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.013875356875360012,\n  'token': 4775,\n  'token_str': 'ک ا ر ب ر د ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] کاربردی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.0132302762940526,\n  'token': 3566,\n  'token_str': 'ج ذ ا ب',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] جذاب نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.012931299395859241,\n  'token': 12311,\n  'token_str': 'ن ش د ن ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] نشدنی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.011404243297874928,\n  'token': 2586,\n  'token_str': 'م ش خ ص',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مشخص نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.011242495849728584,\n  'token': 4002,\n  'token_str': 'م ر ت ب ط',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مرتبط نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.011139336973428726,\n  'token': 1983,\n  'token_str': 'ش د ه',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] شده نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.009800798259675503,\n  'token': 6358,\n  'token_str': 'ی ک س ا ن',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] یکسان نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.009680557064712048,\n  'token': 3406,\n  'token_str': 'م ت ف ا و ت',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] متفاوت نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.009136738255620003,\n  'token': 5933,\n  'token_str': 'م ن ط ق ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] منطقی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.008639754727482796,\n  'token': 5172,\n  'token_str': 'ق ب و ل',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] قبول نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.008031144738197327,\n  'token': 6990,\n  'token_str': 'س ا ز گ ا ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] سازگار نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.007924414239823818,\n  'token': 3195,\n  'token_str': 'ع م ل ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] عملی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.006895949598401785,\n  'token': 3020,\n  'token_str': 'د ر س ت',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] درست نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.006338624749332666,\n  'token': 3376,\n  'token_str': 'ج د ی د ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] جدیدی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.006105430424213409,\n  'token': 6540,\n  'token_str': 'ق ط ع ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] قطعی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.006078369449824095,\n  'token': 5470,\n  'token_str': 'ص ح ی ح',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] صحیح نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.005998319946229458,\n  'token': 5363,\n  'token_str': 'ف ه م',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] فهم نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.00593611178919673,\n  'token': 5580,\n  'token_str': 'م ع ن ا',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] معنا نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.005896000191569328,\n  'token': 5365,\n  'token_str': 'ر ا ی ج',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] رایج نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.005701115820556879,\n  'token': 6910,\n  'token_str': 'پ ذ ی ر ف ت ه',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] پذیرفته نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.005584299564361572,\n  'token': 4167,\n  'token_str': 'ی ا د گ ی ر ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] یادگیری نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.005481337197124958,\n  'token': 8027,\n  'token_str': 'ف ر ا گ ی ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] فراگیر نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.005452076438814402,\n  'token': 3654,\n  'token_str': 'م ش ت ر ک',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مشترک نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.005406579468399286,\n  'token': 2162,\n  'token_str': 'ج د ی د',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] جدید نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.004711121786385775,\n  'token': 4276,\n  'token_str': 'ث ا ب ت',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] ثابت نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.00470323208719492,\n  'token': 3645,\n  'token_str': 'م ج ا ز ی',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مجازی نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.00465690391138196,\n  'token': 2458,\n  'token_str': 'م ه م',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مهم نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.004619051236659288,\n  'token': 2694,\n  'token_str': 'ا ر ت ب ا ط',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] ارتباط نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.004487005993723869,\n  'token': 2376,\n  'token_str': 'ه م ر ا ه',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] همراه نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.004273892380297184,\n  'token': 9402,\n  'token_str': 'م ر س و م',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مرسوم نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.00422415416687727,\n  'token': 4376,\n  'token_str': 'م ف ی د',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مفید نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.00412861630320549,\n  'token': 2403,\n  'token_str': 'ت غ ی ی ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] تغییر نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.00408933823928237,\n  'token': 5039,\n  'token_str': 'ع ج ی ب',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] عجیب نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.003888400038704276,\n  'token': 2979,\n  'token_str': 'ک ا م ل',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] کامل نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.0038855455350130796,\n  'token': 5258,\n  'token_str': 'م ف ه و م',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] مفهوم نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.003637738060206175,\n  'token': 3763,\n  'token_str': 'ج ا ل ب',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] جالب نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.0034867224749177694,\n  'token': 1979,\n  'token_str': 'ک ا ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] کار نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.003477877238765359,\n  'token': 8092,\n  'token_str': 'ن ا پ ذ ی ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] ناپذیر نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.003452395088970661,\n  'token': 9531,\n  'token_str': 'م ی س ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] میسر نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.0033659334294497967,\n  'token': 4208,\n  'token_str': 'ا ش ت ب ا ه',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] اشتباه نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.0033494310919195414,\n  'token': 1934,\n  'token_str': 'ب ر',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] بر نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'},\n {'score': 0.0033451139461249113,\n  'token': 4067,\n  'token_str': 'ر ا ح ت',\n  'sequence': '[CLS] بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [MASK] راحت نیست و برای یادگیری باید به فلسفههای خاصی [MASK] کرد. [SEP]'}]"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_text = \"The capital of Iran is [MASK].\"\n",
    "# input_text = \"Yesterday, I played [MASK] in the [MASK].\"\n",
    "# input_text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "# input_text = \"I'm a very [MASK] player in football.\"\n",
    "# input_text = \"He drived a [MASK].\"\n",
    "# input_text = \"I love playing [MASK].\"\n",
    "# input_text = \"He was [MASK] english for the tomorrow [MASK].\"\n",
    "\n",
    "# input_text = \"امروز در استادیوم آزادی [ماسک] ملی ایران و روسیه مسایقه می‌دهند.\"\n",
    "# input_text = \"پس از سال‌ها تلاش رازی موفق به [ماسک] الکل شد. این دانشمند [ماسک] باعث افتخار در تاریخ [ماسک] است.\"\n",
    "input_text = \"بسیاری از مباحث علوم غیر طبیعی با استفاده از فیزیک دنیای مادی [ماسک] [ماسک] نیست و برای یادگیری باید به فلسفه‌های خاصی [ماسک] کرد.\"\n",
    "\n",
    "input_text = input_text.replace(\"[ماسک]\", \"[MASK]\")\n",
    "\n",
    "result = unmasker(input_text, top_k=50)\n",
    "result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Stanza\n",
    "\n",
    "spaCy's tokenization is non-destructive, so it always represents the original input text and never adds or deletes anything. This is kind of a core principle of the Doc object: you should always be able to reconstruct and reproduce the original input text.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import spacy_stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 03:37:07 WARNING: Directory /home/ahur4/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2613b50aab243ac86969e32f0baf7df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 03:37:07 INFO: Downloading default packages for language: fa (Persian)...\n",
      "2022-06-05 03:37:08 INFO: File exists: /home/ahur4/stanza_resources/fa/default.zip\n",
      "2022-06-05 03:37:10 INFO: Finished downloading models and saved to /home/ahur4/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd28514108e9454b814a14c2730c4bed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 03:37:12 INFO: Loading these models for language: fa (Persian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | perdt   |\n",
      "| mwt       | perdt   |\n",
      "| pos       | perdt   |\n",
      "| lemma     | perdt   |\n",
      "| depparse  | perdt   |\n",
      "| ner       | arman   |\n",
      "=======================\n",
      "\n",
      "2022-06-05 03:37:12 INFO: Use device: cpu\n",
      "2022-06-05 03:37:12 INFO: Loading: tokenize\n",
      "2022-06-05 03:37:12 INFO: Loading: mwt\n",
      "2022-06-05 03:37:12 INFO: Loading: pos\n",
      "2022-06-05 03:37:12 INFO: Loading: lemma\n",
      "2022-06-05 03:37:12 INFO: Loading: depparse\n",
      "2022-06-05 03:37:12 INFO: Loading: ner\n",
      "2022-06-05 03:37:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "if language == 'fa':\n",
    "    stanza.install_corenlp()\n",
    "    stanza.download('fa')\n",
    "    nlp = spacy_stanza.load_pipeline(\"fa\")\n",
    "\n",
    "elif language == 'en':\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"{language} not supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "vocab:set = set(tokenizer.get_vocab().keys())\n",
    "MASK:str = \"[MASK]\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correct Lexico Typo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "def half_space_case(current_text:str, predicted_text:str):\n",
    "                if '‌' in current_text:\n",
    "                    current_text_wo_half_space = current_text.replace('‌', '')\n",
    "                    return current_text_wo_half_space == predicted_text\n",
    "\n",
    "def lexico_typo_correction(\n",
    "        text,\n",
    "        max_edit_distance_to_length_ratio = 0.45,\n",
    "        max_edit_distance = 2,\n",
    "        min_score = 1e-7,\n",
    "        top_k = 10,\n",
    "        verbose=False,\n",
    "):\n",
    "\n",
    "    while True:\n",
    "        some_token_corrected = False\n",
    "        doc = nlp(text)\n",
    "        for index, current_token in enumerate(doc):\n",
    "            current_token: Token\n",
    "            start_char_index: int = current_token.idx\n",
    "            end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "            if current_token.text not in vocab:\n",
    "                if verbose:\n",
    "                    print(\"*\" * 50)\n",
    "                    print(f\"[{current_token.text}] is not in vocab\")\n",
    "\n",
    "                masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "                predicts = unmasker(masked_text, top_k=top_k)\n",
    "\n",
    "                ### Select Token From Predicts\n",
    "                predicts = pd.DataFrame(predicts)\n",
    "\n",
    "                predicts['token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "                predicts['edit_distance'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk))\n",
    "                predicts['edit_distance_to_len_ratio'] = predicts['edit_distance'] / len(current_token.text)\n",
    "\n",
    "                selected_predicts = predicts[(predicts['edit_distance_to_len_ratio'] <= max_edit_distance_to_length_ratio) & (predicts['edit_distance'] <= max_edit_distance) & (predicts['score'] >= min_score)]\n",
    "\n",
    "                try:\n",
    "                    selected_predict = selected_predicts['token_str'].iloc[0]\n",
    "                except:\n",
    "                    selected_predict = current_token.text\n",
    "\n",
    "                if selected_predict != current_token.text and not half_space_case(current_token.text, selected_predict):\n",
    "                    some_token_corrected = True\n",
    "                    result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "                    text = result_text\n",
    "\n",
    "                if verbose :\n",
    "                    print(\"*\" * 50)\n",
    "                    print(f\"Token: {current_token.text}\")\n",
    "\n",
    "                    print(\"Predicts: \\n\")\n",
    "                    print(predicts[['token_str', 'score']])\n",
    "\n",
    "                    print(\"Filtered Predicts: \\n\")\n",
    "                    print(selected_predicts[['token_str', 'score']])\n",
    "                    print(f\"{current_token.text} -> {selected_predict}\")\n",
    "\n",
    "                    print(\"SOME TOKEN CORRECTED!!!\") \\\n",
    "\n",
    "                if some_token_corrected:\n",
    "                    break\n",
    "\n",
    "        if not some_token_corrected:\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Result text: \\n{text}\")\n",
    "\n",
    "    return text\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correct Contextual Typo\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def contextual_typo_correction(\n",
    "        text,\n",
    "        max_edit_distance_to_length_ratio = 0.45,\n",
    "        max_edit_distance = 2,\n",
    "        min_score = 1e-7,\n",
    "        top_k = 10,\n",
    "        verbose=False,\n",
    "):\n",
    "\n",
    "    while True:\n",
    "        some_token_corrected = False\n",
    "        doc = nlp(text)\n",
    "        for index in range(len(doc)):\n",
    "            current_token: Token = doc[index]\n",
    "\n",
    "            start_char_index = current_token.idx\n",
    "            end_char_index = start_char_index + len(current_token)\n",
    "\n",
    "            masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "\n",
    "            predicts = unmasker(masked_text, top_k=top_k)\n",
    "\n",
    "            ### Select Token From Predicts\n",
    "            predicts = pd.DataFrame(predicts)\n",
    "\n",
    "            predicts['token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "            predicts['edit_distance'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk))\n",
    "            predicts['edit_distance_to_len_ratio'] = predicts['edit_distance'] / len(current_token.text)\n",
    "\n",
    "            selected_predicts = predicts[(predicts['edit_distance_to_len_ratio'] <= max_edit_distance_to_length_ratio) & (predicts['edit_distance'] <= max_edit_distance) & (predicts['score'] >= min_score)]\n",
    "\n",
    "            try:\n",
    "                if current_token.text in string.punctuation:\n",
    "                    selected_predict = selected_predicts['token_str'].iloc[0]\n",
    "\n",
    "                elif current_token.text.isdigit():\n",
    "                    selected_predict = current_token.text\n",
    "\n",
    "                elif current_token.text in selected_predicts['token_str'].values:\n",
    "                    selected_predict = current_token.text\n",
    "\n",
    "                else:\n",
    "                    selected_predict = selected_predicts.sort_values('edit_distance_to_len_ratio')['token_str'].iloc[0]\n",
    "\n",
    "            except:\n",
    "                selected_predict = current_token.text\n",
    "\n",
    "            if selected_predict != current_token.text and not half_space_case(current_token.text, selected_predict):\n",
    "                some_token_corrected = True\n",
    "                result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "                text = result_text\n",
    "\n",
    "            if verbose :\n",
    "                print(\"*\" * 50)\n",
    "                print(f\"Token: {current_token.text}\")\n",
    "\n",
    "                print(\"Predicts: \\n\")\n",
    "                print(predicts[['token_str', 'score']])\n",
    "\n",
    "                print(\"Filtered Predicts: \\n\")\n",
    "                print(selected_predicts[['token_str', 'score']])\n",
    "                print(f\"{current_token.text} -> {selected_predict}\")\n",
    "\n",
    "                print(\"SOME TOKEN CORRECTED!!!\")\n",
    "\n",
    "            if some_token_corrected:\n",
    "                break\n",
    "\n",
    "\n",
    "        if not some_token_corrected:\n",
    "            break\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Result text: \\n{text}\")\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correction Pipeline Class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_edit_distance_to_length_ratio = 0.45,\n",
    "            max_edit_distance = 2,\n",
    "            min_score = 1e-7,\n",
    "            verbose = False,\n",
    "            top_k = 50\n",
    "    ):\n",
    "        self.max_edit_distance_to_length_ratio = max_edit_distance_to_length_ratio\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.min_score = min_score\n",
    "        self.verbose = verbose\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def _lexico_typo_correction(self, text):\n",
    "        return lexico_typo_correction(text, self.max_edit_distance_to_length_ratio, self.max_edit_distance, self.min_score, self.top_k, self.verbose, )\n",
    "\n",
    "    def _contextual_typo_correction(self, text):\n",
    "        return contextual_typo_correction(text, self.max_edit_distance_to_length_ratio, self.max_edit_distance, self.min_score, self.top_k, self.verbose,)\n",
    "\n",
    "    def correction_pipeline(self, text):\n",
    "        print(\"Lexico Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._lexico_typo_correction(text)\n",
    "\n",
    "        print(\"Contextual Correction ...\") if self.verbose else print()\n",
    "        corrected_text = self._contextual_typo_correction(corrected_text)\n",
    "        return corrected_text\n",
    "\n",
    "    def __call__(self, text, *args, **kwargs):\n",
    "        return self.correction_pipeline(text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test On Sample Texts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized text: با تولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تأمین کندم جهان داشتند، بازار کندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان کندم بود، در همه جای جهان افزایش یافت.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fa\" id=\"ad91d100afa7444a956fec856bbcaf5a-0\" class=\"displacy\" width=\"7400\" height=\"837.0\" direction=\"rtl\" style=\"max-width: none; height: 837.0px; color: #000000; background: #ffffff; font-family: Arial; direction: rtl\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7350\">با</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7350\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7175\">تولانی</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7175\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7000\">شدن</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7000\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6825\">جنگ</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6825\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6650\">روسیه</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6650\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6475\">و</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6475\">CCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6300\">اوکراین</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6300\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6125\">و</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6125\">CCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5950\">سهم</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5950\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5775\">قابل</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5775\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5600\">توجهی</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5600\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5425\">که</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5425\">SCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5250\">این</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5250\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5075\">دو</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5075\">NUM</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4900\">کشور</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4900\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4725\">در</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4725\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4550\">تأمین</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4550\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4375\">کندم</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4375\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4200\">جهان</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4200\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4025\">داشتند،</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4025\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3850\">بازار</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3850\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3675\">کندم</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3675\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3500\">با</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3500\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3325\">نوسانات</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3325\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3150\">زیادی</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3150\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2975\">مواجه</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2975\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2800\">شد</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2800\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2625\">و</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2625\">CCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2450\">قیمت</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2450\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2275\">محصولاتی</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2275\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2100\">که</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2100\">SCONJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1925\">مواد</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1925\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1750\">اولیه‌</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1750\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1575\">شان</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1575\">PRON</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">کندم</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1225\">بود،</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1225\">AUX</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">در</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"875\">همه</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"875\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"700\">جای</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"700\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"525\">جهان</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"525\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">افزایش</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"175\">یافت.</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"175\">VERB</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-0\" stroke-width=\"2px\" d=\"M7330,702.0 C7330,527.0 7030.0,527.0 7030.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M7330,704.0 L7322,692.0 7338,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-1\" stroke-width=\"2px\" d=\"M7155,702.0 C7155,614.5 7035.0,614.5 7035.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M7155,704.0 L7147,692.0 7163,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-2\" stroke-width=\"2px\" d=\"M6980,702.0 C6980,264.5 4040.0,264.5 4040.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M6980,704.0 L6972,692.0 6988,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-3\" stroke-width=\"2px\" d=\"M6980,702.0 C6980,614.5 6860.0,614.5 6860.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M6860.0,704.0 L6868.0,692.0 6852.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-4\" stroke-width=\"2px\" d=\"M6805,702.0 C6805,614.5 6685.0,614.5 6685.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M6685.0,704.0 L6693.0,692.0 6677.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-5\" stroke-width=\"2px\" d=\"M6455,702.0 C6455,614.5 6335.0,614.5 6335.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M6455,704.0 L6447,692.0 6463,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-6\" stroke-width=\"2px\" d=\"M6630,702.0 C6630,527.0 6330.0,527.0 6330.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M6330.0,704.0 L6338.0,692.0 6322.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-7\" stroke-width=\"2px\" d=\"M6105,702.0 C6105,614.5 5985.0,614.5 5985.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M6105,704.0 L6097,692.0 6113,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-8\" stroke-width=\"2px\" d=\"M6980,702.0 C6980,439.5 5975.0,439.5 5975.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M5975.0,704.0 L5983.0,692.0 5967.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-9\" stroke-width=\"2px\" d=\"M5930,702.0 C5930,614.5 5810.0,614.5 5810.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M5810.0,704.0 L5818.0,692.0 5802.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-10\" stroke-width=\"2px\" d=\"M5755,702.0 C5755,614.5 5635.0,614.5 5635.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">obl:arg</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M5635.0,704.0 L5643.0,692.0 5627.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-11\" stroke-width=\"2px\" d=\"M5405,702.0 C5405,439.5 4400.0,439.5 4400.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M5405,704.0 L5397,692.0 5413,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-12\" stroke-width=\"2px\" d=\"M5230,702.0 C5230,527.0 4930.0,527.0 4930.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M5230,704.0 L5222,692.0 5238,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-13\" stroke-width=\"2px\" d=\"M5055,702.0 C5055,614.5 4935.0,614.5 4935.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M5055,704.0 L5047,692.0 5063,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-14\" stroke-width=\"2px\" d=\"M4880,702.0 C4880,527.0 4405.0,527.0 4405.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M4880,704.0 L4872,692.0 4888,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-15\" stroke-width=\"2px\" d=\"M4705,702.0 C4705,614.5 4585.0,614.5 4585.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M4705,704.0 L4697,692.0 4713,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-16\" stroke-width=\"2px\" d=\"M4530,702.0 C4530,614.5 4410.0,614.5 4410.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">obl:arg</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M4530,704.0 L4522,692.0 4538,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-17\" stroke-width=\"2px\" d=\"M5580,702.0 C5580,352.0 4395.0,352.0 4395.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M4395.0,704.0 L4403.0,692.0 4387.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-18\" stroke-width=\"2px\" d=\"M4180,702.0 C4180,614.5 4060.0,614.5 4060.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M4180,704.0 L4172,692.0 4188,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-19\" stroke-width=\"2px\" d=\"M3830,702.0 C3830,614.5 3710.0,614.5 3710.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M3830,704.0 L3822,692.0 3838,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-20\" stroke-width=\"2px\" d=\"M3655,702.0 C3655,439.5 2825.0,439.5 2825.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M3655,704.0 L3647,692.0 3663,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-21\" stroke-width=\"2px\" d=\"M3480,702.0 C3480,614.5 3360.0,614.5 3360.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M3480,704.0 L3472,692.0 3488,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-22\" stroke-width=\"2px\" d=\"M3305,702.0 C3305,527.0 3005.0,527.0 3005.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">obl:arg</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M3305,704.0 L3297,692.0 3313,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-23\" stroke-width=\"2px\" d=\"M3305,702.0 C3305,614.5 3185.0,614.5 3185.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M3185.0,704.0 L3193.0,692.0 3177.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-24\" stroke-width=\"2px\" d=\"M2955,702.0 C2955,614.5 2835.0,614.5 2835.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2955,704.0 L2947,692.0 2963,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-25\" stroke-width=\"2px\" d=\"M4005,702.0 C4005,352.0 2820.0,352.0 2820.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2820.0,704.0 L2828.0,692.0 2812.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-26\" stroke-width=\"2px\" d=\"M2605,702.0 C2605,89.5 180.0,89.5 180.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2605,704.0 L2597,692.0 2613,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-27\" stroke-width=\"2px\" d=\"M2430,702.0 C2430,177.0 185.0,177.0 185.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-27\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2430,704.0 L2422,692.0 2438,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-28\" stroke-width=\"2px\" d=\"M2430,702.0 C2430,614.5 2310.0,614.5 2310.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-28\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2310.0,704.0 L2318.0,692.0 2302.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-29\" stroke-width=\"2px\" d=\"M2080,702.0 C2080,352.0 1420.0,352.0 1420.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-29\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M2080,704.0 L2072,692.0 2088,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-30\" stroke-width=\"2px\" d=\"M1905,702.0 C1905,439.5 1425.0,439.5 1425.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-30\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1905,704.0 L1897,692.0 1913,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-31\" stroke-width=\"2px\" d=\"M1905,702.0 C1905,614.5 1785.0,614.5 1785.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-31\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1785.0,704.0 L1793.0,692.0 1777.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-32\" stroke-width=\"2px\" d=\"M1905,702.0 C1905,527.0 1605.0,527.0 1605.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-32\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1605.0,704.0 L1613.0,692.0 1597.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-33\" stroke-width=\"2px\" d=\"M2255,702.0 C2255,264.5 1415.0,264.5 1415.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-33\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1415.0,704.0 L1423.0,692.0 1407.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-34\" stroke-width=\"2px\" d=\"M1380,702.0 C1380,614.5 1260.0,614.5 1260.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-34\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1260.0,704.0 L1268.0,692.0 1252.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-35\" stroke-width=\"2px\" d=\"M1030,702.0 C1030,527.0 730.0,527.0 730.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-35\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1030,704.0 L1022,692.0 1038,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-36\" stroke-width=\"2px\" d=\"M855,702.0 C855,614.5 735.0,614.5 735.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-36\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M855,704.0 L847,692.0 863,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-37\" stroke-width=\"2px\" d=\"M680,702.0 C680,527.0 205.0,527.0 205.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-37\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M680,704.0 L672,692.0 688,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-38\" stroke-width=\"2px\" d=\"M680,702.0 C680,614.5 560.0,614.5 560.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-38\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M560.0,704.0 L568.0,692.0 552.0,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-39\" stroke-width=\"2px\" d=\"M330,702.0 C330,614.5 210.0,614.5 210.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-39\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">compound:lvc</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M330,704.0 L322,692.0 338,692.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-ad91d100afa7444a956fec856bbcaf5a-0-40\" stroke-width=\"2px\" d=\"M2780,702.0 C2780,2.0 175.0,2.0 175.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-ad91d100afa7444a956fec856bbcaf5a-0-40\" class=\"displacy-label\" startOffset=\"50%\" side=\"right\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M175.0,704.0 L183.0,692.0 167.0,692.0\" fill=\"currentColor\"/>\n</g>\n</svg></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexico Correction ...\n",
      "**************************************************\n",
      "[تولانی] is not in vocab\n",
      "**************************************************\n",
      "Token: تولانی\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        شروع  0.238026\n",
      "1       نزدیک  0.208401\n",
      "2        تمام  0.041988\n",
      "3      طولانی  0.028183\n",
      "4        بزرگ  0.025659\n",
      "..        ...       ...\n",
      "495     اعمال  0.000070\n",
      "496     نایاب  0.000069\n",
      "497     پیاده  0.000069\n",
      "498    سنگینی  0.000069\n",
      "499      ژاپن  0.000069\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "3      طولانی  0.028183\n",
      "188    طوفانی  0.000333\n",
      "192    دورانی  0.000328\n",
      "تولانی -> طولانی\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "[تأمین] is not in vocab\n",
      "**************************************************\n",
      "Token: تأمین\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       بازار  0.744573\n",
      "1    بازارهای  0.058623\n",
      "2         بخش  0.011134\n",
      "3      اقتصاد  0.010307\n",
      "4       برابر  0.008009\n",
      "..        ...       ...\n",
      "495  تکنولوژی  0.000024\n",
      "496     منافع  0.000024\n",
      "497     کنگره  0.000024\n",
      "498      حاضر  0.000024\n",
      "499     اشباع  0.000023\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "25      تامین  0.001739\n",
      "32      تعیین  0.001333\n",
      "75      دومین  0.000414\n",
      "183     سومین  0.000123\n",
      "319      زمین  0.000052\n",
      "350     تکمیل  0.000043\n",
      "420     تدوین  0.000030\n",
      "468      همین  0.000026\n",
      "تأمین -> تامین\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "[کندم] is not in vocab\n",
      "**************************************************\n",
      "Token: کندم\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0     نیازهای  0.164996\n",
      "1        نیاز  0.138716\n",
      "2     محصولات  0.094002\n",
      "3        مواد  0.090410\n",
      "4     کالاهای  0.047300\n",
      "..        ...       ...\n",
      "495  اختراعات  0.000040\n",
      "496    اتانول  0.000040\n",
      "497     توسعه  0.000040\n",
      "498    خواسته  0.000039\n",
      "499    نباتات  0.000039\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "106      گندم  0.000454\n",
      "کندم -> گندم\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "[کندم] is not in vocab\n",
      "**************************************************\n",
      "Token: کندم\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        گندم  0.901410\n",
      "1        برنج  0.020271\n",
      "2        سویا  0.008511\n",
      "3        غلات  0.006879\n",
      "4         ذرت  0.005608\n",
      "..        ...       ...\n",
      "495       صدف  0.000009\n",
      "496     بایدن  0.000009\n",
      "497    دریاها  0.000009\n",
      "498    اینکار  0.000009\n",
      "499    احتکار  0.000009\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str    score\n",
      "0      گندم  0.90141\n",
      "کندم -> گندم\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "[اولیه‌] is not in vocab\n",
      "**************************************************\n",
      "Token: اولیه‌\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       غذایی  0.303957\n",
      "1       اولیه  0.195811\n",
      "2        مغذی  0.170835\n",
      "3       اساسی  0.044010\n",
      "4       معدنی  0.033572\n",
      "..        ...       ...\n",
      "495       عسل  0.000031\n",
      "496     شیلات  0.000031\n",
      "497      ##هن  0.000031\n",
      "498       نیز  0.000031\n",
      "499    پیوندی  0.000031\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "1     اولیه  0.195811\n",
      "اولیه‌ -> اولیه\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "[کندم] is not in vocab\n",
      "**************************************************\n",
      "Token: کندم\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        برنج  0.102665\n",
      "1         خام  0.051764\n",
      "2        گندم  0.036185\n",
      "3        لازم  0.031233\n",
      "4       بیشتر  0.026394\n",
      "..        ...       ...\n",
      "495     بازار  0.000138\n",
      "496      پنبه  0.000138\n",
      "497       سبک  0.000138\n",
      "498   تایوانی  0.000137\n",
      "499     کاهشی  0.000137\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "2      گندم  0.036185\n",
      "کندم -> گندم\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "[اولیه‌] is not in vocab\n",
      "**************************************************\n",
      "Token: اولیه‌\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       اولیه  0.526260\n",
      "1       غذایی  0.140910\n",
      "2        اصلی  0.118762\n",
      "3       اساسی  0.085550\n",
      "4        مغذی  0.025680\n",
      "..        ...       ...\n",
      "495      ##ni  0.000011\n",
      "496      خامی  0.000011\n",
      "497      باید  0.000011\n",
      "498  گرانقیمت  0.000011\n",
      "499       بدی  0.000010\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       اولیه  0.526260\n",
      "283     اولین  0.000027\n",
      "اولیه‌ -> اولیه\n",
      "SOME TOKEN CORRECTED!!!\n",
      "Result text: \n",
      "با طولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تامین گندم جهان داشتند، بازار گندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان گندم بود، در همه جای جهان افزایش یافت.\n",
      "Contextual Correction ...\n",
      "**************************************************\n",
      "Token: با\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0          با  9.740760e-01\n",
      "1          از  5.052485e-03\n",
      "2      بدنبال  3.292569e-03\n",
      "3        درپی  2.304288e-03\n",
      "4      علیرغم  2.206314e-03\n",
      "..        ...           ...\n",
      "495    ##نگام  2.550222e-07\n",
      "496    قربانی  2.531880e-07\n",
      "497   میانگین  2.527767e-07\n",
      "498     برحسب  2.506350e-07\n",
      "499    برنامه  2.506123e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        با  0.974076\n",
      "با -> با\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: طولانی\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       نزدیک  0.251092\n",
      "1        شروع  0.192725\n",
      "2        تمام  0.039439\n",
      "3        بزرگ  0.033977\n",
      "4      طولانی  0.029773\n",
      "..        ...       ...\n",
      "495     خارجی  0.000070\n",
      "496     ملموس  0.000069\n",
      "497      ##دی  0.000069\n",
      "498      بعدی  0.000069\n",
      "499  قهرمانان  0.000069\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "4      طولانی  0.029773\n",
      "165    دورانی  0.000383\n",
      "211     طلایی  0.000270\n",
      "230    طوفانی  0.000238\n",
      "طولانی -> طولانی\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: شدن\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0         شدن  9.311884e-01\n",
      "1        کردن  2.361948e-02\n",
      "2       ماندن  1.529287e-02\n",
      "3        بودن  1.131288e-02\n",
      "4         مدت  4.691172e-03\n",
      "..        ...           ...\n",
      "495     سومین  5.063768e-07\n",
      "496     معلوم  5.032321e-07\n",
      "497    اجباری  5.026700e-07\n",
      "498       گشت  5.016475e-07\n",
      "499    کارزار  5.000560e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0         شدن  9.311884e-01\n",
      "7        نشدن  1.522321e-03\n",
      "24       شدنش  8.898528e-05\n",
      "34        زدن  4.968440e-05\n",
      "78        شده  1.179701e-05\n",
      "93       شدنم  8.812442e-06\n",
      "106        شد  6.670505e-06\n",
      "377      شدند  7.831750e-07\n",
      "شدن -> شدن\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: جنگ\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0         جنگ  0.274375\n",
      "1       روابط  0.183480\n",
      "2     مذاکرات  0.073128\n",
      "3         مرز  0.064042\n",
      "4       رقابت  0.057479\n",
      "..        ...       ...\n",
      "495     ریاست  0.000027\n",
      "496   ترانزیت  0.000027\n",
      "497       برق  0.000027\n",
      "498    ارزهای  0.000027\n",
      "499      رالی  0.000026\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0         جنگ  0.274375\n",
      "310      جنگی  0.000055\n",
      "جنگ -> جنگ\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: روسیه\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       ایران  0.182519\n",
      "1       روسیه  0.141353\n",
      "2         چین  0.090407\n",
      "3        عراق  0.069577\n",
      "4       شوروی  0.065875\n",
      "..        ...       ...\n",
      "495      شرقی  0.000011\n",
      "496     لوزان  0.000011\n",
      "497     المان  0.000010\n",
      "498   خوزستان  0.000010\n",
      "499      غربی  0.000010\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "1       روسیه  0.141353\n",
      "14      سوریه  0.014142\n",
      "68        روس  0.000589\n",
      "293      روسی  0.000025\n",
      "374     روسها  0.000017\n",
      "روسیه -> روسیه\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: و\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0           و  9.262456e-01\n",
      "1           -  5.910785e-02\n",
      "2           ،  4.161936e-03\n",
      "3          با  4.056505e-03\n",
      "4          در  1.860866e-03\n",
      "..        ...           ...\n",
      "495     عمدتا  1.250967e-07\n",
      "496     اخیرا  1.244476e-07\n",
      "497     شدیدی  1.244252e-07\n",
      "498        ۹۸  1.243220e-07\n",
      "499      سقوط  1.242955e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0         و  0.926246\n",
      "و -> و\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: اوکراین\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       روسیه  0.439261\n",
      "1         چین  0.131935\n",
      "2      انگلیس  0.050648\n",
      "3        عراق  0.048506\n",
      "4        ژاپن  0.039750\n",
      "..        ...       ...\n",
      "495      کشور  0.000009\n",
      "496     مرمره  0.000009\n",
      "497  بویراحمد  0.000009\n",
      "498  کمونیستی  0.000009\n",
      "499   پولشویی  0.000009\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "11    اوکراین  0.008904\n",
      "168    اکراین  0.000068\n",
      "264  اوکراینی  0.000027\n",
      "اوکراین -> اوکراین\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: و\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0           و  9.873016e-01\n",
      "1           ،  4.197881e-03\n",
      "2          با  1.957770e-03\n",
      "3          یا  1.042768e-03\n",
      "4        برای  1.000294e-03\n",
      "..        ...           ...\n",
      "495      ۱۳۹۸  1.211412e-07\n",
      "496      2015  1.210430e-07\n",
      "497       الی  1.181586e-07\n",
      "498    ماهانه  1.181100e-07\n",
      "499       چرا  1.170149e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0         و  0.987302\n",
      "و -> و\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: سهم\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0      پیشرفت  0.159161\n",
      "1      موفقیت  0.113762\n",
      "2         رشد  0.099206\n",
      "3         نقش  0.045426\n",
      "4     تغییرات  0.042952\n",
      "..        ...       ...\n",
      "495       حسن  0.000047\n",
      "496       شوق  0.000047\n",
      "497  ایستادگی  0.000046\n",
      "498       مدت  0.000046\n",
      "499   نبردهای  0.000046\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "5         سهم  0.042465\n",
      "228       مهم  0.000182\n",
      "281      سهمی  0.000121\n",
      "سهم -> سهم\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: قابل\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0        قابل  9.941458e-01\n",
      "1       شایان  1.202606e-03\n",
      "2        جالب  1.139303e-03\n",
      "3          بی  7.856247e-04\n",
      "4       درخور  6.386554e-04\n",
      "..        ...           ...\n",
      "495    ازدیاد  6.677870e-08\n",
      "496      ##نگ  6.664662e-08\n",
      "497    ناگفته  6.659998e-08\n",
      "498     درصدی  6.653028e-08\n",
      "499      لوکس  6.632603e-08\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0        قابل  9.941458e-01\n",
      "377     مقابل  1.085311e-07\n",
      "قابل -> قابل\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: توجهی\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       توجهی  0.916830\n",
      "1     اعتمادی  0.024152\n",
      "2        توجه  0.018033\n",
      "3       قبولی  0.009788\n",
      "4      اتکایی  0.007179\n",
      "..        ...       ...\n",
      "495     شاخصی  0.000003\n",
      "496     شتابی  0.000003\n",
      "497   طرفداری  0.000003\n",
      "498       ضرب  0.000003\n",
      "499    مشارکت  0.000003\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       توجهی  0.916830\n",
      "2        توجه  0.018033\n",
      "67      توجیه  0.000056\n",
      "104     تعجبی  0.000025\n",
      "144     توجهش  0.000015\n",
      "379     توقعی  0.000004\n",
      "487      وجهی  0.000003\n",
      "توجهی -> توجهی\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: که\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0          که  9.945062e-01\n",
      "1          از  3.312191e-03\n",
      "2        ##که  9.730795e-04\n",
      "3         سهم  1.368098e-04\n",
      "4           ،  1.364419e-04\n",
      "..        ...           ...\n",
      "495      داعش  1.490147e-07\n",
      "496     اینها  1.488544e-07\n",
      "497     تعداد  1.478358e-07\n",
      "498  دریافتند  1.475818e-07\n",
      "499   بتوانند  1.474439e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        که  0.994506\n",
      "که -> که\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: این\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0         این  0.659941\n",
      "1          هر  0.309322\n",
      "2        تنها  0.002359\n",
      "3          از  0.001082\n",
      "4         بین  0.001015\n",
      "..        ...       ...\n",
      "495   دریافتی  0.000004\n",
      "496       جنگ  0.000004\n",
      "497     عوامل  0.000004\n",
      "498    سفیران  0.000004\n",
      "499      نزدک  0.000004\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0         این  0.659941\n",
      "4         بین  0.001015\n",
      "60        چین  0.000087\n",
      "82         ین  0.000060\n",
      "270       اون  0.000010\n",
      "308      ازین  0.000009\n",
      "این -> این\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: دو\n",
      "Predicts: \n",
      "\n",
      "      token_str     score\n",
      "0            دو  0.778935\n",
      "1            سه  0.108921\n",
      "2          چهار  0.043357\n",
      "3           پنج  0.014913\n",
      "4            شش  0.004781\n",
      "..          ...       ...\n",
      "495  بازنشستگان  0.000002\n",
      "496       دیوان  0.000002\n",
      "497          ۹۳  0.000002\n",
      "498       ایلات  0.000002\n",
      "499          53  0.000002\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        دو  0.778935\n",
      "دو -> دو\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: کشور\n",
      "Predicts: \n",
      "\n",
      "     token_str     score\n",
      "0         کشور  0.952903\n",
      "1        بازار  0.006136\n",
      "2       کشورها  0.005373\n",
      "3         صنعت  0.005103\n",
      "4         شرکت  0.003535\n",
      "..         ...       ...\n",
      "495      بیشتر  0.000003\n",
      "496  بازرگانان  0.000003\n",
      "497       گانه  0.000003\n",
      "498      کشورد  0.000003\n",
      "499      فهرست  0.000003\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        کشور  0.952903\n",
      "85      کشوری  0.000042\n",
      "296     کشورم  0.000006\n",
      "498     کشورد  0.000003\n",
      "کشور -> کشور\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: در\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0          در  8.093439e-01\n",
      "1          از  1.366852e-01\n",
      "2        برای  3.844017e-02\n",
      "3          بر  8.359332e-03\n",
      "4         روی  1.524752e-03\n",
      "..        ...           ...\n",
      "495       ازش  6.831756e-07\n",
      "496     سومین  6.818128e-07\n",
      "497     پایین  6.795129e-07\n",
      "498       علم  6.787059e-07\n",
      "499      وجود  6.776665e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        در  0.809344\n",
      "در -> در\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: تامین\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       بازار  0.788247\n",
      "1         سبد  0.054627\n",
      "2    بازارهای  0.028973\n",
      "3       تولید  0.018836\n",
      "4      صادرات  0.011185\n",
      "..        ...       ...\n",
      "495     سیطره  0.000010\n",
      "496     تلاطم  0.000010\n",
      "497      پختن  0.000010\n",
      "498     انجمن  0.000010\n",
      "499     استان  0.000010\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "17      تامین  0.001355\n",
      "26      تاریخ  0.000889\n",
      "48      تعیین  0.000337\n",
      "160       این  0.000059\n",
      "207      زمین  0.000040\n",
      "271     دومین  0.000028\n",
      "342     تمامی  0.000020\n",
      "تامین -> تامین\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: گندم\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        گندم  0.137607\n",
      "1        نیاز  0.097318\n",
      "2     نیازهای  0.089603\n",
      "3        برنج  0.049470\n",
      "4     محصولات  0.048909\n",
      "..        ...       ...\n",
      "495    صبحانه  0.000056\n",
      "496   وارداتی  0.000056\n",
      "497     تعادل  0.000055\n",
      "498       همه  0.000055\n",
      "499       عطر  0.000055\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0      گندم  0.137607\n",
      "گندم -> گندم\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: جهان\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        کشور  0.128874\n",
      "1       ایران  0.083599\n",
      "2         خود  0.077801\n",
      "3       ##شان  0.038146\n",
      "4        مردم  0.032279\n",
      "..        ...       ...\n",
      "495        ان  0.000102\n",
      "496      ساله  0.000102\n",
      "497     برجای  0.000102\n",
      "498    اسلامی  0.000101\n",
      "499     تامیل  0.000101\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "5       جهانی  0.032201\n",
      "12       جهان  0.014016\n",
      "483      جوان  0.000106\n",
      "جهان -> جهان\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: داشتند\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0      داشتند  8.445799e-01\n",
      "1       دارند  1.275869e-01\n",
      "2        داشت  1.611551e-02\n",
      "3        دارد  2.985660e-03\n",
      "4       کردند  1.851056e-03\n",
      "..        ...           ...\n",
      "495       دوم  1.830139e-07\n",
      "496      بردم  1.822870e-07\n",
      "497    پیروزی  1.822749e-07\n",
      "498     نگاشت  1.818873e-07\n",
      "499    توانست  1.818113e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0      داشتند  8.445799e-01\n",
      "1       دارند  1.275869e-01\n",
      "2        داشت  1.611551e-02\n",
      "5     نداشتند  6.864680e-04\n",
      "7     گذاشتند  5.871683e-04\n",
      "9       داشته  5.533934e-04\n",
      "10     یافتند  4.784647e-04\n",
      "11      دادند  4.230126e-04\n",
      "16     داشتیم  1.324715e-04\n",
      "17     ساختند  1.128680e-04\n",
      "26      داشتن  5.259762e-05\n",
      "27      داشتم  4.590753e-05\n",
      "38   برداشتند  2.177691e-05\n",
      "40    دانستند  2.089824e-05\n",
      "58     نوشتند  8.465066e-06\n",
      "70      داشتی  6.221079e-06\n",
      "76     باختند  4.981593e-06\n",
      "104     کشتند  2.688327e-06\n",
      "131    داشتید  1.691561e-06\n",
      "153   ##اشتند  1.255094e-06\n",
      "259     گشتند  5.485538e-07\n",
      "264     باشند  5.356330e-07\n",
      "291    نداشتن  4.362783e-07\n",
      "305     دانند  3.966571e-07\n",
      "306  پنداشتند  3.960614e-07\n",
      "479   نداشتید  1.976086e-07\n",
      "داشتند -> داشتند\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: ،\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0           ،  9.820303e-01\n",
      "1       بازار  6.329770e-03\n",
      "2        قیمت  3.229886e-03\n",
      "3           ؛  2.350074e-03\n",
      "4           .  1.117720e-03\n",
      "..        ...           ...\n",
      "495      جدال  5.665809e-07\n",
      "496      گردش  5.645653e-07\n",
      "497    انگیزه  5.634266e-07\n",
      "498      مکان  5.595546e-07\n",
      "499       قطع  5.593556e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str    score\n",
      "0         ،  0.98203\n",
      "، -> ،\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: بازار\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        قیمت  0.652956\n",
      "1       بازار  0.091043\n",
      "2         نرخ  0.059374\n",
      "3       تولید  0.035017\n",
      "4        مصرف  0.018554\n",
      "..        ...       ...\n",
      "495   کالاهای  0.000010\n",
      "496     عواید  0.000009\n",
      "497    مجموعه  0.000009\n",
      "498      شروع  0.000009\n",
      "499   استحصال  0.000009\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "1       بازار  0.091043\n",
      "131     بازده  0.000080\n",
      "190   بازارها  0.000043\n",
      "299     مازاد  0.000021\n",
      "385     بازهم  0.000014\n",
      "بازار -> بازار\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: گندم\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        گندم  0.895840\n",
      "1        بورس  0.013478\n",
      "2        برنج  0.013076\n",
      "3        سویا  0.006608\n",
      "4         ارز  0.004927\n",
      "..        ...       ...\n",
      "495    اتیوپی  0.000012\n",
      "496    کامبوج  0.000012\n",
      "497      چینی  0.000012\n",
      "498     بامبو  0.000012\n",
      "499      فعلی  0.000012\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str    score\n",
      "0      گندم  0.89584\n",
      "گندم -> گندم\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: با\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0          با  9.996070e-01\n",
      "1        ##با  1.993851e-04\n",
      "2           و  1.077497e-04\n",
      "3          به  1.233370e-05\n",
      "4          در  1.057246e-05\n",
      "..        ...           ...\n",
      "495       همه  7.584357e-09\n",
      "496     باهدف  7.574656e-09\n",
      "497      سیاه  7.545341e-09\n",
      "498      وارد  7.514979e-09\n",
      "499    بلاخره  7.512801e-09\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        با  0.999607\n",
      "با -> با\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: نوسانات\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0     استقبال  0.125158\n",
      "1         افت  0.109438\n",
      "2      مشکلات  0.102187\n",
      "3        رونق  0.093268\n",
      "4     نوسانات  0.068566\n",
      "..        ...       ...\n",
      "495    سنگینی  0.000042\n",
      "496    رسوایی  0.000042\n",
      "497    کشفیات  0.000042\n",
      "498   فزاینده  0.000042\n",
      "499   معترضان  0.000042\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "   token_str     score\n",
      "4    نوسانات  0.068566\n",
      "21     نوسان  0.005141\n",
      "نوسانات -> نوسانات\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: زیادی\n",
      "Predicts: \n",
      "\n",
      "     token_str     score\n",
      "0        شدیدی  0.572921\n",
      "1         شدید  0.087914\n",
      "2        زیادی  0.071150\n",
      "3         قیمت  0.034467\n",
      "4      فراوانی  0.029284\n",
      "..         ...       ...\n",
      "495       عددی  0.000014\n",
      "496      غذایی  0.000014\n",
      "497        ##ه  0.000013\n",
      "498    معاملات  0.000013\n",
      "499  ناخوشایند  0.000013\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "2       زیادی  0.071150\n",
      "51       زیاد  0.000615\n",
      "88      سیاسی  0.000265\n",
      "90      پیاپی  0.000251\n",
      "102     ریالی  0.000191\n",
      "103     زمانی  0.000179\n",
      "293    بنیادی  0.000032\n",
      "367   زیادتری  0.000023\n",
      "425     زیادش  0.000018\n",
      "زیادی -> زیادی\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: مواجه\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0       مواجه  5.694528e-01\n",
      "1       روبرو  2.776965e-01\n",
      "2       همراه  1.441149e-01\n",
      "3        شروع  8.910943e-04\n",
      "4       انجام  7.487168e-04\n",
      "..        ...           ...\n",
      "495   توانسته  9.622634e-07\n",
      "496     ##فته  9.595261e-07\n",
      "497     متحمل  9.591053e-07\n",
      "498   محدودتر  9.508876e-07\n",
      "499      دلار  9.447140e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       مواجه  0.569453\n",
      "171      موجب  0.000006\n",
      "199    مواجهه  0.000004\n",
      "206     مشابه  0.000004\n",
      "232     متوجه  0.000004\n",
      "259     موازی  0.000003\n",
      "349     روانه  0.000002\n",
      "373      مواج  0.000002\n",
      "418     موافق  0.000001\n",
      "478   مواجهیم  0.000001\n",
      "مواجه -> مواجه\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: شد\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0          شد  9.562056e-01\n",
      "1         گشت  1.779953e-02\n",
      "2       گردید  7.578693e-03\n",
      "3         شده  7.315370e-03\n",
      "4         بود  4.593331e-03\n",
      "..        ...           ...\n",
      "495       نشه  1.643642e-07\n",
      "496    انحراف  1.640745e-07\n",
      "497      ۱۹۸۰  1.640120e-07\n",
      "498    نگذشته  1.637203e-07\n",
      "499      حباب  1.631386e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        شد  0.956206\n",
      "شد -> شد\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: و\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0           و  9.757824e-01\n",
      "1           .  1.171675e-02\n",
      "2         اما  6.162404e-03\n",
      "3           ؛  1.375139e-03\n",
      "4         ولی  1.261062e-03\n",
      "..        ...           ...\n",
      "495       شود  6.028590e-09\n",
      "496    مشکلات  6.003404e-09\n",
      "497    باتوجه  5.981927e-09\n",
      "498    تقاضای  5.961925e-09\n",
      "499    انگیزه  5.955083e-09\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0         و  0.975782\n",
      "و -> و\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: قیمت\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        قیمت  0.316960\n",
      "1       تعداد  0.186677\n",
      "2       میزان  0.063859\n",
      "3        ارزش  0.038365\n",
      "4      همچنین  0.034588\n",
      "..        ...       ...\n",
      "495       شهد  0.000022\n",
      "496   مطلوبیت  0.000022\n",
      "497    گنجایش  0.000022\n",
      "498      فرصت  0.000022\n",
      "499      وضوح  0.000022\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        قیمت  0.316960\n",
      "93      قیمتی  0.000402\n",
      "185      قدمت  0.000122\n",
      "236     قیمتش  0.000082\n",
      "قیمت -> قیمت\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: محصولاتی\n",
      "Predicts: \n",
      "\n",
      "     token_str     score\n",
      "0     کالاهایی  0.218943\n",
      "1         گندم  0.207027\n",
      "2     محصولاتی  0.196248\n",
      "3      غذاهایی  0.016952\n",
      "4        موادی  0.014883\n",
      "..         ...       ...\n",
      "495  کارخانجات  0.000076\n",
      "496     پلیمری  0.000076\n",
      "497        زنی  0.000076\n",
      "498     کمیابی  0.000075\n",
      "499     درحالی  0.000075\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "2    محصولاتی  0.196248\n",
      "16    محصولات  0.004964\n",
      "26     محصولی  0.002343\n",
      "162  محصولاتش  0.000291\n",
      "محصولاتی -> محصولاتی\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: که\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0          که  9.974781e-01\n",
      "1        ##که  1.282285e-03\n",
      "2         چون  2.087021e-04\n",
      "3          از  1.929158e-04\n",
      "4           و  8.545105e-05\n",
      "..        ...           ...\n",
      "495      درون  6.136091e-08\n",
      "496   شیمیایی  6.084625e-08\n",
      "497     کلیدی  6.067473e-08\n",
      "498   مستعمره  6.056223e-08\n",
      "499        تو  6.032589e-08\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        که  0.997478\n",
      "که -> که\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: مواد\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       محصول  0.184631\n",
      "1        مواد  0.154016\n",
      "2        نیاز  0.121477\n",
      "3       خوراک  0.080418\n",
      "4        ماده  0.058677\n",
      "..        ...       ...\n",
      "495     تجربه  0.000050\n",
      "496       مرغ  0.000049\n",
      "497   تجهیزات  0.000049\n",
      "498      بسته  0.000049\n",
      "499       شیر  0.000049\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "1        مواد  0.154016\n",
      "278      مورد  0.000133\n",
      "312      موعد  0.000114\n",
      "مواد -> مواد\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: اولیه‌\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       اولیه  0.526260\n",
      "1       غذایی  0.140910\n",
      "2        اصلی  0.118762\n",
      "3       اساسی  0.085550\n",
      "4        مغذی  0.025680\n",
      "..        ...       ...\n",
      "495      ##ni  0.000011\n",
      "496      خامی  0.000011\n",
      "497      باید  0.000011\n",
      "498  گرانقیمت  0.000011\n",
      "499       بدی  0.000010\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0       اولیه  0.526260\n",
      "283     اولین  0.000027\n",
      "اولیه‌ -> اولیه\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: شان\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        اصلی  0.466684\n",
      "1       اولیه  0.099197\n",
      "2         این  0.031718\n",
      "3        عمده  0.031179\n",
      "4        برای  0.023281\n",
      "..        ...       ...\n",
      "495   نامطلوب  0.000057\n",
      "496    کمپانی  0.000057\n",
      "497    حبوبات  0.000056\n",
      "498      سوخت  0.000056\n",
      "499     تاسیس  0.000056\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "16        شان  0.004054\n",
      "103        ان  0.000586\n",
      "142       نان  0.000384\n",
      "شان -> شان\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: گندم\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        برنج  0.102665\n",
      "1         خام  0.051764\n",
      "2        گندم  0.036185\n",
      "3        لازم  0.031233\n",
      "4       بیشتر  0.026394\n",
      "..        ...       ...\n",
      "495     بازار  0.000138\n",
      "496      پنبه  0.000138\n",
      "497       سبک  0.000138\n",
      "498   تایوانی  0.000137\n",
      "499     کاهشی  0.000137\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "2      گندم  0.036185\n",
      "گندم -> گندم\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: بود\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0         است  0.511576\n",
      "1         بود  0.276347\n",
      "2       بودند  0.080731\n",
      "3        بوده  0.035174\n",
      "4       هستند  0.035015\n",
      "..        ...       ...\n",
      "495       طلا  0.000002\n",
      "496     خریدم  0.000002\n",
      "497     ##دان  0.000002\n",
      "498        نر  0.000002\n",
      "499  میخواهند  0.000002\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "1         بود  0.276347\n",
      "3        بوده  0.035174\n",
      "9        نبود  0.003044\n",
      "46        شود  0.000070\n",
      "101      بودن  0.000018\n",
      "226      بشود  0.000006\n",
      "229      بودو  0.000006\n",
      "272      بوند  0.000005\n",
      "433       یود  0.000002\n",
      "بود -> بود\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: ،\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0           ،  0.686664\n",
      "1         نیز  0.211526\n",
      "2          هم  0.046519\n",
      "3      تقریبا  0.014173\n",
      "4           و  0.013690\n",
      "..        ...       ...\n",
      "495  استرالیا  0.000001\n",
      "496     مالزی  0.000001\n",
      "497        پز  0.000001\n",
      "498     روندی  0.000001\n",
      "499     هرماه  0.000001\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0         ،  0.686664\n",
      "، -> ،\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: در\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0          در  9.720080e-01\n",
      "1          از  9.061441e-03\n",
      "2          به  5.935654e-03\n",
      "3          تا  5.582538e-03\n",
      "4      تقریبا  2.718235e-03\n",
      "..        ...           ...\n",
      "495      ساعت  3.412456e-07\n",
      "496     صنعتی  3.411519e-07\n",
      "497       کشت  3.400170e-07\n",
      "498       هرز  3.389699e-07\n",
      "499     مرتبا  3.388713e-07\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0        در  0.972008\n",
      "در -> در\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: همه\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0         همه  0.694758\n",
      "1          هر  0.136312\n",
      "2         جای  0.039314\n",
      "3        تمام  0.018467\n",
      "4       بیشتر  0.017565\n",
      "..        ...       ...\n",
      "495       زیر  0.000005\n",
      "496    هزارتا  0.000005\n",
      "497      عملا  0.000005\n",
      "498       باب  0.000005\n",
      "499    انحصار  0.000005\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0         همه  0.694758\n",
      "230        هم  0.000016\n",
      "همه -> همه\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: جای\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0     کشورهای  0.397734\n",
      "1         جای  0.232196\n",
      "2        نقاط  0.216619\n",
      "3       مناطق  0.036742\n",
      "4    بازارهای  0.031969\n",
      "..        ...       ...\n",
      "495     قیمتی  0.000008\n",
      "496     دستان  0.000008\n",
      "497      بستر  0.000008\n",
      "498        صد  0.000008\n",
      "499      سران  0.000008\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "1         جای  0.232196\n",
      "30       کجای  0.000578\n",
      "39         جا  0.000379\n",
      "243       های  0.000022\n",
      "291       جات  0.000017\n",
      "394       تای  0.000010\n",
      "395      جایی  0.000010\n",
      "جای -> جای\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: جهان\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0        دنیا  0.523212\n",
      "1        جهان  0.403485\n",
      "2        کشور  0.024965\n",
      "3       بازار  0.010243\n",
      "4         سال  0.009698\n",
      "..        ...       ...\n",
      "495      مجمع  0.000002\n",
      "496        پر  0.000002\n",
      "497        به  0.000002\n",
      "498      لیبی  0.000002\n",
      "499     بشریت  0.000002\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "1        جهان  0.403485\n",
      "16      جهانی  0.000508\n",
      "317      جهات  0.000003\n",
      "جهان -> جهان\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: افزایش\n",
      "Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0      افزایش  0.680124\n",
      "1        کاهش  0.243380\n",
      "2       گسترش  0.012002\n",
      "3       ادامه  0.011234\n",
      "4         افت  0.008988\n",
      "..        ...       ...\n",
      "495    میلادی  0.000002\n",
      "496       طول  0.000002\n",
      "497   پایینتر  0.000002\n",
      "498      نبود  0.000002\n",
      "499     اجازه  0.000002\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str     score\n",
      "0      افزایش  0.680124\n",
      "37    افزایشی  0.000171\n",
      "116    ##زایش  0.000023\n",
      "223     افرای  0.000007\n",
      "افزایش -> افزایش\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: یافت\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0        یافت  9.910963e-01\n",
      "1        داشت  3.851635e-03\n",
      "2      یافتند  1.245524e-03\n",
      "3        یابد  9.780817e-04\n",
      "4       نیافت  5.508530e-04\n",
      "..        ...           ...\n",
      "495       تنش  3.202894e-08\n",
      "496       ناگ  3.185506e-08\n",
      "497    فیزیکی  3.184995e-08\n",
      "498     بیشتر  3.183076e-08\n",
      "499        ۴۹  3.162302e-08\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0        یافت  9.910963e-01\n",
      "4       نیافت  5.508530e-04\n",
      "11      یافته  1.287383e-04\n",
      "51      یافتن  2.341609e-06\n",
      "265     یافتم  7.996164e-08\n",
      "463     میافت  3.521053e-08\n",
      "یافت -> یافت\n",
      "SOME TOKEN CORRECTED!!!\n",
      "**************************************************\n",
      "Token: .\n",
      "Predicts: \n",
      "\n",
      "    token_str         score\n",
      "0           .  9.962485e-01\n",
      "1           !  2.054227e-03\n",
      "2           …  1.473879e-03\n",
      "3           :  1.540716e-04\n",
      "4           ؛  1.792236e-05\n",
      "..        ...           ...\n",
      "495     شرایط  4.215823e-09\n",
      "496   فروشگاه  4.210222e-09\n",
      "497     سیمین  4.186336e-09\n",
      "498        مس  4.168575e-09\n",
      "499       چرا  4.156539e-09\n",
      "\n",
      "[500 rows x 2 columns]\n",
      "Filtered Predicts: \n",
      "\n",
      "  token_str     score\n",
      "0         .  0.996249\n",
      ". -> .\n",
      "SOME TOKEN CORRECTED!!!\n",
      "Result text: \n",
      "با طولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تامین گندم جهان داشتند، بازار گندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان گندم بود، در همه جای جهان افزایش یافت.\n"
     ]
    },
    {
     "data": {
      "text/plain": "'با طولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تامین گندم جهان داشتند، بازار گندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه\\u200cشان گندم بود، در همه جای جهان افزایش یافت.'"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_EDIT_DISTANCE_TO_LEN_RATIO = 0.4\n",
    "MAX_EDIT_DISTANCE = 2\n",
    "MIN_SCORE = 0\n",
    "TOP_K = 250\n",
    "VERBOSE = True\n",
    "\n",
    "if language == 'en':\n",
    "    input_text = \"The capitan of Iran is tehran.\"\n",
    "    input_text = \"i am speeking english very wall.\"\n",
    "    input_text = \"He was stadying english for the finall exam.\"\n",
    "    # text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "    # text = \"I'm a very [MASK] player in football.\"\n",
    "    input_text = \"He drove a cat.\"\n",
    "    # text = \"do you want to watch tv.\"\n",
    "    # text = \"I love playing [MASK].\"\n",
    "\n",
    "if language == 'fa':\n",
    "    input_text = \"امروز در استادیوم آزادی تیم ملی ایران و روسیه مسایقه می‌دهند.\"\n",
    "    input_text = \"پس از سال‌ها تلاش رازی موفق به کسف الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ کور است.\"\n",
    "    input_text = \"هفته آینده احتمالا توافق بسته‌ای امضا می‌شود.\"\n",
    "    input_text = \"اهل کدام کشور هستی.\"\n",
    "    input_text = \"سن شما چقدر است.\"\n",
    "    input_text = \"وقتی قیمت گوست قرمز یا صفید در کشورهای دیگر بیشتر شده است، ممکن است در جیران هم گرا شود.\"\n",
    "    input_text = \"در هفته گذشته قیمت تلا تغییر چندانی نداشت، و در همان محدوده 1850 دلاری کار خود را به پایان رساند. \"\n",
    "    input_text = \"هدف از زندگانی چیست!\"\n",
    "    input_text = \"همه رأس ساعت 3 در جلسه حاضر باشند.\"\n",
    "\n",
    "    input_text = \"بر اساس مسوبه سران قوا، معاملات فردایی طلا همانند معاملات فردایی ارض، ممنوع و غیرقانونی شناخته شد و فعالان این بازار به جرم اخلال اقتصادی، تحت پیگرد قرار خواهند گرفت. در نتیجه تانک مرکزی در بازار فردایی مداخله نخواهد کرد\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "        با نزدیک شدن قیمت دار غیر رسمی به سفف خود در روز قبل، تحلیلگران در بازار برای هفته بعد هشدار میدادند که باید احطیاط کرد و اقدامات امنیتی در بازار افزایش خواهد یافت.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = \"\"\"\n",
    "    با تولانی شدن جنگ روسیه و اوکراین و سهم قابل توجهی که این دو کشور در تأمین کندم جهان داشتند، بازار کندم با نوسانات زیادی مواجه شد و قیمت محصولاتی که مواد اولیه‌شان کندم بود، در همه جای جهان افزایش یافت.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "if language == 'en':\n",
    "    input_text = input_text.lower()\n",
    "\n",
    "\n",
    "input_text = input_text.strip()\n",
    "\n",
    "print(f\"normalized text: {input_text}\")\n",
    "\n",
    "spell_corrector = SpellCorrector(MAX_EDIT_DISTANCE_TO_LEN_RATIO, MAX_EDIT_DISTANCE, MIN_SCORE, VERBOSE, TOP_K)\n",
    "\n",
    "doc = nlp(input_text)\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")\n",
    "\n",
    "result = \" \".join([spell_corrector(sentence.text) for sentence in doc.sents])\n",
    "result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"تلا\" in vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# text = \"The capital of Iran is [MASK].\"\n",
    "# text = \"Yesterday, I played [MASK] in the [MASK].\"\n",
    "# text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "# text = \"I'm a very [MASK] player in football.\"\n",
    "# text = \"He drived a [MASK].\"\n",
    "# text = \"I love playing [MASK].\"\n",
    "# text = \"I am [MASK] english very [MASK]. \"\n",
    "#\n",
    "# # text = \"امروز در استادیوم آزادی، تیم ملی [MASK] ایران و سوریه مسابقه می‌دهند.\"\n",
    "# # text = \"امروز در استادیوم آزادی [ماسک] ملی ایران و روسیه مسایقه می‌دهند.\"\n",
    "# # text = \"پس از سال‌ها تلاش رازی موفق به [ماسک] الکل شد. این دانشمند تیرانی باعث افتخار در تاریخ [ماسک] است.\"\n",
    "# # text = \"اهل کدام کشور هستی[ماسک]\"\n",
    "# # text = text.replace(\"[ماسک]\", \"[MASK]\")\n",
    "#\n",
    "# unmasker(text, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
    "# tokens\n",
    "# tokenizer.convert_tokens_to_string(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ContextualSpellCheck Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import contextualSpellCheck\n",
    "#\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "# contextualSpellCheck.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# text = 'i am speaking english very wall.'\n",
    "# doc = nlp(text)\n",
    "#\n",
    "# print(doc._.performed_spellCheck) #Should be True\n",
    "# print(doc._.outcome_spellCheck) #Income was $9.4 million compared to the prior year of $2.7 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
    "# # tokenizer.convert_tokens_to_string(tokens)\n",
    "#\n",
    "# # tokenizer.get_vocab()\n",
    "#\n",
    "# text = \"The capitan of Iran is tehran.\"\n",
    "# text = \"i am speeking english very wall.\"\n",
    "# # text = \"Yesterday, I played [MASK] in the [MASK].\"\n",
    "# # text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "# # text = \"I'm a very [MASK] player in football.\"\n",
    "# # text = \"He drove a cat.\"\n",
    "# # text = \"do you want to watch tv.\"\n",
    "# # text = \"I love playing [MASK].\"\n",
    "#\n",
    "# text = text.lower()\n",
    "# vocab: dict = tokenizer.get_vocab()\n",
    "# MASK = \"[MASK]\"\n",
    "#\n",
    "# MAX_EDIT_DISTANCE_TO_LEN_RATIO = 0.5\n",
    "# MIN_SCORE = 0.01\n",
    "#\n",
    "# ### Correct Lexico Typo\n",
    "# print(\"\\n\\nCorrect Lexico Typo:\\n\\n\")\n",
    "# print(f\"Text: {text}\\n\\n\")\n",
    "# while True:\n",
    "#     some_token_corrected = False\n",
    "#     doc = nlp(text)\n",
    "#     for index in range(len(doc)):\n",
    "#         current_token: Token = doc[index]\n",
    "#         start_char_index = current_token.idx\n",
    "#         end_char_index = start_char_index + len(current_token)\n",
    "#\n",
    "#         if current_token.text not in vocab:\n",
    "#             print(f\"[{current_token.text}] is not in vocab\")\n",
    "#\n",
    "#             tokens = list(doc)\n",
    "#             print(f\"Tokens: {tokens}\")\n",
    "#\n",
    "#             masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "#\n",
    "#             predicts = unmasker(masked_text)\n",
    "#\n",
    "#             ### Select Token From Predicts\n",
    "#             predicts = pd.DataFrame(predicts)\n",
    "#\n",
    "#\n",
    "#             predicts['token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "#             predicts['edit_distance_to_len_ratio'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk) / len(current_token.text))\n",
    "#\n",
    "#             print(\"Predicts: \\n\")\n",
    "#             print(predicts)\n",
    "#\n",
    "#             predicts = predicts[predicts['edit_distance_to_len_ratio'] <= MAX_EDIT_DISTANCE_TO_LEN_RATIO]\n",
    "#\n",
    "#             print(\"Filtered Predicts: \\n\")\n",
    "#             print(predicts)\n",
    "#\n",
    "#             selected_predict = predicts['token_str'].iloc[0]\n",
    "#             ###\n",
    "#\n",
    "#             result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "#\n",
    "#             print(f\"{current_token.text} -> {selected_predict}\")\n",
    "#             print(f\"Text: {result_text}\\n\\n\")\n",
    "#\n",
    "#             text = result_text\n",
    "#             some_token_corrected = True\n",
    "#             break\n",
    "#\n",
    "#     if not some_token_corrected:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # tokens = tokenizer.convert_ids_to_tokens(tokenizer(text)['input_ids'])\n",
    "# # tokenizer.convert_tokens_to_string(tokens)\n",
    "#\n",
    "# # tokenizer.get_vocab()\n",
    "#\n",
    "# text = \"The capitan of Iran is tehran.\"\n",
    "# text = \"i am speeking english very wall.\"\n",
    "# # text = \"Yesterday, I played [MASK] in the [MASK].\"\n",
    "# # text = \"I'm studying [MASK] learning in my computer class.\"\n",
    "# # text = \"I'm a very [MASK] player in football.\"\n",
    "# # text = \"He drove a cat.\"\n",
    "# # text = \"do you want to watch tv.\"\n",
    "# # text = \"I love playing [MASK].\"\n",
    "#\n",
    "# text = text.lower()\n",
    "# vocab: dict = tokenizer.get_vocab()\n",
    "# MASK = \"[MASK]\"\n",
    "#\n",
    "# MAX_EDIT_DISTANCE_TO_LEN_RATIO = 0.5\n",
    "# MIN_SCORE = 0.01\n",
    "#\n",
    "# ### Correct Lexico Typo\n",
    "# print(\"\\n\\nCorrect Lexico Typo:\\n\\n\")\n",
    "# print(f\"Text: {text}\\n\\n\")\n",
    "# while True:\n",
    "#     some_token_corrected = False\n",
    "#     doc = nlp(text)\n",
    "#     for index in range(len(doc)):\n",
    "#         current_token: Token = doc[index]\n",
    "#         start_char_index = current_token.idx\n",
    "#         end_char_index = start_char_index + len(current_token)\n",
    "#\n",
    "#         if current_token.text not in vocab:\n",
    "#             print(f\"[{current_token.text}] is not in vocab\")\n",
    "#\n",
    "#             tokens = list(doc)\n",
    "#             print(f\"Tokens: {tokens}\")\n",
    "#\n",
    "#             masked_text = doc.text[:start_char_index] + MASK + doc.text[end_char_index:]\n",
    "#\n",
    "#             predicts = unmasker(masked_text)\n",
    "#\n",
    "#             ### Select Token From Predicts\n",
    "#             predicts = pd.DataFrame(predicts)\n",
    "#\n",
    "#\n",
    "#             predicts['token_str'] = predicts['token_str'].apply(lambda tk: tk.replace(\" \", \"\"))\n",
    "#             predicts['edit_distance_to_len_ratio'] = predicts['token_str'].apply(lambda tk: editdistance.eval(current_token.text, tk) / len(current_token.text))\n",
    "#\n",
    "#             print(\"Predicts: \\n\")\n",
    "#             print(predicts)\n",
    "#\n",
    "#             predicts = predicts[predicts['edit_distance_to_len_ratio'] <= MAX_EDIT_DISTANCE_TO_LEN_RATIO]\n",
    "#\n",
    "#             print(\"Filtered Predicts: \\n\")\n",
    "#             print(predicts)\n",
    "#\n",
    "#             selected_predict = predicts['token_str'].iloc[0]\n",
    "#             ###\n",
    "#\n",
    "#             result_text = masked_text.replace(MASK, selected_predict, 1)\n",
    "#\n",
    "#             print(f\"{current_token.text} -> {selected_predict}\")\n",
    "#             print(f\"Text: {result_text}\\n\\n\")\n",
    "#\n",
    "#             text = result_text\n",
    "#             some_token_corrected = True\n",
    "#             break\n",
    "#\n",
    "#     if not some_token_corrected:\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34daa296ffe99e8a66e159d01b1dfeb9a87967b5cca691fda43c054f03617153"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}